{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "991d0502",
   "metadata": {},
   "source": [
    "# KuaiRec Data Preparation (Clean, Burst-Aware, LightGBM-Ready)\n",
    "\n",
    "This notebook rebuilds the entire data-prep pipeline end-to-end:\n",
    "\n",
    "1. Load raw CSVs and construct sessions (1h gap)  \n",
    "2. Build labels for 4 heads (`complete`, `long`, `rewatch`, `neg`)  \n",
    "3. Merge **user** and **item** static features  \n",
    "4. Add **caption category** features  \n",
    "5. Create **context** + **session-structure** features  \n",
    "6. Build **history** features (leakage-safe)  \n",
    "7. Detect **bursts** and apply **prior resets** at the starts of bursts 2 & 3  \n",
    "8. Clean redundancies and save outputs\n",
    "9. Create empirical category distribution for each user's session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8899d1b",
   "metadata": {},
   "source": [
    "## Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7845871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Base path for your CSVs\n",
    "BASE = Path(\"/Users/haozhangao/Desktop/RecSys Research/KuaiRec 2.0/data\")\n",
    "\n",
    "# Output dir\n",
    "OUT = BASE / \"prepared\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e240d2",
   "metadata": {},
   "source": [
    "## 1) Load `big_matrix.csv` and build sessions (1h gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2459a608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded interactions: (12530806, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>play_duration</th>\n",
       "      <th>video_duration</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>watch_ratio</th>\n",
       "      <th>ts</th>\n",
       "      <th>session</th>\n",
       "      <th>sess_rank</th>\n",
       "      <th>sess_len</th>\n",
       "      <th>sess_rank_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3649</td>\n",
       "      <td>13838</td>\n",
       "      <td>10867</td>\n",
       "      <td>2020-07-05 00:08:23.438</td>\n",
       "      <td>20200705</td>\n",
       "      <td>1.593879e+09</td>\n",
       "      <td>1.273397</td>\n",
       "      <td>2020-07-04 16:08:23.437999964</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>9598</td>\n",
       "      <td>13665</td>\n",
       "      <td>10984</td>\n",
       "      <td>2020-07-05 00:13:41.297</td>\n",
       "      <td>20200705</td>\n",
       "      <td>1.593879e+09</td>\n",
       "      <td>1.244082</td>\n",
       "      <td>2020-07-04 16:13:41.296999931</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5262</td>\n",
       "      <td>851</td>\n",
       "      <td>7908</td>\n",
       "      <td>2020-07-05 00:16:06.687</td>\n",
       "      <td>20200705</td>\n",
       "      <td>1.593879e+09</td>\n",
       "      <td>0.107613</td>\n",
       "      <td>2020-07-04 16:16:06.687000036</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  video_id  play_duration  video_duration                     time  \\\n",
       "0        0      3649          13838           10867  2020-07-05 00:08:23.438   \n",
       "1        0      9598          13665           10984  2020-07-05 00:13:41.297   \n",
       "2        0      5262            851            7908  2020-07-05 00:16:06.687   \n",
       "\n",
       "       date     timestamp  watch_ratio                            ts  session  \\\n",
       "0  20200705  1.593879e+09     1.273397 2020-07-04 16:08:23.437999964        1   \n",
       "1  20200705  1.593879e+09     1.244082 2020-07-04 16:13:41.296999931        1   \n",
       "2  20200705  1.593879e+09     0.107613 2020-07-04 16:16:06.687000036        1   \n",
       "\n",
       "   sess_rank  sess_len  sess_rank_frac  \n",
       "0          1         6        0.166667  \n",
       "1          2         6        0.333333  \n",
       "2          3         6        0.500000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load full interaction matrix\n",
    "big_path = BASE / \"big_matrix.csv\"\n",
    "df = pd.read_csv(big_path, low_memory=False)\n",
    "\n",
    "# Normalize key columns\n",
    "for col in [\"user_id\", \"video_id\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Timestamp → ts (auto-detect ms vs s)\n",
    "if \"timestamp\" not in df.columns:\n",
    "    raise KeyError(\"Expected 'timestamp' in big_matrix.csv\")\n",
    "ts_unit = \"ms\" if pd.to_numeric(df[\"timestamp\"], errors=\"coerce\").max() > 10**11 else \"s\"\n",
    "df[\"ts\"] = pd.to_datetime(pd.to_numeric(df[\"timestamp\"], errors=\"coerce\"), unit=ts_unit, errors=\"coerce\")\n",
    "\n",
    "# Sort and sessionize (1-hour gap)\n",
    "df.sort_values([\"user_id\", \"ts\"], inplace=True, kind=\"mergesort\")\n",
    "gap = df.groupby(\"user_id\")[\"ts\"].diff().dt.total_seconds().fillna(np.inf)\n",
    "df[\"session\"] = (gap > 3600).groupby(df[\"user_id\"]).cumsum().astype(\"Int64\")\n",
    "\n",
    "# Within-session order\n",
    "df[\"sess_rank\"] = df.groupby([\"user_id\",\"session\"]).cumcount().astype(\"int32\") + 1\n",
    "df[\"sess_len\"]  = df.groupby([\"user_id\",\"session\"])[\"video_id\"].transform(\"size\").astype(\"int32\")\n",
    "df[\"sess_rank_frac\"] = (df[\"sess_rank\"] / df[\"sess_len\"].clip(lower=1)).astype(\"float32\")\n",
    "\n",
    "print(\"Loaded interactions:\", df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ff8a3",
   "metadata": {},
   "source": [
    "## 2) Create 4 head labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a3d93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durations interpreted as milliseconds. Positive rates: {'y_complete': 0.6995, 'y_long': 0.457, 'y_rewatch': 0.0747, 'y_neg': 0.1251}\n"
     ]
    }
   ],
   "source": [
    "# If watch_ratio missing, compute it\n",
    "if \"watch_ratio\" not in df.columns:\n",
    "    if \"play_duration\" in df.columns and \"video_duration\" in df.columns:\n",
    "        df[\"watch_ratio\"] = pd.to_numeric(df[\"play_duration\"], errors=\"coerce\") / pd.to_numeric(df[\"video_duration\"], errors=\"coerce\")\n",
    "    else:\n",
    "        raise KeyError(\"Need 'watch_ratio' or both 'play_duration' and 'video_duration'.\")\n",
    "\n",
    "# Determine duration unit (ms vs s) from 'video_duration' if present, else assume ms if values large\n",
    "vd_col = None\n",
    "for c in [\"video_duration\", \"video_duration_x\", \"video_duration_y\"]:\n",
    "    if c in df.columns:\n",
    "        vd_col = c\n",
    "        break\n",
    "\n",
    "if vd_col is None:\n",
    "    # fallback: infer from play_duration\n",
    "    ref = pd.to_numeric(df[\"play_duration\"], errors=\"coerce\").dropna()\n",
    "    dur_is_ms = (ref.median() > 1000.0) if not ref.empty else True\n",
    "else:\n",
    "    ref = pd.to_numeric(df[vd_col], errors=\"coerce\").dropna()\n",
    "    dur_is_ms = (ref.median() > 1000.0) if not ref.empty else True\n",
    "\n",
    "scale = 1000 if dur_is_ms else 1\n",
    "twenty_s = 20 * scale\n",
    "two_s = 2 * scale\n",
    "unit_str = \"milliseconds\" if dur_is_ms else \"seconds\"\n",
    "\n",
    "# Clean rows for safe labels\n",
    "good = (\n",
    "    df[\"watch_ratio\"].replace([np.inf, -np.inf], np.nan).notna() &\n",
    "    pd.to_numeric(df[\"play_duration\"], errors=\"coerce\").notna()\n",
    ")\n",
    "if vd_col is not None:\n",
    "    good &= pd.to_numeric(df[vd_col], errors=\"coerce\").notna() & (pd.to_numeric(df[vd_col], errors=\"coerce\") > 0)\n",
    "\n",
    "if (~good).any():\n",
    "    print(f\"Dropping {(~good).sum()} rows with invalid durations/ratios for label construction\")\n",
    "    df = df.loc[good].copy()\n",
    "\n",
    "# Labels\n",
    "df[\"y_complete\"] = (df[\"watch_ratio\"] >= 0.4).astype(\"int8\")\n",
    "if vd_col is not None:\n",
    "    dur_cut = np.minimum(twenty_s, 0.8 * pd.to_numeric(df[vd_col], errors=\"coerce\").to_numpy())\n",
    "else:\n",
    "    dur_cut = np.full(len(df), twenty_s, dtype=\"float64\")\n",
    "df[\"y_long\"] = (pd.to_numeric(df[\"play_duration\"], errors=\"coerce\").to_numpy() >= dur_cut).astype(\"int8\")\n",
    "df[\"y_rewatch\"] = (df[\"watch_ratio\"] >= 2.0).astype(\"int8\")\n",
    "df[\"y_neg\"] = (pd.to_numeric(df[\"play_duration\"], errors=\"coerce\") < two_s).astype(\"int8\")\n",
    "\n",
    "pos_rates = {k: round(float(df[k].mean()), 4) for k in [\"y_complete\",\"y_long\",\"y_rewatch\",\"y_neg\"]}\n",
    "print(f\"Durations interpreted as {unit_str}. Positive rates:\", pos_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b658d6d6",
   "metadata": {},
   "source": [
    "## 3) Merge user static features (`user_features.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "692a6233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user columns: 34\n"
     ]
    }
   ],
   "source": [
    "user_path = BASE / \"user_features.csv\"\n",
    "hdr = pd.read_csv(user_path, nrows=0).columns.tolist()\n",
    "\n",
    "LOW_CARD_CATS = [\"user_active_degree\",\"follow_user_num_range\",\"fans_user_num_range\",\"friend_user_num_range\",\"register_days_range\"]\n",
    "BIN_FLAGS = [\"is_lowactive_period\",\"is_live_streamer\",\"is_video_author\"]\n",
    "NUMS = [\"follow_user_num\",\"fans_user_num\",\"friend_user_num\",\"register_days\"]\n",
    "ONEHOTS = [f\"onehot_feat{i}\" for i in range(18)]\n",
    "\n",
    "usecols = [\"user_id\"] + [c for c in (LOW_CARD_CATS + BIN_FLAGS + NUMS + ONEHOTS) if c in hdr]\n",
    "uf = pd.read_csv(user_path, usecols=usecols, low_memory=False)\n",
    "\n",
    "# Type cast\n",
    "for c in BIN_FLAGS:\n",
    "    if c in uf.columns:\n",
    "        uf[c] = pd.to_numeric(uf[c], errors=\"coerce\").fillna(0).astype(\"int8\").clip(0,1)\n",
    "\n",
    "for c in LOW_CARD_CATS + ONEHOTS:\n",
    "    if c in uf.columns:\n",
    "        uf[c] = uf[c].astype(\"string\").fillna(\"unknown\").replace({\"\": \"unknown\"}).astype(\"category\")\n",
    "\n",
    "for c in NUMS:\n",
    "    if c in uf.columns:\n",
    "        uf[c] = pd.to_numeric(uf[c], errors=\"coerce\").astype(\"float32\")\n",
    "        uf[f\"{c}_log1p\"] = np.log1p(uf[c].clip(lower=0)).astype(\"float32\")\n",
    "\n",
    "# Prefix\n",
    "rename_map = {c: f\"u_{c}\" for c in uf.columns if c != \"user_id\"}\n",
    "uf.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "# Merge\n",
    "before = set(df.columns)\n",
    "df = df.merge(uf, on=\"user_id\", how=\"left\")\n",
    "print(\"Added user columns:\", len(df.columns) - len(before))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2227ca7d",
   "metadata": {},
   "source": [
    "## 4) Merge item static features (`item_daily_features.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a2d23c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added item columns: 14\n"
     ]
    }
   ],
   "source": [
    "item_path = BASE / \"item_daily_features.csv\"\n",
    "ihdr = pd.read_csv(item_path, nrows=0).columns.tolist()\n",
    "\n",
    "want = [\"video_id\",\"author_id\",\"video_type\",\"upload_dt\",\"upload_type\",\"visible_status\",\"video_duration\",\"video_width\",\"video_height\",\"music_id\",\"video_tag_id\",\"video_tag_name\"]\n",
    "usecols = [c for c in want if c in ihdr]\n",
    "it = pd.read_csv(item_path, usecols=usecols, low_memory=False)\n",
    "\n",
    "# Earliest upload per video_id\n",
    "if \"upload_dt\" in it.columns:\n",
    "    if pd.api.types.is_numeric_dtype(it[\"upload_dt\"]):\n",
    "        unit = \"ms\" if it[\"upload_dt\"].max() > 10**11 else \"s\"\n",
    "        up = pd.to_datetime(pd.to_numeric(it[\"upload_dt\"], errors=\"coerce\"), unit=unit, errors=\"coerce\")\n",
    "    else:\n",
    "        up = pd.to_datetime(it[\"upload_dt\"], errors=\"coerce\")\n",
    "    up_df = pd.DataFrame({\"video_id\": it[\"video_id\"], \"i_upload_ts\": up})\n",
    "    up_df = up_df.dropna().groupby(\"video_id\", as_index=False)[\"i_upload_ts\"].min()\n",
    "else:\n",
    "    up_df = pd.DataFrame({\"video_id\": it[\"video_id\"].drop_duplicates(), \"i_upload_ts\": pd.NaT})\n",
    "\n",
    "# Collapse static fields\n",
    "agg = {c: \"first\" for c in it.columns if c not in [\"video_id\",\"upload_dt\"]}\n",
    "per_video = it.groupby(\"video_id\", as_index=False).agg(agg).merge(up_df, on=\"video_id\", how=\"left\")\n",
    "\n",
    "# Numeric transforms\n",
    "if \"video_duration\" in per_video.columns:\n",
    "    med = pd.to_numeric(per_video[\"video_duration\"], errors=\"coerce\").dropna().median()\n",
    "    dur_is_ms_it = (med > 1000.0) if pd.notna(med) else True\n",
    "    per_video[\"i_video_duration_s\"] = (pd.to_numeric(per_video[\"video_duration\"], errors=\"coerce\") / (1000.0 if dur_is_ms_it else 1.0)).astype(\"float32\")\n",
    "else:\n",
    "    per_video[\"i_video_duration_s\"] = np.nan\n",
    "\n",
    "if {\"video_width\",\"video_height\"}.issubset(per_video.columns):\n",
    "    per_video[\"i_aspect_ratio\"] = (\n",
    "        pd.to_numeric(per_video[\"video_width\"], errors=\"coerce\") / pd.to_numeric(per_video[\"video_height\"], errors=\"coerce\")\n",
    "    ).astype(\"float32\")\n",
    "else:\n",
    "    per_video[\"i_aspect_ratio\"] = np.float32(np.nan)\n",
    "\n",
    "# Categoricals\n",
    "for c in [\"author_id\",\"video_type\",\"upload_type\",\"visible_status\",\"music_id\",\"video_tag_id\",\"video_tag_name\"]:\n",
    "    if c in per_video.columns:\n",
    "        per_video[c] = per_video[c].astype(\"string\").fillna(\"unknown\").replace({\"\": \"unknown\"}).astype(\"category\")\n",
    "        per_video.rename(columns={c: f\"i_{c}\"}, inplace=True)\n",
    "\n",
    "# Merge to df\n",
    "before = set(df.columns)\n",
    "df = df.merge(per_video.rename(columns={\"video_id\":\"_vid\"}), left_on=\"video_id\", right_on=\"_vid\", how=\"left\")\n",
    "if \"_vid\" in df.columns:\n",
    "    df.drop(columns=[\"_vid\"], inplace=True)\n",
    "\n",
    "# Age since upload at interaction time\n",
    "if \"i_upload_ts\" in df.columns:\n",
    "    df[\"i_age_since_upload_days\"] = ((df[\"ts\"] - df[\"i_upload_ts\"]).dt.total_seconds() / 86400.0).astype(\"float32\")\n",
    "else:\n",
    "    df[\"i_age_since_upload_days\"] = np.nan\n",
    "\n",
    "print(\"Added item columns:\", len(df.columns) - len(before))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d450d31",
   "metadata": {},
   "source": [
    "## 5) Add caption/category (`kuairec_caption_category.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4144ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added caption/category columns: 1\n"
     ]
    }
   ],
   "source": [
    "cap_path = BASE / \"kuairec_caption_category.csv\"\n",
    "\n",
    "# Robust stream parse with Python csv to avoid parser errors\n",
    "VID_ALIASES = [\"video_id\", \"item_id\"]\n",
    "CAT_ID_ALIASES = [\"first_level_category_id\",\"category_id_1\",\"top_category_id\"]\n",
    "CAT_NAME_ALIASES = [\"first_level_category_name\",\"category_name_1\",\"top_category_name\"]\n",
    "\n",
    "mapping = {}\n",
    "with open(cap_path, \"r\", encoding=\"utf-8\", errors=\"ignore\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "    def find_idx(cands):\n",
    "        for c in cands:\n",
    "            if c in header: return header.index(c), c\n",
    "        return None, None\n",
    "    vid_idx, vid_name = find_idx(VID_ALIASES)\n",
    "    cid_idx, _ = find_idx(CAT_ID_ALIASES)\n",
    "    cnm_idx, _ = find_idx(CAT_NAME_ALIASES)\n",
    "    if vid_idx is None: raise KeyError(\"No video_id/item_id in caption file.\")\n",
    "    for row in reader:\n",
    "        if not row or len(row) <= vid_idx: continue\n",
    "        vid = (row[vid_idx] or \"\").strip()\n",
    "        if not vid: continue\n",
    "        cat_id = (row[cid_idx].strip() if (cid_idx is not None and len(row)>cid_idx) else \"unknown\")\n",
    "        cat_nm = (row[cnm_idx].strip() if (cnm_idx is not None and len(row)>cnm_idx) else \"unknown\")\n",
    "        if vid not in mapping:\n",
    "            mapping[vid] = (cat_id or \"unknown\", cat_nm or \"unknown\")\n",
    "\n",
    "cat = pd.DataFrame([(k, v[0], v[1]) for k,v in mapping.items()],\n",
    "                   columns=[\"video_id_key\",\"i_top_category_id\",\"i_top_category_name\"])\n",
    "cat[\"video_id_key\"] = cat[\"video_id_key\"].astype(\"string\")\n",
    "cat[\"i_top_category_id\"] = cat[\"i_top_category_id\"].astype(\"string\").replace({\"\": \"unknown\"}).astype(\"category\")\n",
    "cat[\"i_top_category_name\"] = cat[\"i_top_category_name\"].astype(\"string\").replace({\"\": \"unknown\"}).astype(\"category\")\n",
    "\n",
    "df[\"video_id_key\"] = df[\"video_id\"].astype(\"string\").str.strip()\n",
    "before = set(df.columns)\n",
    "df = df.merge(cat, on=\"video_id_key\", how=\"left\")\n",
    "df.drop(columns=[\"video_id_key\"], inplace=True)\n",
    "print(\"Added caption/category columns:\", len(df.columns) - len(before))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330aad55",
   "metadata": {},
   "source": [
    "## 6) Context/time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0053c9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context features added.\n"
     ]
    }
   ],
   "source": [
    "secs = (df[\"ts\"].dt.hour * 3600 + df[\"ts\"].dt.minute * 60 + df[\"ts\"].dt.second).astype(\"float32\")\n",
    "frac_day = secs / np.float32(24*3600)\n",
    "df[\"ctx_hour_sin\"] = np.sin(2*np.pi*frac_day).astype(\"float32\")\n",
    "df[\"ctx_hour_cos\"] = np.cos(2*np.pi*frac_day).astype(\"float32\")\n",
    "df[\"ctx_dow\"] = df[\"ts\"].dt.dayofweek.astype(\"int8\")\n",
    "print(\"Context features added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c6e8c9",
   "metadata": {},
   "source": [
    "## 7) History features (session-level and per-row; leakage-safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb3dfda4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kx/mcwvbnt12wl9fy5xcs5jlnx00000gn/T/ipykernel_5019/1859929821.py:64: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\"within_sess_wr_slope\": _slope_wr_vs_rank(g)}))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History features merged.\n"
     ]
    }
   ],
   "source": [
    "# Ensure sorted\n",
    "df.sort_values([\"user_id\",\"session\",\"ts\"], inplace=True, kind=\"mergesort\")\n",
    "\n",
    "# Per-session aggregates\n",
    "sess = (\n",
    "    df.groupby([\"user_id\",\"session\"], as_index=False)\n",
    "      .agg(\n",
    "          sess_start=(\"ts\",\"min\"),\n",
    "          sess_end=(\"ts\",\"max\"),\n",
    "          sess_len=(\"video_id\",\"size\"),\n",
    "          sess_wr_mean=(\"watch_ratio\",\"mean\"),\n",
    "          sess_wr_var=(\"watch_ratio\",\"var\"),\n",
    "          sess_complete_rate=(\"y_complete\",\"mean\"),\n",
    "      )\n",
    "      .sort_values([\"user_id\",\"session\"], kind=\"mergesort\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "# Session index\n",
    "sess[\"sess_index\"] = sess.groupby(\"user_id\").cumcount().astype(\"int32\") + 1\n",
    "\n",
    "# Previous-session length & gap\n",
    "g_user = sess.groupby(\"user_id\", sort=False)\n",
    "prev_start = g_user[\"sess_start\"].shift(1)\n",
    "prev_end   = g_user[\"sess_end\"].shift(1)\n",
    "sess[\"prev_session_length_min\"] = ((prev_end - prev_start).dt.total_seconds()/60.0).astype(\"float32\")\n",
    "sess[\"inter_session_gap_hours\"] = ((sess[\"sess_start\"] - prev_end).dt.total_seconds()/3600.0).astype(\"float32\")\n",
    "\n",
    "# Rolling histories (t-1)\n",
    "sess[\"hist_last3_complete_rate\"] = (\n",
    "    g_user[\"sess_complete_rate\"].transform(lambda s: s.shift(1).rolling(3, min_periods=1).mean())\n",
    ").astype(\"float32\")\n",
    "sess[\"hist_last10_complete_rate\"] = (\n",
    "    g_user[\"sess_complete_rate\"].transform(lambda s: s.shift(1).rolling(10, min_periods=1).mean())\n",
    ").astype(\"float32\")\n",
    "sess[\"hist_last3_wr_mean\"] = (\n",
    "    g_user[\"sess_wr_mean\"].transform(lambda s: s.shift(1).rolling(3, min_periods=1).mean())\n",
    ").astype(\"float32\")\n",
    "sess[\"hist_last3_wr_var\"] = (\n",
    "    g_user[\"sess_wr_mean\"].transform(lambda s: s.shift(1).rolling(3, min_periods=2).var())\n",
    ").astype(\"float32\")\n",
    "\n",
    "# EMAs up to t-1 (lambda=0.9 default for overall)\n",
    "lam = 0.9; alpha = 1.0 - lam\n",
    "sess[\"hist_ema_complete\"] = (\n",
    "    g_user[\"sess_complete_rate\"].transform(lambda s: s.shift(1).ewm(alpha=alpha, adjust=False).mean())\n",
    ").astype(\"float32\")\n",
    "sess[\"hist_ema_wr_mean\"] = (\n",
    "    g_user[\"sess_wr_mean\"].transform(lambda s: s.shift(1).ewm(alpha=alpha, adjust=False).mean())\n",
    ").astype(\"float32\")\n",
    "\n",
    "# Within-session fatigue slope for previous session\n",
    "def _slope_wr_vs_rank(grp: pd.DataFrame) -> float:\n",
    "    x = grp[\"sess_rank\"].to_numpy(dtype=\"float64\")\n",
    "    y = grp[\"watch_ratio\"].to_numpy(dtype=\"float64\")\n",
    "    n = x.size\n",
    "    if n < 2: return np.nan\n",
    "    xm, ym = x.mean(), y.mean()\n",
    "    den = np.sum((x - xm)**2)\n",
    "    if den <= 0: return np.nan\n",
    "    return float(np.sum((x - xm)*(y - ym)) / den)\n",
    "\n",
    "sl = (\n",
    "    df.groupby([\"user_id\",\"session\"], group_keys=False)\n",
    "      .apply(lambda g: pd.Series({\"within_sess_wr_slope\": _slope_wr_vs_rank(g)}))\n",
    "      .reset_index()\n",
    ")\n",
    "sess = sess.merge(sl, on=[\"user_id\",\"session\"], how=\"left\")\n",
    "sess[\"hist_prev_within_sess_wr_slope\"] = sess.groupby(\"user_id\")[\"within_sess_wr_slope\"].shift(1).astype(\"float32\")\n",
    "\n",
    "# Merge back to df\n",
    "hist_cols = [\"sess_index\",\"prev_session_length_min\",\"inter_session_gap_hours\",\n",
    "             \"hist_last3_complete_rate\",\"hist_last10_complete_rate\",\n",
    "             \"hist_last3_wr_mean\",\"hist_last3_wr_var\",\n",
    "             \"hist_ema_complete\",\"hist_ema_wr_mean\",\"hist_prev_within_sess_wr_slope\"]\n",
    "df = df.merge(sess[[\"user_id\",\"session\"] + hist_cols], on=[\"user_id\",\"session\"], how=\"left\")\n",
    "print(\"History features merged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95fa349a-c2ca-45f2-b896-1d7f9888fed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kx/mcwvbnt12wl9fy5xcs5jlnx00000gn/T/ipykernel_5019/67836695.py:10: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  prev_seen = df.groupby([\"user_id\", \"i_author_id\"], sort=False)[\"ts\"].shift(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added: ['hist_author_recency_days', 'hist_author_recency_log1p']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>i_author_id</th>\n",
       "      <th>ts</th>\n",
       "      <th>hist_author_recency_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7136</td>\n",
       "      <td>2020-07-04 16:08:23.437999964</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2661</td>\n",
       "      <td>2020-07-04 16:13:41.296999931</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1854</td>\n",
       "      <td>2020-07-04 16:16:06.687000036</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5415</td>\n",
       "      <td>2020-07-04 16:20:26.792000055</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>395</td>\n",
       "      <td>2020-07-04 16:43:05.128000020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id i_author_id                            ts  hist_author_recency_days\n",
       "0        0        7136 2020-07-04 16:08:23.437999964                       NaN\n",
       "1        0        2661 2020-07-04 16:13:41.296999931                       NaN\n",
       "2        0        1854 2020-07-04 16:16:06.687000036                       NaN\n",
       "3        0        5415 2020-07-04 16:20:26.792000055                       NaN\n",
       "4        0         395 2020-07-04 16:43:05.128000020                       NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Author recency: time since user i last saw creator j (t-1) ---\n",
    "# Requires: df with ['user_id','i_author_id','ts'] and already sorted by user + ts\n",
    "\n",
    "# Make sure creator id exists and is stable\n",
    "if \"i_author_id\" not in df.columns:\n",
    "    raise KeyError(\"i_author_id not found. Merge item_daily_features first.\")\n",
    "df[\"i_author_id\"] = df[\"i_author_id\"].astype(\"string\").fillna(\"unknown\").astype(\"category\")\n",
    "\n",
    "# Previous seen time per (user, author), using t-1 via shift\n",
    "prev_seen = df.groupby([\"user_id\", \"i_author_id\"], sort=False)[\"ts\"].shift(1)\n",
    "\n",
    "# Recency in days (NaN if never seen this creator before)\n",
    "df[\"hist_author_recency_days\"] = (\n",
    "    (df[\"ts\"] - prev_seen).dt.total_seconds() / 86400.0\n",
    ").astype(\"float32\")\n",
    "\n",
    "# (Optional) a compressed version for tree models\n",
    "df[\"hist_author_recency_log1p\"] = np.log1p(df[\"hist_author_recency_days\"]).astype(\"float32\")\n",
    "\n",
    "print(\"Added:\", [\"hist_author_recency_days\", \"hist_author_recency_log1p\"])\n",
    "df[[\"user_id\",\"i_author_id\",\"ts\",\"hist_author_recency_days\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484bd2ed",
   "metadata": {},
   "source": [
    "## 8) Per-category completion EMA + entropy (λ=0.95; leakage-safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dc57508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kx/mcwvbnt12wl9fy5xcs5jlnx00000gn/T/ipykernel_5019/2245815462.py:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(\"user_id\", group_keys=False).apply(_per_user_cat_ema).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category history features added.\n"
     ]
    }
   ],
   "source": [
    "cat_col = \"i_top_category_id\" if \"i_top_category_id\" in df.columns else (\n",
    "          \"i_top_category_name\" if \"i_top_category_name\" in df.columns else None)\n",
    "if cat_col is None:\n",
    "    raise KeyError(\"Need i_top_category_id or i_top_category_name for category history.\")\n",
    "\n",
    "df.sort_values([\"user_id\",\"ts\"], inplace=True, kind=\"mergesort\")\n",
    "lam = 0.95; alpha = 1.0 - lam\n",
    "\n",
    "def _per_user_cat_ema(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    cat_ema = {}\n",
    "    n = len(g)\n",
    "    ema_cur = np.full(n, np.nan, dtype=\"float32\")\n",
    "    ent_arr = np.full(n, np.nan, dtype=\"float32\")\n",
    "    cats = g[cat_col].astype(\"string\").fillna(\"unknown\").to_numpy()\n",
    "    ys = g[\"y_complete\"].astype(\"float32\").to_numpy()\n",
    "    for i,(c,y) in enumerate(zip(cats, ys)):\n",
    "        prev = cat_ema.get(c, np.nan)\n",
    "        ema_cur[i] = np.float32(prev) if prev == prev else np.nan\n",
    "        if cat_ema:\n",
    "            vals = np.array(list(cat_ema.values()), dtype=\"float64\")\n",
    "            s = vals.sum()\n",
    "            ent_arr[i] = np.float32(-np.sum((vals/s)*np.log(np.clip(vals/s,1e-12,1.0)))) if s>0 else np.nan\n",
    "        else:\n",
    "            ent_arr[i] = np.nan\n",
    "        # update\n",
    "        if prev != prev:\n",
    "            cat_ema[c] = float(y)\n",
    "        else:\n",
    "            cat_ema[c] = (1.0 - alpha) * float(prev) + alpha * float(y)\n",
    "    out = g.copy()\n",
    "    out[\"hist_user_cat_complete_ema\"] = ema_cur\n",
    "    out[\"hist_cat_entropy\"] = ent_arr\n",
    "    return out\n",
    "\n",
    "df = df.groupby(\"user_id\", group_keys=False).apply(_per_user_cat_ema).reset_index(drop=True)\n",
    "print(\"Category history features added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b829936",
   "metadata": {},
   "source": [
    "## 9) Detect bursts (daily activity runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7945b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected bursts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-04</td>\n",
       "      <td>2020-07-12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>2020-08-10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-08-26</td>\n",
       "      <td>2020-09-05</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       start        end  days\n",
       "1 2020-07-04 2020-07-12     9\n",
       "2 2020-07-31 2020-08-10    11\n",
       "3 2020-08-26 2020-09-05    11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "burst_id distribution:\n",
      "burst_id\n",
      "1       2191830\n",
      "2       7007529\n",
      "3       3329672\n",
      "<NA>       1775\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# --- Burst detection (fixed) ---\n",
    "# Idea:\n",
    "# 1) Aggregate daily interaction counts.\n",
    "# 2) Mark an \"active day\" if count >= threshold (thr).\n",
    "# 3) Take the sequence of active-day dates only.\n",
    "# 4) Whenever the gap between consecutive active days > GAP_BETWEEN_ACTIVE, start a new burst.\n",
    "\n",
    "GAP_BETWEEN_ACTIVE = 3   # days between *active* days to start a new burst\n",
    "# You can tighten/loosen this. Larger -> fewer, longer bursts.\n",
    "\n",
    "# Daily counts\n",
    "daily = (\n",
    "    df.set_index(\"ts\")\n",
    "      .groupby(pd.Grouper(freq=\"D\"))[\"user_id\"]\n",
    "      .size()\n",
    "      .rename(\"n\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Threshold for \"active day\"\n",
    "med = daily.loc[daily[\"n\"] > 0, \"n\"].median()\n",
    "thr = max(100, int(0.10 * med)) if pd.notna(med) else 100\n",
    "# ↑ Knob #1: 'thr' filters out low-activity days. Increase if you want stricter bursts.\n",
    "\n",
    "daily[\"is_active\"] = daily[\"n\"] >= thr\n",
    "\n",
    "# Sequence of active-day dates only\n",
    "active_dates = (\n",
    "    daily.loc[daily[\"is_active\"], \"ts\"]\n",
    "         .dt.normalize()\n",
    "         .sort_values()\n",
    "         .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "if active_dates.empty:\n",
    "    raise RuntimeError(\"No active days detected — raise 'thr' or check timestamps.\")\n",
    "\n",
    "# Split whenever the gap between *active* days is large\n",
    "gaps = active_dates.diff().dt.days.fillna(0).astype(\"int32\")\n",
    "burst_group = (gaps > GAP_BETWEEN_ACTIVE).cumsum()\n",
    "\n",
    "# Start/end per burst (fixed: no .to_series())\n",
    "grp = active_dates.groupby(burst_group)\n",
    "bursts = pd.DataFrame({\n",
    "    \"start\": grp.min(),\n",
    "    \"end\":   grp.max(),\n",
    "})\n",
    "bursts[\"days\"] = (bursts[\"end\"] - bursts[\"start\"]).dt.days + 1\n",
    "bursts.index = np.arange(1, len(bursts) + 1)  # burst_id 1..B\n",
    "\n",
    "print(\"Detected bursts:\")\n",
    "display(bursts)\n",
    "\n",
    "# Assign burst_id to every row using closed-left intervals [start, end+1)\n",
    "intervals = pd.IntervalIndex.from_arrays(bursts[\"start\"],\n",
    "                                         bursts[\"end\"] + pd.Timedelta(days=1),\n",
    "                                         closed=\"left\")\n",
    "row_dates = df[\"ts\"].dt.normalize()\n",
    "\n",
    "idx = intervals.get_indexer(row_dates)      # NumPy int array; -1 means \"no interval\"\n",
    "s = pd.Series(idx, index=df.index)          # make it a pandas Series\n",
    "df[\"burst_id\"] = s.where(s >= 0).add(1).astype(\"Int64\")  # 0.. -> 1..; -1 -> <NA>\n",
    "\n",
    "print(\"burst_id distribution:\")\n",
    "print(df[\"burst_id\"].value_counts(dropna=False).sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea84cf5",
   "metadata": {},
   "source": [
    "## 10) Prior resets at starts of bursts 2 & 3 (EMA features only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "994b4734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kx/mcwvbnt12wl9fy5xcs5jlnx00000gn/T/ipykernel_5019/3925910373.py:11: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  c_prior = df.loc[hist_mask].groupby(cat_col)[\"y_complete\"].mean().to_dict()\n",
      "/var/folders/kx/mcwvbnt12wl9fy5xcs5jlnx00000gn/T/ipykernel_5019/3925910373.py:62: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df.loc[mask_b].groupby([\"user_id\",\"burst_id\"], group_keys=False).apply(_apply_ema_with_priors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initialized EMA features at the starts of bursts 2 & 3 using time-truncated priors.\n"
     ]
    }
   ],
   "source": [
    "if \"burst_id\" not in df.columns:\n",
    "    raise RuntimeError(\"burst_id missing — run burst detection first.\")\n",
    "\n",
    "# Compute priors before each burst start\n",
    "priors = {}\n",
    "for b, row in bursts.iterrows():\n",
    "    start = row[\"start\"]\n",
    "    hist_mask = df[\"ts\"] < start\n",
    "    if hist_mask.any():\n",
    "        g_prior = float(df.loc[hist_mask, \"y_complete\"].mean())\n",
    "        c_prior = df.loc[hist_mask].groupby(cat_col)[\"y_complete\"].mean().to_dict()\n",
    "    else:\n",
    "        g_prior = float(df[\"y_complete\"].mean())\n",
    "        c_prior = {}\n",
    "    priors[int(b)] = {\"global\": g_prior, \"per_cat\": c_prior}\n",
    "\n",
    "# Rebuild EMA features for bursts 2 & 3 only\n",
    "target_bursts = [b for b in priors.keys() if b in (2,3)]\n",
    "mask_b = df[\"burst_id\"].isin(target_bursts)\n",
    "\n",
    "def _apply_ema_with_priors(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    b = int(group.name[1])\n",
    "    g_prior = priors[b][\"global\"]\n",
    "    c_prior = priors[b][\"per_cat\"]\n",
    "    g = group.sort_values(\"ts\").copy()\n",
    "\n",
    "    y = g[\"y_complete\"].to_numpy(dtype=\"float32\")\n",
    "    cats = g[cat_col].astype(\"string\").fillna(\"unknown\").to_numpy()\n",
    "\n",
    "    out_overall = np.full(len(g), np.nan, dtype=\"float32\")\n",
    "    out_cat = np.full(len(g), np.nan, dtype=\"float32\")\n",
    "    out_ent = np.full(len(g), np.nan, dtype=\"float32\")\n",
    "\n",
    "    ema_overall = np.nan\n",
    "    ema_cat = {}\n",
    "\n",
    "    lam = 0.95; alpha = 1.0 - lam\n",
    "    for i,(c, yi) in enumerate(zip(cats, y)):\n",
    "        out_overall[i] = ema_overall if ema_overall == ema_overall else np.float32(g_prior)\n",
    "        if c in ema_cat and ema_cat[c] == ema_cat[c]:\n",
    "            out_cat[i] = np.float32(ema_cat[c])\n",
    "        else:\n",
    "            out_cat[i] = np.float32(c_prior.get(c, g_prior))\n",
    "        if ema_cat:\n",
    "            vals = np.array(list(ema_cat.values()), dtype=\"float64\")\n",
    "            s = vals.sum()\n",
    "            out_ent[i] = np.float32(-np.sum((vals/s)*np.log(np.clip(vals/s,1e-12,1.0)))) if s>0 else np.nan\n",
    "        else:\n",
    "            out_ent[i] = np.nan\n",
    "        # update to t\n",
    "        prev_overall = out_overall[i]\n",
    "        ema_overall = (1.0 - alpha) * float(prev_overall) + alpha * float(yi)\n",
    "        prev_cat = out_cat[i]\n",
    "        ema_cat[c] = (1.0 - alpha) * float(prev_cat) + alpha * float(yi)\n",
    "\n",
    "    g[\"hist_ema_complete\"] = out_overall\n",
    "    g[\"hist_user_cat_complete_ema\"] = out_cat\n",
    "    g[\"hist_cat_entropy\"] = out_ent\n",
    "    return g\n",
    "\n",
    "df_reset = (\n",
    "    df.loc[mask_b].groupby([\"user_id\",\"burst_id\"], group_keys=False).apply(_apply_ema_with_priors)\n",
    ")\n",
    "cols_upd = [\"hist_ema_complete\",\"hist_user_cat_complete_ema\",\"hist_cat_entropy\"]\n",
    "df.loc[df_reset.index, cols_upd] = df_reset[cols_upd].values\n",
    "\n",
    "print(\"Re-initialized EMA features at the starts of bursts 2 & 3 using time-truncated priors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a028a",
   "metadata": {},
   "source": [
    "## 11) Clean redundant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e3e07a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 7 redundant columns: ['date', 'i_top_category_name', 'i_upload_ts', 'i_video_tag_name', 'time', 'video_duration_x', 'video_duration_y']\n",
      "Now have 78 columns.\n"
     ]
    }
   ],
   "source": [
    "to_drop = []\n",
    "# raw duration duplicates or helpers\n",
    "for c in [\"video_duration_x\",\"video_duration_y\",\"time\",\"date\"]:\n",
    "    if c in df.columns: to_drop.append(c)\n",
    "# upload_ts (age is derived)\n",
    "if \"i_age_since_upload_days\" in df.columns and \"i_upload_ts\" in df.columns:\n",
    "    to_drop.append(\"i_upload_ts\")\n",
    "# keep IDs; drop names (adjust if you prefer)\n",
    "if \"i_video_tag_id\" in df.columns and \"i_video_tag_name\" in df.columns:\n",
    "    to_drop.append(\"i_video_tag_name\")\n",
    "if \"i_top_category_id\" in df.columns and \"i_top_category_name\" in df.columns:\n",
    "    to_drop.append(\"i_top_category_name\")\n",
    "\n",
    "to_drop = sorted(set(to_drop))\n",
    "df.drop(columns=[c for c in to_drop if c in df.columns], inplace=True, errors=\"ignore\")\n",
    "print(f\"Dropped {len(to_drop)} redundant columns:\", to_drop)\n",
    "print(\"Now have\", df.shape[1], \"columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caa681fc-2b92-4f52-a9f0-61c54cd03c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'video_id',\n",
       " 'play_duration',\n",
       " 'timestamp',\n",
       " 'watch_ratio',\n",
       " 'ts',\n",
       " 'session',\n",
       " 'sess_rank',\n",
       " 'sess_len',\n",
       " 'sess_rank_frac',\n",
       " 'y_complete',\n",
       " 'y_long',\n",
       " 'y_rewatch',\n",
       " 'y_neg',\n",
       " 'u_user_active_degree',\n",
       " 'u_is_lowactive_period',\n",
       " 'u_is_live_streamer',\n",
       " 'u_is_video_author',\n",
       " 'u_follow_user_num',\n",
       " 'u_follow_user_num_range',\n",
       " 'u_fans_user_num',\n",
       " 'u_fans_user_num_range',\n",
       " 'u_friend_user_num',\n",
       " 'u_friend_user_num_range',\n",
       " 'u_register_days',\n",
       " 'u_register_days_range',\n",
       " 'u_onehot_feat0',\n",
       " 'u_onehot_feat1',\n",
       " 'u_onehot_feat2',\n",
       " 'u_onehot_feat3',\n",
       " 'u_onehot_feat4',\n",
       " 'u_onehot_feat5',\n",
       " 'u_onehot_feat6',\n",
       " 'u_onehot_feat7',\n",
       " 'u_onehot_feat8',\n",
       " 'u_onehot_feat9',\n",
       " 'u_onehot_feat10',\n",
       " 'u_onehot_feat11',\n",
       " 'u_onehot_feat12',\n",
       " 'u_onehot_feat13',\n",
       " 'u_onehot_feat14',\n",
       " 'u_onehot_feat15',\n",
       " 'u_onehot_feat16',\n",
       " 'u_onehot_feat17',\n",
       " 'u_follow_user_num_log1p',\n",
       " 'u_fans_user_num_log1p',\n",
       " 'u_friend_user_num_log1p',\n",
       " 'u_register_days_log1p',\n",
       " 'i_author_id',\n",
       " 'i_video_type',\n",
       " 'i_upload_type',\n",
       " 'i_visible_status',\n",
       " 'video_width',\n",
       " 'video_height',\n",
       " 'i_music_id',\n",
       " 'i_video_tag_id',\n",
       " 'i_video_duration_s',\n",
       " 'i_aspect_ratio',\n",
       " 'i_age_since_upload_days',\n",
       " 'i_top_category_id',\n",
       " 'ctx_hour_sin',\n",
       " 'ctx_hour_cos',\n",
       " 'ctx_dow',\n",
       " 'sess_index',\n",
       " 'prev_session_length_min',\n",
       " 'inter_session_gap_hours',\n",
       " 'hist_last3_complete_rate',\n",
       " 'hist_last10_complete_rate',\n",
       " 'hist_last3_wr_mean',\n",
       " 'hist_last3_wr_var',\n",
       " 'hist_ema_complete',\n",
       " 'hist_ema_wr_mean',\n",
       " 'hist_prev_within_sess_wr_slope',\n",
       " 'hist_author_recency_days',\n",
       " 'hist_author_recency_log1p',\n",
       " 'hist_user_cat_complete_ema',\n",
       " 'hist_cat_entropy',\n",
       " 'burst_id']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df.columns), \"columns\")\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4b56c",
   "metadata": {},
   "source": [
    "## 12) Save outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e18d9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "- /Users/haozhangao/Desktop/RecSys Research/KuaiRec 2.0/data/prepared/features_full.parquet\n",
      "- /Users/haozhangao/Desktop/RecSys Research/KuaiRec 2.0/data/prepared/features_full.csv.gz\n",
      "Rows: 12530806 Cols: 78\n"
     ]
    }
   ],
   "source": [
    "parquet_path = OUT / \"features_full.parquet\"\n",
    "csv_path = OUT / \"features_full.csv.gz\"\n",
    "\n",
    "df.to_parquet(parquet_path, index=False)\n",
    "df.to_csv(csv_path, index=False, compression=\"gzip\")\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"-\", parquet_path)\n",
    "print(\"-\", csv_path)\n",
    "print(\"Rows:\", len(df), \"Cols:\", len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c6d3c5-43bc-415c-9e91-ed7f61d86e25",
   "metadata": {},
   "source": [
    "## 13) Create empirical category distribuion and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90f9fe3a-226f-4395-9465-9d40d5ccf2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kx/mcwvbnt12wl9fy5xcs5jlnx00000gn/T/ipykernel_5019/1021953351.py:21: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = (expo.groupby([\"user_id\",\"session\",cat_col], as_index=False)\n",
      "/var/folders/kx/mcwvbnt12wl9fy5xcs5jlnx00000gn/T/ipykernel_5019/1021953351.py:66: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  emp_wide = (emp.pivot_table(index=[\"user_id\",\"session\"],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved wide distribution to: /Users/haozhangao/Desktop/RecSys Research/KuaiRec 2.0/data/prepared/empirical_category_distribution_wide.csv\n",
      "Example per-session sum(p_empirical_cat_sm): [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Saved empirical per-session category distribution to: /Users/haozhangao/Desktop/RecSys Research/KuaiRec 2.0/data/prepared/empirical_category_distribution.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session</th>\n",
       "      <th>i_top_category_id</th>\n",
       "      <th>n_exposed</th>\n",
       "      <th>n_total</th>\n",
       "      <th>p_empirical_cat</th>\n",
       "      <th>K_session</th>\n",
       "      <th>p_empirical_cat_sm</th>\n",
       "      <th>enough_expo</th>\n",
       "      <th>empirical_cat_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-124</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>False</td>\n",
       "      <td>1.592372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>False</td>\n",
       "      <td>1.592372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>False</td>\n",
       "      <td>1.592372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>False</td>\n",
       "      <td>1.592372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>False</td>\n",
       "      <td>1.592372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>False</td>\n",
       "      <td>1.612163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>False</td>\n",
       "      <td>1.612163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>False</td>\n",
       "      <td>1.612163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>False</td>\n",
       "      <td>1.612163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>False</td>\n",
       "      <td>1.612163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  session i_top_category_id  n_exposed  n_total  p_empirical_cat  \\\n",
       "0         0        1              -124          0        6              0.0   \n",
       "1         0        1                 1          0        6              0.0   \n",
       "2         0        1                10          0        6              0.0   \n",
       "3         0        1                11          0        6              0.0   \n",
       "4         0        1                12          0        6              0.0   \n",
       "..      ...      ...               ...        ...      ...              ...   \n",
       "95        0        3                22          0        1              0.0   \n",
       "96        0        3                23          0        1              0.0   \n",
       "97        0        3                24          0        1              0.0   \n",
       "98        0        3                25          0        1              0.0   \n",
       "99        0        3                26          0        1              0.0   \n",
       "\n",
       "    K_session  p_empirical_cat_sm  enough_expo  empirical_cat_entropy  \n",
       "0          40            0.001563        False               1.592372  \n",
       "1          40            0.001563        False               1.592372  \n",
       "2          40            0.001563        False               1.592372  \n",
       "3          40            0.001563        False               1.592372  \n",
       "4          40            0.001563        False               1.592372  \n",
       "..        ...                 ...          ...                    ...  \n",
       "95         40            0.007143        False               1.612163  \n",
       "96         40            0.007143        False               1.612163  \n",
       "97         40            0.007143        False               1.612163  \n",
       "98         40            0.007143        False               1.612163  \n",
       "99         40            0.007143        False               1.612163  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Empirical category distribution per (user, session) ===\n",
    "# Assumes df has: ['user_id','session','ts', 'i_top_category_id' or 'i_top_category_name']\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Pick the category column (ID preferred, fallback to name)\n",
    "cat_col = None\n",
    "for c in [\"i_top_category_id\", \"i_top_category_name\"]:\n",
    "    if c in df.columns:\n",
    "        cat_col = c\n",
    "        break\n",
    "if cat_col is None:\n",
    "    raise KeyError(\"No category column found. Expected i_top_category_id or i_top_category_name in df.\")\n",
    "\n",
    "# 2) Treat each row as an exposure; drop missing category\n",
    "expo = df.loc[df[cat_col].notna(), [\"user_id\",\"session\",cat_col,\"ts\"]].copy()\n",
    "expo.sort_values([\"user_id\",\"session\",\"ts\"], inplace=True, kind=\"mergesort\")\n",
    "\n",
    "# 3) Counts per (user, session, category)\n",
    "g = (expo.groupby([\"user_id\",\"session\",cat_col], as_index=False)\n",
    "         .size()\n",
    "         .rename(columns={\"size\":\"n_exposed\"}))\n",
    "\n",
    "# 4) Totals per (user, session)\n",
    "tot = (g.groupby([\"user_id\",\"session\"], as_index=False)[\"n_exposed\"]\n",
    "         .sum()\n",
    "         .rename(columns={\"n_exposed\":\"n_total\"}))\n",
    "\n",
    "# 5) Join + raw probabilities\n",
    "emp = g.merge(tot, on=[\"user_id\",\"session\"], how=\"left\")\n",
    "emp[\"p_empirical_cat\"] = (emp[\"n_exposed\"] / emp[\"n_total\"]).astype(\"float32\")\n",
    "\n",
    "# 5.5) Dirichlet/Laplace smoothing per session\n",
    "eta = 1e-2  # try 1e-3 .. 1e-1\n",
    "K = (emp.groupby([\"user_id\",\"session\"], as_index=False)[cat_col]\n",
    "       .nunique()\n",
    "       .rename(columns={cat_col:\"K_session\"}))\n",
    "emp = emp.merge(K, on=[\"user_id\",\"session\"], how=\"left\")\n",
    "emp[\"p_empirical_cat_sm\"] = (\n",
    "    (emp[\"n_exposed\"] + eta) / (emp[\"n_total\"] + eta * emp[\"K_session\"])\n",
    ").astype(\"float32\")\n",
    "\n",
    "# 6) Quality flag + entropy (use smoothed probs by default)\n",
    "min_exposures = 3\n",
    "emp[\"enough_expo\"] = emp[\"n_total\"] >= min_exposures\n",
    "\n",
    "def _entropy(ps):\n",
    "    p = np.asarray(ps, dtype=\"float64\")\n",
    "    p = p[(p > 0) & np.isfinite(p)]\n",
    "    if p.size == 0:\n",
    "        return np.nan\n",
    "    return float(-np.sum(p * np.log(p)))\n",
    "\n",
    "prob_col = \"p_empirical_cat_sm\"  # switch to \"p_empirical_cat\" for raw\n",
    "sess_entropy = (emp.groupby([\"user_id\",\"session\"])[prob_col]\n",
    "                  .apply(_entropy)\n",
    "                  .reset_index()\n",
    "                  .rename(columns={prob_col:\"empirical_cat_entropy\"}))\n",
    "emp = emp.merge(sess_entropy, on=[\"user_id\",\"session\"], how=\"left\")\n",
    "\n",
    "# 7) Optional wide pivot (one column per category)\n",
    "num_cats = int(emp[cat_col].nunique())\n",
    "emp_wide = None\n",
    "if num_cats <= 200:  # adjust threshold if needed\n",
    "    emp_wide = (emp.pivot_table(index=[\"user_id\",\"session\"],\n",
    "                                columns=cat_col,\n",
    "                                values=prob_col,\n",
    "                                fill_value=0.0)\n",
    "                  .reset_index())\n",
    "\n",
    "# 8) Save artifacts\n",
    "OUT_PREP = BASE / \"prepared\"\n",
    "OUT_PREP.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "emp_path = OUT_PREP / \"empirical_category_distribution.csv\"\n",
    "emp.to_csv(emp_path, index=False)\n",
    "\n",
    "if emp_wide is not None:\n",
    "    emp_wide_path = OUT_PREP / \"empirical_category_distribution_wide.csv\"\n",
    "    emp_wide.to_csv(emp_wide_path, index=False)\n",
    "    print(f\"Saved wide distribution to: {emp_wide_path}\")\n",
    "\n",
    "# 9) Quick sanity checks & preview\n",
    "#   - per-session probs (smoothed) should sum ~1\n",
    "chk = (emp.groupby([\"user_id\",\"session\"])[\"p_empirical_cat_sm\"].sum()\n",
    "         .reset_index(drop=True).head(5).round(6).tolist())\n",
    "print(\"Example per-session sum(p_empirical_cat_sm):\", chk)\n",
    "print(f\"Saved empirical per-session category distribution to: {emp_path}\")\n",
    "display(emp.head(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe96fc3-def7-43ba-a5dd-4570eb9ecc71",
   "metadata": {},
   "source": [
    "### Example: empirical distribution of the 1st user's 1st session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34d5e08a-4a23-4cc8-879a-04b1b310dd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 0 — Session 11 | exposures = 59 | entropy = 2.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kx/mcwvbnt12wl9fy5xcs5jlnx00000gn/T/ipykernel_5019/3077497698.py:16: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  tab = (sess_df.groupby(cat_col, as_index=False).size()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_top_category_id</th>\n",
       "      <th>n</th>\n",
       "      <th>p_empirical_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>0.186441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>0.101695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.084746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.067797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0.067797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.067797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.050847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.050847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-124</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i_top_category_id   n  p_empirical_cat\n",
       "21                28  11         0.186441\n",
       "27                34   6         0.101695\n",
       "34                 5   5         0.084746\n",
       "37                 8   4         0.067797\n",
       "3                 11   4         0.067797\n",
       "2                 10   4         0.067797\n",
       "35                 6   3         0.050847\n",
       "18                25   3         0.050847\n",
       "26                33   2         0.033898\n",
       "25                32   2         0.033898\n",
       "24                31   2         0.033898\n",
       "1                  1   2         0.033898\n",
       "11                19   2         0.033898\n",
       "38                 9   1         0.016949\n",
       "7                 15   1         0.016949\n",
       "8                 16   1         0.016949\n",
       "36                 7   1         0.016949\n",
       "22                29   1         0.016949\n",
       "4                 12   1         0.016949\n",
       "16                23   1         0.016949\n",
       "29                36   1         0.016949\n",
       "19                26   1         0.016949\n",
       "0               -124   0         0.000000\n",
       "30                37   0         0.000000\n",
       "31                38   0         0.000000\n",
       "28                35   0         0.000000\n",
       "33                 4   0         0.000000\n",
       "32                39   0         0.000000\n",
       "20                27   0         0.000000\n",
       "23                 3   0         0.000000\n",
       "17                24   0         0.000000\n",
       "15                22   0         0.000000\n",
       "14                21   0         0.000000\n",
       "13                20   0         0.000000\n",
       "12                 2   0         0.000000\n",
       "10                18   0         0.000000\n",
       "9                 17   0         0.000000\n",
       "6                 14   0         0.000000\n",
       "5                 13   0         0.000000\n",
       "39           unknown   0         0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Empirical category distribution for FIRST user's FIRST session\n",
    "\n",
    "# 1) find first user & their first session\n",
    "df_srt = df.sort_values([\"user_id\",\"sess_index\",\"ts\"], kind=\"mergesort\")\n",
    "u0 = df_srt[\"user_id\"].iloc[0]\n",
    "s0 = df_srt.loc[df_srt[\"user_id\"] == u0, \"session\"].min()+10\n",
    "\n",
    "# 2) pick a category column\n",
    "cat_col = next((c for c in [\"i_top_category_id\",\"i_top_category_name\"] if c in df.columns), None)\n",
    "if cat_col is None:\n",
    "    raise KeyError(\"No category column found (looked for i_top_category_* or i_video_tag_*).\")\n",
    "\n",
    "# 3) slice the session & compute distribution\n",
    "sess_df = (df[(df[\"user_id\"] == u0) & (df[\"session\"] == s0)]\n",
    "             .sort_values(\"ts\", kind=\"mergesort\"))\n",
    "tab = (sess_df.groupby(cat_col, as_index=False).size()\n",
    "                  .rename(columns={\"size\":\"n\"}))\n",
    "tab[\"p_empirical_cat\"] = (tab[\"n\"] / tab[\"n\"].sum()).astype(\"float32\")\n",
    "\n",
    "# (optional) entropy of the session’s category mix\n",
    "p = tab[\"p_empirical_cat\"].to_numpy(dtype=\"float64\")\n",
    "entropy = float(-np.sum(p[p>0] * np.log(p[p>0])))\n",
    "\n",
    "print(f\"User {u0} — Session {s0} | exposures = {len(sess_df)} | entropy = {entropy:0.3f}\")\n",
    "display(tab.sort_values(\"p_empirical_cat\", ascending=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
