{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0f21ddf-7bb6-4053-bb79-9676e6ffa42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Train 4 LightGBM heads on Burst 1 (3 train days / 1 valid day / 5 left for inference later) ====\n",
    "from pathlib import Path\n",
    "import json, os, math, numpy as np, pandas as pd, lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, log_loss\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f040cca-b481-4270-902f-f8c8aff47837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Paths ----------\n",
    "BASE   = Path(\"/Users/haozhangao/Desktop/RecSys Research/KuaiRec 2.0/data\")\n",
    "PREP   = BASE / \"prepared\" / 。\n",
    "OUTDIR = BASE / \"models\" / \"burst1\"\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Load prepared features ----------\n",
    "df = pd.read_parquet(PREP)\n",
    "assert \"burst_id\" in df.columns, \"burst_id missing. Run the data-prep notebook first.\"\n",
    "assert \"ts\" in df.columns, \"ts missing (datetime).\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5577f17-5f42-4969-baa2-ffe53b4fe00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burst 1 days: ['2020-07-04', '2020-07-05', '2020-07-06', '2020-07-07', '2020-07-08', '2020-07-09', '2020-07-10', '2020-07-11', '2020-07-12']\n",
      "Train days  : ['2020-07-04', '2020-07-05', '2020-07-06']\n",
      "Valid day   : ['2020-07-07']\n",
      "Hold (5d)   : ['2020-07-08', '2020-07-09', '2020-07-10', '2020-07-11', '2020-07-12']\n"
     ]
    }
   ],
   "source": [
    "# ---------- Identify Burst 1 9 days and split 3/1/5 ----------\n",
    "b1 = df[df[\"burst_id\"] == 1].copy()\n",
    "b1[\"day\"] = b1[\"ts\"].dt.normalize()\n",
    "days = np.sort(b1[\"day\"].unique())\n",
    "if len(days) < 9:\n",
    "    raise ValueError(f\"Burst 1 has {len(days)} days; need at least 9 for 3/1/5 split.\")\n",
    "train_days = days[:3]\n",
    "valid_days = days[3:4]\n",
    "hold_days  = days[4:9]    # we won't use these here (reserved for inference step later)\n",
    "\n",
    "print(\"Burst 1 days:\", pd.Series(days).dt.strftime(\"%Y-%m-%d\").tolist())\n",
    "print(\"Train days  :\", pd.Series(train_days).dt.strftime(\"%Y-%m-%d\").tolist())\n",
    "print(\"Valid day   :\", pd.Series(valid_days).dt.strftime(\"%Y-%m-%d\").tolist())\n",
    "print(\"Hold (5d)   :\", pd.Series(hold_days).dt.strftime(\"%Y-%m-%d\").tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e4abd15-9c32-4680-b9ff-cd7ebdd451c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 feature cols  |  removed leaks: ['play_duration', 'sess_len', 'sess_rank', 'sess_rank_frac', 'watch_ratio']\n"
     ]
    }
   ],
   "source": [
    "# ---------- Build feature list (NO LEAKAGE) ----------\n",
    "label_cols = [\"y_complete\",\"y_long\",\"y_rewatch\",\"y_neg\"]\n",
    "\n",
    "# IDs / helpers we never train on\n",
    "key_cols   = [\"user_id\",\"video_id\",\"timestamp\",\"ts\",\"day\",\"session\"]\n",
    "\n",
    "# columns that leak the target or use future info\n",
    "leak_cols = {\n",
    "    \"play_duration\",      # outcome\n",
    "    \"watch_ratio\",        # outcome\n",
    "    \"sess_rank\",          # needs full session\n",
    "    \"sess_len\",           # needs full session\n",
    "    \"sess_rank_frac\",     # needs full session\n",
    "}\n",
    "\n",
    "drop_cols = set(label_cols + key_cols) | leak_cols\n",
    "feat_cols = [c for c in b1.columns if c not in drop_cols]\n",
    "\n",
    "# mark categoricals (keep pandas 'category' dtype)\n",
    "cat_cols = [c for c in feat_cols\n",
    "            if getattr(b1[c].dtype, \"name\", \"\") == \"category\" or str(b1[c].dtype) == \"category\"]\n",
    "\n",
    "print(f\"{len(feat_cols)} feature cols  |  removed leaks: {sorted(leak_cols & set(b1.columns))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da4f1af8-ca05-4d18-ac0b-40ad3844d2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_complete: train pos=349572/489948, valid pos=190256/270269\n",
      "y_long: train pos=239061/489948, valid pos=127574/270269\n",
      "y_rewatch: train pos=44867/489948, valid pos=23392/270269\n",
      "y_neg: train pos=62558/489948, valid pos=35086/270269\n"
     ]
    }
   ],
   "source": [
    "# ---------- Make splits ----------\n",
    "train = b1[b1[\"day\"].isin(train_days)]\n",
    "valid = b1[b1[\"day\"].isin(valid_days)]\n",
    "\n",
    "# Small sanity\n",
    "for y in label_cols:\n",
    "    print(f\"{y}: train pos={int(train[y].sum())}/{len(train)}, valid pos={int(valid[y].sum())}/{len(valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d10ee6e-0df8-47dd-bedb-d9a0fd058b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u_user_active_degree',\n",
       " 'u_is_lowactive_period',\n",
       " 'u_is_live_streamer',\n",
       " 'u_is_video_author',\n",
       " 'u_follow_user_num',\n",
       " 'u_follow_user_num_range',\n",
       " 'u_fans_user_num',\n",
       " 'u_fans_user_num_range',\n",
       " 'u_friend_user_num',\n",
       " 'u_friend_user_num_range',\n",
       " 'u_register_days',\n",
       " 'u_register_days_range',\n",
       " 'u_onehot_feat0',\n",
       " 'u_onehot_feat1',\n",
       " 'u_onehot_feat2',\n",
       " 'u_onehot_feat3',\n",
       " 'u_onehot_feat4',\n",
       " 'u_onehot_feat5',\n",
       " 'u_onehot_feat6',\n",
       " 'u_onehot_feat7',\n",
       " 'u_onehot_feat8',\n",
       " 'u_onehot_feat9',\n",
       " 'u_onehot_feat10',\n",
       " 'u_onehot_feat11',\n",
       " 'u_onehot_feat12',\n",
       " 'u_onehot_feat13',\n",
       " 'u_onehot_feat14',\n",
       " 'u_onehot_feat15',\n",
       " 'u_onehot_feat16',\n",
       " 'u_onehot_feat17',\n",
       " 'u_follow_user_num_log1p',\n",
       " 'u_fans_user_num_log1p',\n",
       " 'u_friend_user_num_log1p',\n",
       " 'u_register_days_log1p',\n",
       " 'i_author_id',\n",
       " 'i_video_type',\n",
       " 'i_upload_type',\n",
       " 'i_visible_status',\n",
       " 'video_width',\n",
       " 'video_height',\n",
       " 'i_music_id',\n",
       " 'i_video_tag_id',\n",
       " 'i_video_duration_s',\n",
       " 'i_aspect_ratio',\n",
       " 'i_age_since_upload_days',\n",
       " 'i_top_category_id',\n",
       " 'ctx_hour_sin',\n",
       " 'ctx_hour_cos',\n",
       " 'ctx_dow',\n",
       " 'sess_index',\n",
       " 'prev_session_length_min',\n",
       " 'inter_session_gap_hours',\n",
       " 'hist_last3_complete_rate',\n",
       " 'hist_last10_complete_rate',\n",
       " 'hist_last3_wr_mean',\n",
       " 'hist_last3_wr_var',\n",
       " 'hist_ema_complete',\n",
       " 'hist_ema_wr_mean',\n",
       " 'hist_prev_within_sess_wr_slope',\n",
       " 'hist_author_recency_days',\n",
       " 'hist_author_recency_log1p',\n",
       " 'hist_user_cat_complete_ema',\n",
       " 'hist_cat_entropy',\n",
       " 'burst_id']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "778da455-9dda-4eba-ac84-b64b3ddb954b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04f68232-69aa-4df6-93e9-70a51edf538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Training helper (per head) ----------\n",
    "def train_head(label: str):\n",
    "    X_tr, y_tr = train[feat_cols], train[label].astype(\"int8\")\n",
    "    X_va, y_va = valid[feat_cols], valid[label].astype(\"int8\")\n",
    "\n",
    "    # scale_pos_weight from train only\n",
    "    pos = int(y_tr.sum()); neg = int(len(y_tr) - pos)\n",
    "    spw = (neg / max(pos, 1)) if pos > 0 else 1.0\n",
    "\n",
    "    dtrain = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_cols, free_raw_data=False)\n",
    "    dvalid = lgb.Dataset(X_va, label=y_va, categorical_feature=cat_cols, reference=dtrain, free_raw_data=False)\n",
    "\n",
    "    params = dict(\n",
    "        objective=\"binary\",\n",
    "        metric=\"auc\",\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=127,\n",
    "        max_depth=7,\n",
    "        min_data_in_leaf=200,\n",
    "        feature_fraction=0.8,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=1,\n",
    "        lambda_l2=2.0,\n",
    "        max_bin=255,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1,\n",
    "        scale_pos_weight=spw,\n",
    "        seed=2025,\n",
    "    )\n",
    "\n",
    "    # ---> use callbacks for early stopping + logging (compatible with older LightGBM)\n",
    "    callbacks = [\n",
    "        lgb.early_stopping(stopping_rounds=100, verbose=True),\n",
    "        lgb.log_evaluation(period=200),\n",
    "    ]\n",
    "\n",
    "    booster = lgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=4000,\n",
    "        valid_sets=[dtrain, dvalid],\n",
    "        valid_names=[\"train\",\"valid\"],\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    # predictions\n",
    "    pred_tr_raw = booster.predict(X_tr, num_iteration=booster.best_iteration)\n",
    "    pred_va_raw = booster.predict(X_va, num_iteration=booster.best_iteration)\n",
    "    margin_va   = booster.predict(X_va, num_iteration=booster.best_iteration, raw_score=True)\n",
    "\n",
    "    def safe_metrics(y, p):\n",
    "        if y.nunique() < 2:\n",
    "            return dict(auc=float(\"nan\"), ap=float(\"nan\"), logloss=float(\"nan\"))\n",
    "        return dict(\n",
    "            auc=roc_auc_score(y, p),\n",
    "            ap=average_precision_score(y, p),\n",
    "            logloss=log_loss(y, np.clip(p, 1e-6, 1-1e-6)),\n",
    "        )\n",
    "\n",
    "    m_tr_raw = safe_metrics(y_tr, pred_tr_raw)\n",
    "    m_va_raw = safe_metrics(y_va, pred_va_raw)\n",
    "\n",
    "    # Platt calibration on valid\n",
    "    calibrator = {\"type\": \"platt\", \"coef\": 1.0, \"intercept\": 0.0}\n",
    "    if y_va.nunique() == 2 and (np.std(margin_va) > 1e-8):\n",
    "        lr = LogisticRegression(solver=\"lbfgs\", C=1e6, max_iter=1000, fit_intercept=True)\n",
    "        lr.fit(margin_va.reshape(-1,1), y_va.to_numpy())\n",
    "        a = float(lr.coef_.ravel()[0]); b = float(lr.intercept_.ravel()[0])\n",
    "        calibrator.update({\"coef\": a, \"intercept\": b})\n",
    "        pred_va_cal = 1.0 / (1.0 + np.exp(-(a*margin_va + b)))\n",
    "        m_va_cal = safe_metrics(y_va, pred_va_cal)\n",
    "    else:\n",
    "        m_va_cal = m_va_raw\n",
    "\n",
    "    # save artifacts\n",
    "    head_dir = OUTDIR / label\n",
    "    head_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model_path = head_dir / f\"lgb_b1_{label}.txt\"\n",
    "    booster.save_model(str(model_path), num_iteration=booster.best_iteration)\n",
    "\n",
    "    imp = pd.DataFrame({\n",
    "        \"feature\": booster.feature_name(),\n",
    "        \"gain\": booster.feature_importance(importance_type=\"gain\"),\n",
    "        \"split\": booster.feature_importance(importance_type=\"split\"),\n",
    "    }).sort_values(\"gain\", ascending=False)\n",
    "    imp_path = head_dir / f\"feature_importance.csv\"\n",
    "    imp.to_csv(imp_path, index=False)\n",
    "\n",
    "    meta = {\n",
    "        \"burst\": 1,\n",
    "        \"label\": label,\n",
    "        \"best_iteration\": int(booster.best_iteration),\n",
    "        \"class_balance\": {\"pos\": pos, \"neg\": neg, \"scale_pos_weight\": spw},\n",
    "        \"metrics\": {\"train_raw\": m_tr_raw, \"valid_raw\": m_va_raw, \"valid_cal\": m_va_cal},\n",
    "        \"params\": params,\n",
    "        \"n_features\": len(feat_cols),\n",
    "        \"n_categorical\": len(cat_cols),\n",
    "        \"train_days\": pd.Series(train_days).dt.strftime(\"%Y-%m-%d\").tolist(),\n",
    "        \"valid_days\": pd.Series(valid_days).dt.strftime(\"%Y-%m-%d\").tolist(),\n",
    "        \"hold_days\":  pd.Series(hold_days).dt.strftime(\"%Y-%m-%d\").tolist(),\n",
    "        \"model_path\": str(model_path),\n",
    "        \"importance_path\": str(imp_path),\n",
    "    }\n",
    "    (head_dir / \"meta.json\").write_text(json.dumps(meta, indent=2))\n",
    "    (head_dir / \"calibrator.json\").write_text(json.dumps(calibrator, indent=2))\n",
    "    (head_dir / \"features.json\").write_text(json.dumps({\"feat_cols\": feat_cols, \"cat_cols\": cat_cols}, indent=2))\n",
    "\n",
    "    print(f\"\\n[{label}] best_iter={meta['best_iteration']}  \"\n",
    "          f\"Train AUC={m_tr_raw['auc']:.4f} | \"\n",
    "          f\"Valid AUC raw={m_va_raw['auc']:.4f} cal={m_va_cal['auc']:.4f}\")\n",
    "    print(f\"[{label}] Valid AP raw={m_va_raw['ap']:.4f} cal={m_va_cal['ap']:.4f}   \"\n",
    "          f\"LogLoss raw={m_va_raw['logloss']:.4f} cal={m_va_cal['logloss']:.4f}\")\n",
    "    print(f\"Saved → {model_path}\\n\")\n",
    "\n",
    "    return booster, meta, imp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ff14e80-dad7-40fa-a200-3f906d4b4780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "Training: y_complete\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.876591\tvalid's auc: 0.846778\n",
      "Early stopping, best iteration is:\n",
      "[195]\ttrain's auc: 0.876031\tvalid's auc: 0.84681\n",
      "\n",
      "[y_complete] best_iter=195  Train AUC=0.8760 | Valid AUC raw=0.8468 cal=0.8468\n",
      "[y_complete] Valid AP raw=0.9259 cal=0.9259   LogLoss raw=0.4702 cal=0.4263\n",
      "Saved → /Users/haozhangao/Desktop/RecSys Research/KuaiRec 2.0/data/models/burst1/y_complete/lgb_b1_y_complete.txt\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "Training: y_long\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.848966\tvalid's auc: 0.819844\n",
      "[400]\ttrain's auc: 0.86547\tvalid's auc: 0.820828\n",
      "Early stopping, best iteration is:\n",
      "[469]\ttrain's auc: 0.869725\tvalid's auc: 0.820953\n",
      "\n",
      "[y_long] best_iter=469  Train AUC=0.8697 | Valid AUC raw=0.8210 cal=0.8210\n",
      "[y_long] Valid AP raw=0.8075 cal=0.8075   LogLoss raw=0.5147 cal=0.5143\n",
      "Saved → /Users/haozhangao/Desktop/RecSys Research/KuaiRec 2.0/data/models/burst1/y_long/lgb_b1_y_long.txt\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "Training: y_rewatch\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[82]\ttrain's auc: 0.875048\tvalid's auc: 0.826469\n",
      "\n",
      "[y_rewatch] best_iter=82  Train AUC=0.8750 | Valid AUC raw=0.8265 cal=0.8265\n",
      "[y_rewatch] Valid AP raw=0.3597 cal=0.3597   LogLoss raw=0.4327 cal=0.2318\n",
      "Saved → /Users/haozhangao/Desktop/RecSys Research/KuaiRec 2.0/data/models/burst1/y_rewatch/lgb_b1_y_rewatch.txt\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "Training: y_neg\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.880909\tvalid's auc: 0.819139\n",
      "Early stopping, best iteration is:\n",
      "[293]\ttrain's auc: 0.896533\tvalid's auc: 0.820575\n",
      "\n",
      "[y_neg] best_iter=293  Train AUC=0.8965 | Valid AUC raw=0.8206 cal=0.8206\n",
      "[y_neg] Valid AP raw=0.4445 cal=0.4445   LogLoss raw=0.4485 cal=0.3021\n",
      "Saved → /Users/haozhangao/Desktop/RecSys Research/KuaiRec 2.0/data/models/burst1/y_neg/lgb_b1_y_neg.txt\n",
      "\n",
      "\n",
      "Done (Burst 1 training + calibration). Next step would be inference on days D5–D9.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Train all four heads (no inference yet) ----------\n",
    "models = {}\n",
    "for label in [\"y_complete\",\"y_long\",\"y_rewatch\",\"y_neg\"]:\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(f\"Training: {label}\")\n",
    "    booster, meta, imp = train_head(label)\n",
    "    models[label] = {\"model\": booster, \"meta\": meta, \"importance\": imp}\n",
    "\n",
    "print(\"\\nDone (Burst 1 training + calibration). Next step would be inference on days D5–D9.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76cb1ae5-4a30-4428-9b4e-405571c15671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Burst-1 Holdout (Days D5–D9) — Overall ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>AUC_raw</th>\n",
       "      <th>AUC_cal</th>\n",
       "      <th>AP_raw</th>\n",
       "      <th>AP_cal</th>\n",
       "      <th>LogLoss_raw</th>\n",
       "      <th>LogLoss_cal</th>\n",
       "      <th>Brier_raw</th>\n",
       "      <th>Brier_cal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y_complete</td>\n",
       "      <td>0.8447</td>\n",
       "      <td>0.8447</td>\n",
       "      <td>0.9266</td>\n",
       "      <td>0.9266</td>\n",
       "      <td>0.4681</td>\n",
       "      <td>0.4248</td>\n",
       "      <td>0.1545</td>\n",
       "      <td>0.1379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>y_long</td>\n",
       "      <td>0.8086</td>\n",
       "      <td>0.8086</td>\n",
       "      <td>0.7949</td>\n",
       "      <td>0.7949</td>\n",
       "      <td>0.5294</td>\n",
       "      <td>0.5292</td>\n",
       "      <td>0.1782</td>\n",
       "      <td>0.1781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>y_rewatch</td>\n",
       "      <td>0.8123</td>\n",
       "      <td>0.8123</td>\n",
       "      <td>0.3388</td>\n",
       "      <td>0.3388</td>\n",
       "      <td>0.4246</td>\n",
       "      <td>0.2393</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0.0678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>y_neg</td>\n",
       "      <td>0.8095</td>\n",
       "      <td>0.8095</td>\n",
       "      <td>0.4115</td>\n",
       "      <td>0.4115</td>\n",
       "      <td>0.4474</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>0.1493</td>\n",
       "      <td>0.0914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         head  AUC_raw  AUC_cal  AP_raw  AP_cal  LogLoss_raw  LogLoss_cal  \\\n",
       "0  y_complete   0.8447   0.8447  0.9266  0.9266       0.4681       0.4248   \n",
       "1      y_long   0.8086   0.8086  0.7949  0.7949       0.5294       0.5292   \n",
       "2   y_rewatch   0.8123   0.8123  0.3388  0.3388       0.4246       0.2393   \n",
       "3       y_neg   0.8095   0.8095  0.4115  0.4115       0.4474       0.3040   \n",
       "\n",
       "   Brier_raw  Brier_cal  \n",
       "0     0.1545     0.1379  \n",
       "1     0.1782     0.1781  \n",
       "2     0.1343     0.0678  \n",
       "3     0.1493     0.0914  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Per-day metrics (calibrated) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>day</th>\n",
       "      <th>AUC_cal</th>\n",
       "      <th>AP_cal</th>\n",
       "      <th>LogLoss_cal</th>\n",
       "      <th>Brier_cal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y_complete</td>\n",
       "      <td>2020-07-08</td>\n",
       "      <td>0.8557</td>\n",
       "      <td>0.9308</td>\n",
       "      <td>0.4154</td>\n",
       "      <td>0.1346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>y_complete</td>\n",
       "      <td>2020-07-09</td>\n",
       "      <td>0.8473</td>\n",
       "      <td>0.9272</td>\n",
       "      <td>0.4237</td>\n",
       "      <td>0.1378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>y_complete</td>\n",
       "      <td>2020-07-10</td>\n",
       "      <td>0.8488</td>\n",
       "      <td>0.9252</td>\n",
       "      <td>0.4239</td>\n",
       "      <td>0.1372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>y_complete</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>0.8411</td>\n",
       "      <td>0.9291</td>\n",
       "      <td>0.4216</td>\n",
       "      <td>0.1365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>y_complete</td>\n",
       "      <td>2020-07-12</td>\n",
       "      <td>0.8243</td>\n",
       "      <td>0.9176</td>\n",
       "      <td>0.4443</td>\n",
       "      <td>0.1451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>y_long</td>\n",
       "      <td>2020-07-08</td>\n",
       "      <td>0.8171</td>\n",
       "      <td>0.8050</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>0.1742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>y_long</td>\n",
       "      <td>2020-07-09</td>\n",
       "      <td>0.8118</td>\n",
       "      <td>0.7927</td>\n",
       "      <td>0.5243</td>\n",
       "      <td>0.1763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>y_long</td>\n",
       "      <td>2020-07-10</td>\n",
       "      <td>0.8110</td>\n",
       "      <td>0.7945</td>\n",
       "      <td>0.5261</td>\n",
       "      <td>0.1770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>y_long</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>0.8046</td>\n",
       "      <td>0.7980</td>\n",
       "      <td>0.5344</td>\n",
       "      <td>0.1802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>y_long</td>\n",
       "      <td>2020-07-12</td>\n",
       "      <td>0.7940</td>\n",
       "      <td>0.7798</td>\n",
       "      <td>0.5450</td>\n",
       "      <td>0.1846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>y_neg</td>\n",
       "      <td>2020-07-08</td>\n",
       "      <td>0.8181</td>\n",
       "      <td>0.4163</td>\n",
       "      <td>0.2949</td>\n",
       "      <td>0.0886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>y_neg</td>\n",
       "      <td>2020-07-09</td>\n",
       "      <td>0.8197</td>\n",
       "      <td>0.4360</td>\n",
       "      <td>0.2969</td>\n",
       "      <td>0.0893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>y_neg</td>\n",
       "      <td>2020-07-10</td>\n",
       "      <td>0.8065</td>\n",
       "      <td>0.4073</td>\n",
       "      <td>0.3068</td>\n",
       "      <td>0.0924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>y_neg</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>0.8074</td>\n",
       "      <td>0.4051</td>\n",
       "      <td>0.3041</td>\n",
       "      <td>0.0911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>y_neg</td>\n",
       "      <td>2020-07-12</td>\n",
       "      <td>0.7899</td>\n",
       "      <td>0.3836</td>\n",
       "      <td>0.3225</td>\n",
       "      <td>0.0974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>y_rewatch</td>\n",
       "      <td>2020-07-08</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.3411</td>\n",
       "      <td>0.2356</td>\n",
       "      <td>0.0669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>y_rewatch</td>\n",
       "      <td>2020-07-09</td>\n",
       "      <td>0.8124</td>\n",
       "      <td>0.3354</td>\n",
       "      <td>0.2331</td>\n",
       "      <td>0.0656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>y_rewatch</td>\n",
       "      <td>2020-07-10</td>\n",
       "      <td>0.8206</td>\n",
       "      <td>0.3516</td>\n",
       "      <td>0.2369</td>\n",
       "      <td>0.0673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>y_rewatch</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.3450</td>\n",
       "      <td>0.2411</td>\n",
       "      <td>0.0684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>y_rewatch</td>\n",
       "      <td>2020-07-12</td>\n",
       "      <td>0.7921</td>\n",
       "      <td>0.3147</td>\n",
       "      <td>0.2543</td>\n",
       "      <td>0.0719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          head         day  AUC_cal  AP_cal  LogLoss_cal  Brier_cal\n",
       "0   y_complete  2020-07-08   0.8557  0.9308       0.4154     0.1346\n",
       "1   y_complete  2020-07-09   0.8473  0.9272       0.4237     0.1378\n",
       "2   y_complete  2020-07-10   0.8488  0.9252       0.4239     0.1372\n",
       "3   y_complete  2020-07-11   0.8411  0.9291       0.4216     0.1365\n",
       "4   y_complete  2020-07-12   0.8243  0.9176       0.4443     0.1451\n",
       "5       y_long  2020-07-08   0.8171  0.8050       0.5200     0.1742\n",
       "6       y_long  2020-07-09   0.8118  0.7927       0.5243     0.1763\n",
       "7       y_long  2020-07-10   0.8110  0.7945       0.5261     0.1770\n",
       "8       y_long  2020-07-11   0.8046  0.7980       0.5344     0.1802\n",
       "9       y_long  2020-07-12   0.7940  0.7798       0.5450     0.1846\n",
       "15       y_neg  2020-07-08   0.8181  0.4163       0.2949     0.0886\n",
       "16       y_neg  2020-07-09   0.8197  0.4360       0.2969     0.0893\n",
       "17       y_neg  2020-07-10   0.8065  0.4073       0.3068     0.0924\n",
       "18       y_neg  2020-07-11   0.8074  0.4051       0.3041     0.0911\n",
       "19       y_neg  2020-07-12   0.7899  0.3836       0.3225     0.0974\n",
       "10   y_rewatch  2020-07-08   0.8177  0.3411       0.2356     0.0669\n",
       "11   y_rewatch  2020-07-09   0.8124  0.3354       0.2331     0.0656\n",
       "12   y_rewatch  2020-07-10   0.8206  0.3516       0.2369     0.0673\n",
       "13   y_rewatch  2020-07-11   0.8132  0.3450       0.2411     0.0684\n",
       "14   y_rewatch  2020-07-12   0.7921  0.3147       0.2543     0.0719"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved overall and per-day metrics to: /Users/haozhangao/Desktop/RecSys Research/KuaiRec 2.0/data/models/burst1/evaluation\n"
     ]
    }
   ],
   "source": [
    "# ==== Test (Holdout) evaluation on Burst-1 Days D5–D9 ====\n",
    "import json, numpy as np, pandas as pd, lightgbm as lgb\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, log_loss, brier_score_loss\n",
    "\n",
    "# 0) Build test slice\n",
    "test = b1[b1[\"day\"].isin(hold_days)].copy()\n",
    "X_te = test[feat_cols]\n",
    "\n",
    "def load_and_score(head: str):\n",
    "    \"\"\"Load the trained head model + Platt calibrator, and return raw & calibrated probs on the test set.\"\"\"\n",
    "    head_dir = OUTDIR / head\n",
    "    booster = lgb.Booster(model_file=str(head_dir / f\"lgb_b1_{head}.txt\"))\n",
    "    with open(head_dir / \"calibrator.json\") as f:\n",
    "        calib = json.load(f)\n",
    "\n",
    "    # raw prob and raw margin on test\n",
    "    p_raw = booster.predict(X_te, num_iteration=booster.best_iteration)\n",
    "    margin = booster.predict(X_te, num_iteration=booster.best_iteration, raw_score=True)\n",
    "\n",
    "    # Platt calibration\n",
    "    a = float(calib.get(\"coef\", 1.0))\n",
    "    b = float(calib.get(\"intercept\", 0.0))\n",
    "    p_cal = 1.0 / (1.0 + np.exp(-(a * margin + b)))\n",
    "    return booster, p_raw, p_cal\n",
    "\n",
    "def metrics(y, p):\n",
    "    y = y.astype(\"int8\").to_numpy()\n",
    "    p = np.clip(p, 1e-6, 1-1e-6)\n",
    "    out = {}\n",
    "    if y.min() != y.max():\n",
    "        out[\"auc\"] = roc_auc_score(y, p)\n",
    "        out[\"ap\"]  = average_precision_score(y, p)\n",
    "    else:\n",
    "        out[\"auc\"] = np.nan\n",
    "        out[\"ap\"]  = np.nan\n",
    "    out[\"logloss\"] = log_loss(y, p)\n",
    "    out[\"brier\"]   = brier_score_loss(y, p)\n",
    "    return out\n",
    "\n",
    "# 1) Overall metrics on test (D5–D9)\n",
    "report = {}\n",
    "for head in [\"y_complete\",\"y_long\",\"y_rewatch\",\"y_neg\"]:\n",
    "    _, p_raw, p_cal = load_and_score(head)\n",
    "    y = test[head]\n",
    "    report[head] = {\"raw\": metrics(y, p_raw), \"cal\": metrics(y, p_cal)}\n",
    "\n",
    "# 2) Per-day metrics (use calibrated probs)\n",
    "per_day_rows = []\n",
    "for head in [\"y_complete\",\"y_long\",\"y_rewatch\",\"y_neg\"]:\n",
    "    _, _, p_cal = load_and_score(head)\n",
    "    tmp = test[[\"day\", head]].copy()\n",
    "    tmp[\"p_cal\"] = p_cal\n",
    "    for d, g in tmp.groupby(\"day\"):\n",
    "        m = metrics(g[head], g[\"p_cal\"])\n",
    "        per_day_rows.append([head, d.strftime(\"%Y-%m-%d\"), m[\"auc\"], m[\"ap\"], m[\"logloss\"], m[\"brier\"]])\n",
    "\n",
    "# 3) Pretty print\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:0.4f}\")\n",
    "rows = []\n",
    "for head in [\"y_complete\",\"y_long\",\"y_rewatch\",\"y_neg\"]:\n",
    "    rows.append([\n",
    "        head,\n",
    "        report[head][\"raw\"][\"auc\"],   report[head][\"cal\"][\"auc\"],\n",
    "        report[head][\"raw\"][\"ap\"],    report[head][\"cal\"][\"ap\"],\n",
    "        report[head][\"raw\"][\"logloss\"], report[head][\"cal\"][\"logloss\"],\n",
    "        report[head][\"raw\"][\"brier\"],   report[head][\"cal\"][\"brier\"],\n",
    "    ])\n",
    "test_table = pd.DataFrame(rows, columns=[\n",
    "    \"head\",\"AUC_raw\",\"AUC_cal\",\"AP_raw\",\"AP_cal\",\"LogLoss_raw\",\"LogLoss_cal\",\"Brier_raw\",\"Brier_cal\"\n",
    "])\n",
    "\n",
    "print(\"=== Burst-1 Holdout (Days D5–D9) — Overall ===\")\n",
    "display(test_table)\n",
    "\n",
    "per_day = pd.DataFrame(per_day_rows, columns=[\"head\",\"day\",\"AUC_cal\",\"AP_cal\",\"LogLoss_cal\",\"Brier_cal\"])\n",
    "print(\"\\n=== Per-day metrics (calibrated) ===\")\n",
    "display(per_day.sort_values([\"head\",\"day\"]))\n",
    "\n",
    "# 4) Save artifacts\n",
    "metrics_dir = OUTDIR / \"evaluation\"\n",
    "metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "test_table.to_csv(metrics_dir / \"test_overall.csv\", index=False)\n",
    "per_day.to_csv(metrics_dir / \"test_per_day.csv\", index=False)\n",
    "\n",
    "with open(metrics_dir / \"test_overall.json\", \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved overall and per-day metrics to: {metrics_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb4ff6af-9c51-43f1-bd38-253e80cb98bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>play_duration</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>watch_ratio</th>\n",
       "      <th>ts</th>\n",
       "      <th>session</th>\n",
       "      <th>sess_rank</th>\n",
       "      <th>sess_len</th>\n",
       "      <th>sess_rank_frac</th>\n",
       "      <th>...</th>\n",
       "      <th>hist_last3_wr_var</th>\n",
       "      <th>hist_ema_complete</th>\n",
       "      <th>hist_ema_wr_mean</th>\n",
       "      <th>hist_prev_within_sess_wr_slope</th>\n",
       "      <th>hist_author_recency_days</th>\n",
       "      <th>hist_author_recency_log1p</th>\n",
       "      <th>hist_user_cat_complete_ema</th>\n",
       "      <th>hist_cat_entropy</th>\n",
       "      <th>burst_id</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3649</td>\n",
       "      <td>13838</td>\n",
       "      <td>1593878903.4380</td>\n",
       "      <td>1.2734</td>\n",
       "      <td>2020-07-04 16:08:23.437999964</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>9598</td>\n",
       "      <td>13665</td>\n",
       "      <td>1593879221.2970</td>\n",
       "      <td>1.2441</td>\n",
       "      <td>2020-07-04 16:13:41.296999931</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5262</td>\n",
       "      <td>851</td>\n",
       "      <td>1593879366.6870</td>\n",
       "      <td>0.1076</td>\n",
       "      <td>2020-07-04 16:16:06.687000036</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1963</td>\n",
       "      <td>862</td>\n",
       "      <td>1593879626.7920</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>2020-07-04 16:20:26.792000055</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8234</td>\n",
       "      <td>858</td>\n",
       "      <td>1593880985.1280</td>\n",
       "      <td>0.0780</td>\n",
       "      <td>2020-07-04 16:43:05.128000020</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  video_id  play_duration       timestamp  watch_ratio  \\\n",
       "0        0      3649          13838 1593878903.4380       1.2734   \n",
       "1        0      9598          13665 1593879221.2970       1.2441   \n",
       "2        0      5262            851 1593879366.6870       0.1076   \n",
       "3        0      1963            862 1593879626.7920       0.0899   \n",
       "4        0      8234            858 1593880985.1280       0.0780   \n",
       "\n",
       "                             ts  session  sess_rank  sess_len  sess_rank_frac  \\\n",
       "0 2020-07-04 16:08:23.437999964        1          1         6          0.1667   \n",
       "1 2020-07-04 16:13:41.296999931        1          2         6          0.3333   \n",
       "2 2020-07-04 16:16:06.687000036        1          3         6          0.5000   \n",
       "3 2020-07-04 16:20:26.792000055        1          4         6          0.6667   \n",
       "4 2020-07-04 16:43:05.128000020        1          5         6          0.8333   \n",
       "\n",
       "   ...  hist_last3_wr_var  hist_ema_complete  hist_ema_wr_mean  \\\n",
       "0  ...                NaN                NaN               NaN   \n",
       "1  ...                NaN                NaN               NaN   \n",
       "2  ...                NaN                NaN               NaN   \n",
       "3  ...                NaN                NaN               NaN   \n",
       "4  ...                NaN                NaN               NaN   \n",
       "\n",
       "   hist_prev_within_sess_wr_slope hist_author_recency_days  \\\n",
       "0                             NaN                      NaN   \n",
       "1                             NaN                      NaN   \n",
       "2                             NaN                      NaN   \n",
       "3                             NaN                      NaN   \n",
       "4                             NaN                      NaN   \n",
       "\n",
       "   hist_author_recency_log1p  hist_user_cat_complete_ema  hist_cat_entropy  \\\n",
       "0                        NaN                         NaN               NaN   \n",
       "1                        NaN                      1.0000           -0.0000   \n",
       "2                        NaN                         NaN           -0.0000   \n",
       "3                        NaN                      1.0000           -0.0000   \n",
       "4                        NaN                         NaN           -0.0000   \n",
       "\n",
       "   burst_id        day  \n",
       "0         1 2020-07-04  \n",
       "1         1 2020-07-04  \n",
       "2         1 2020-07-04  \n",
       "3         1 2020-07-04  \n",
       "4         1 2020-07-04  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b0ea2be-53a0-450c-9096-a97970c1a860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sessions: 152456\n",
      "         day  n_sessions\n",
      "0 2020-07-04        6421\n",
      "1 2020-07-05       20834\n",
      "2 2020-07-06       20988\n",
      "3 2020-07-07       20707\n",
      "4 2020-07-08       20970\n",
      "5 2020-07-09       20722\n",
      "6 2020-07-10       20541\n",
      "7 2020-07-11       20831\n",
      "8 2020-07-12       14677\n",
      "Sessions with ≥10 exposures: 62152\n",
      "Unique users: 7041\n",
      "Unique videos shown: 2919\n",
      "Unique days: 9\n"
     ]
    }
   ],
   "source": [
    "# total number of (user, session) pairs\n",
    "total_sessions = b1.drop_duplicates([\"user_id\", \"session\"]).shape[0]\n",
    "print(\"Total sessions:\", total_sessions)\n",
    "\n",
    "# sessions per day (using calendar day from ts)\n",
    "sess_per_day = (\n",
    "    b1.assign(day=b1[\"ts\"].dt.normalize())\n",
    "      .drop_duplicates([\"user_id\", \"session\", \"day\"])\n",
    "      .groupby(\"day\").size().rename(\"n_sessions\").reset_index()\n",
    ")\n",
    "print(sess_per_day)\n",
    "\n",
    "# sessions with at least 10 exposures\n",
    "sess_sizes = b1.groupby([\"user_id\",\"session\"]).size().rename(\"n_exposures\")\n",
    "n10 = (sess_sizes >= 10).sum()\n",
    "print(\"Sessions with ≥10 exposures:\", n10)\n",
    "\n",
    "\n",
    "# Unique users in b1\n",
    "n_users = b1[\"user_id\"].nunique()\n",
    "\n",
    "# Unique videos that appeared (shown) in b1\n",
    "n_videos = b1[\"video_id\"].nunique()\n",
    "\n",
    "print(\"Unique users:\", n_users)\n",
    "print(\"Unique videos shown:\", n_videos)\n",
    "\n",
    "\n",
    "n_days = b1[\"day\"].nunique()\n",
    "print(\"Unique days:\", n_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe9117a8-0543-4b77-8aea-fc183d999d71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'video_id',\n",
       " 'play_duration',\n",
       " 'timestamp',\n",
       " 'watch_ratio',\n",
       " 'ts',\n",
       " 'session',\n",
       " 'sess_rank',\n",
       " 'sess_len',\n",
       " 'sess_rank_frac',\n",
       " 'y_complete',\n",
       " 'y_long',\n",
       " 'y_rewatch',\n",
       " 'y_neg',\n",
       " 'u_user_active_degree',\n",
       " 'u_is_lowactive_period',\n",
       " 'u_is_live_streamer',\n",
       " 'u_is_video_author',\n",
       " 'u_follow_user_num',\n",
       " 'u_follow_user_num_range',\n",
       " 'u_fans_user_num',\n",
       " 'u_fans_user_num_range',\n",
       " 'u_friend_user_num',\n",
       " 'u_friend_user_num_range',\n",
       " 'u_register_days',\n",
       " 'u_register_days_range',\n",
       " 'u_onehot_feat0',\n",
       " 'u_onehot_feat1',\n",
       " 'u_onehot_feat2',\n",
       " 'u_onehot_feat3',\n",
       " 'u_onehot_feat4',\n",
       " 'u_onehot_feat5',\n",
       " 'u_onehot_feat6',\n",
       " 'u_onehot_feat7',\n",
       " 'u_onehot_feat8',\n",
       " 'u_onehot_feat9',\n",
       " 'u_onehot_feat10',\n",
       " 'u_onehot_feat11',\n",
       " 'u_onehot_feat12',\n",
       " 'u_onehot_feat13',\n",
       " 'u_onehot_feat14',\n",
       " 'u_onehot_feat15',\n",
       " 'u_onehot_feat16',\n",
       " 'u_onehot_feat17',\n",
       " 'u_follow_user_num_log1p',\n",
       " 'u_fans_user_num_log1p',\n",
       " 'u_friend_user_num_log1p',\n",
       " 'u_register_days_log1p',\n",
       " 'i_author_id',\n",
       " 'i_video_type',\n",
       " 'i_upload_type',\n",
       " 'i_visible_status',\n",
       " 'video_width',\n",
       " 'video_height',\n",
       " 'i_music_id',\n",
       " 'i_video_tag_id',\n",
       " 'i_video_duration_s',\n",
       " 'i_aspect_ratio',\n",
       " 'i_age_since_upload_days',\n",
       " 'i_top_category_id',\n",
       " 'ctx_hour_sin',\n",
       " 'ctx_hour_cos',\n",
       " 'ctx_dow',\n",
       " 'sess_index',\n",
       " 'prev_session_length_min',\n",
       " 'inter_session_gap_hours',\n",
       " 'hist_last3_complete_rate',\n",
       " 'hist_last10_complete_rate',\n",
       " 'hist_last3_wr_mean',\n",
       " 'hist_last3_wr_var',\n",
       " 'hist_ema_complete',\n",
       " 'hist_ema_wr_mean',\n",
       " 'hist_prev_within_sess_wr_slope',\n",
       " 'hist_author_recency_days',\n",
       " 'hist_author_recency_log1p',\n",
       " 'hist_user_cat_complete_ema',\n",
       " 'hist_cat_entropy',\n",
       " 'burst_id',\n",
       " 'day']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(b1.columns), \"columns\")\n",
    "list(b1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed8bb3ed-f425-4d3a-ab65-705d6cb76ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kx/mcwvbnt12wl9fy5xcs5jlnx00000gn/T/ipykernel_12303/3363662624.py:19: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = (expo.groupby([\"user_id\",\"session\",cat_col], as_index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved empirical per-session category distribution → /Users/haozhangao/Desktop/RecSys Research/KuaiRec 2.0/data/prepared/empirical_category_distribution.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session</th>\n",
       "      <th>i_top_category_id</th>\n",
       "      <th>n_exposed</th>\n",
       "      <th>n_total</th>\n",
       "      <th>K_session</th>\n",
       "      <th>p_empirical_cat</th>\n",
       "      <th>p_empirical_cat_sm</th>\n",
       "      <th>enough_expo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-124</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  session i_top_category_id  n_exposed  n_total  K_session  \\\n",
       "0        0        1              -124          0        6         40   \n",
       "1        0        1                 1          0        6         40   \n",
       "2        0        1                10          0        6         40   \n",
       "3        0        1                11          0        6         40   \n",
       "4        0        1                12          0        6         40   \n",
       "5        0        1                13          0        6         40   \n",
       "6        0        1                14          0        6         40   \n",
       "7        0        1                15          0        6         40   \n",
       "8        0        1                16          0        6         40   \n",
       "9        0        1                17          0        6         40   \n",
       "\n",
       "   p_empirical_cat  p_empirical_cat_sm  enough_expo  \n",
       "0           0.0000              0.0016        False  \n",
       "1           0.0000              0.0016        False  \n",
       "2           0.0000              0.0016        False  \n",
       "3           0.0000              0.0016        False  \n",
       "4           0.0000              0.0016        False  \n",
       "5           0.0000              0.0016        False  \n",
       "6           0.0000              0.0016        False  \n",
       "7           0.0000              0.0016        False  \n",
       "8           0.0000              0.0016        False  \n",
       "9           0.0000              0.0016        False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build empirical distribution for each session\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Config\n",
    "cat_col = \"i_top_category_id\"     # use the ID column for categories\n",
    "eta = 1e-2                        # smoothing\n",
    "min_exposures = 10                 # flag \"trustworthy\" sessions\n",
    "OUT_PREP = Path(OUTDIR).parents[1] / \"prepared\"\n",
    "OUT_PREP.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Treat each row in b1 as an exposure; keep (user, session, category)\n",
    "expo = b1.loc[b1[cat_col].notna(), [\"user_id\",\"session\",cat_col,\"ts\"]].copy()\n",
    "expo.sort_values([\"user_id\",\"session\",\"ts\"], inplace=True, kind=\"mergesort\")\n",
    "\n",
    "# 2) Counts per (user, session, category) and totals per (user, session)\n",
    "g = (expo.groupby([\"user_id\",\"session\",cat_col], as_index=False)\n",
    "          .size().rename(columns={\"size\":\"n_exposed\"}))\n",
    "tot = (g.groupby([\"user_id\",\"session\"], as_index=False)[\"n_exposed\"]\n",
    "         .sum().rename(columns={\"n_exposed\":\"n_total\"}))\n",
    "\n",
    "emp = g.merge(tot, on=[\"user_id\",\"session\"], how=\"left\")\n",
    "\n",
    "# 3) #categories observed in the session (for smoothed denominator)\n",
    "K = (emp.groupby([\"user_id\",\"session\"], observed=True)[cat_col]\n",
    "        .nunique()\n",
    "        .rename(\"K_session\")\n",
    "        .reset_index())\n",
    "emp = emp.merge(K, on=[\"user_id\",\"session\"], how=\"left\")\n",
    "\n",
    "# 4) Raw and smoothed probabilities\n",
    "emp[\"p_empirical_cat\"] = (emp[\"n_exposed\"] / emp[\"n_total\"]).astype(\"float32\")\n",
    "emp[\"p_empirical_cat_sm\"] = (\n",
    "    (emp[\"n_exposed\"] + eta) / (emp[\"n_total\"] + eta * emp[\"K_session\"])\n",
    ").astype(\"float32\")\n",
    "\n",
    "# 5) Quality flag: enough impressions in the session\n",
    "emp[\"enough_expo\"] = emp[\"n_total\"] >= min_exposures\n",
    "\n",
    "# Save + quick peek\n",
    "emp_path = OUT_PREP / \"empirical_category_distribution.csv\"\n",
    "emp.to_csv(emp_path, index=False)\n",
    "print(f\"Saved empirical per-session category distribution → {emp_path}\")\n",
    "display(emp.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cba2ab9-daac-4bef-84ee-e80da31e64f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate feature matrix for (user, session) = (0, 1)\n",
      "Shape: (2919, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kx/mcwvbnt12wl9fy5xcs5jlnx00000gn/T/ipykernel_12303/452101178.py:143: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if c in b1.columns and pd.api.types.is_categorical_dtype(b1[c].dtype):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_user_active_degree</th>\n",
       "      <th>u_is_lowactive_period</th>\n",
       "      <th>u_is_live_streamer</th>\n",
       "      <th>u_is_video_author</th>\n",
       "      <th>u_follow_user_num</th>\n",
       "      <th>u_follow_user_num_range</th>\n",
       "      <th>u_fans_user_num</th>\n",
       "      <th>u_fans_user_num_range</th>\n",
       "      <th>u_friend_user_num</th>\n",
       "      <th>u_friend_user_num_range</th>\n",
       "      <th>...</th>\n",
       "      <th>hist_last3_wr_mean</th>\n",
       "      <th>hist_last3_wr_var</th>\n",
       "      <th>hist_ema_complete</th>\n",
       "      <th>hist_ema_wr_mean</th>\n",
       "      <th>hist_prev_within_sess_wr_slope</th>\n",
       "      <th>hist_author_recency_days</th>\n",
       "      <th>hist_author_recency_log1p</th>\n",
       "      <th>hist_user_cat_complete_ema</th>\n",
       "      <th>hist_cat_entropy</th>\n",
       "      <th>burst_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>high_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>(0,10]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365.0000</td>\n",
       "      <td>5.9026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>(0,10]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365.0000</td>\n",
       "      <td>5.9026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>(0,10]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365.0000</td>\n",
       "      <td>5.9026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>(0,10]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365.0000</td>\n",
       "      <td>5.9026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>(0,10]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365.0000</td>\n",
       "      <td>5.9026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  u_user_active_degree  u_is_lowactive_period  u_is_live_streamer  \\\n",
       "0          high_active                      0                   0   \n",
       "1          high_active                      0                   0   \n",
       "2          high_active                      0                   0   \n",
       "3          high_active                      0                   0   \n",
       "4          high_active                      0                   0   \n",
       "\n",
       "   u_is_video_author  u_follow_user_num u_follow_user_num_range  \\\n",
       "0                  0             5.0000                  (0,10]   \n",
       "1                  0             5.0000                  (0,10]   \n",
       "2                  0             5.0000                  (0,10]   \n",
       "3                  0             5.0000                  (0,10]   \n",
       "4                  0             5.0000                  (0,10]   \n",
       "\n",
       "   u_fans_user_num u_fans_user_num_range  u_friend_user_num  \\\n",
       "0           0.0000                     0             0.0000   \n",
       "1           0.0000                     0             0.0000   \n",
       "2           0.0000                     0             0.0000   \n",
       "3           0.0000                     0             0.0000   \n",
       "4           0.0000                     0             0.0000   \n",
       "\n",
       "  u_friend_user_num_range  ...  hist_last3_wr_mean hist_last3_wr_var  \\\n",
       "0                       0  ...                 NaN               NaN   \n",
       "1                       0  ...                 NaN               NaN   \n",
       "2                       0  ...                 NaN               NaN   \n",
       "3                       0  ...                 NaN               NaN   \n",
       "4                       0  ...                 NaN               NaN   \n",
       "\n",
       "  hist_ema_complete hist_ema_wr_mean hist_prev_within_sess_wr_slope  \\\n",
       "0               NaN              NaN                            NaN   \n",
       "1               NaN              NaN                            NaN   \n",
       "2               NaN              NaN                            NaN   \n",
       "3               NaN              NaN                            NaN   \n",
       "4               NaN              NaN                            NaN   \n",
       "\n",
       "  hist_author_recency_days hist_author_recency_log1p  \\\n",
       "0                 365.0000                    5.9026   \n",
       "1                 365.0000                    5.9026   \n",
       "2                 365.0000                    5.9026   \n",
       "3                 365.0000                    5.9026   \n",
       "4                 365.0000                    5.9026   \n",
       "\n",
       "  hist_user_cat_complete_ema hist_cat_entropy burst_id  \n",
       "0                        NaN              NaN        1  \n",
       "1                        NaN              NaN        1  \n",
       "2                        NaN              NaN        1  \n",
       "3                        NaN              NaN        1  \n",
       "4                        NaN              NaN        1  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- prerequisites assumed to exist ---\n",
    "# b1: DataFrame of burst-1 exposures with all 79 cols you listed\n",
    "# feat_cols: the 64 feature names used to train LightGBM (exact list/order below)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- 1) pick first user's first session ----------\n",
    "first_pair = (b1[[\"user_id\",\"session\",\"ts\"]]\n",
    "              .sort_values([\"user_id\",\"session\",\"ts\"])\n",
    "              .drop_duplicates(subset=[\"user_id\",\"session\"])\n",
    "              .iloc[0])\n",
    "u0 = int(first_pair[\"user_id\"])\n",
    "s0 = int(first_pair[\"session\"])\n",
    "\n",
    "g = (b1[(b1.user_id==u0) & (b1.session==s0)]\n",
    "       .sort_values(\"ts\", kind=\"mergesort\")\n",
    "       .reset_index(drop=True))\n",
    "if g.empty:\n",
    "    raise RuntimeError(\"No rows for first user's first session\")\n",
    "\n",
    "# session anchor time (use session start)\n",
    "t_session = pd.Timestamp(g[\"ts\"].iloc[0])\n",
    "\n",
    "# ---------- 2) build a per-video item bank from b1 (keeps dtypes) ----------\n",
    "item_cols = [\"video_id\",\"i_author_id\",\"i_video_type\",\"i_upload_type\",\"i_visible_status\",\n",
    "             \"video_width\",\"video_height\",\"i_music_id\",\"i_video_tag_id\",\"i_video_duration_s\",\n",
    "             \"i_aspect_ratio\",\"i_top_category_id\",\"i_age_since_upload_days\",\"ts\"]\n",
    "# keep only available columns\n",
    "item_cols = [c for c in item_cols if c in b1.columns]\n",
    "\n",
    "item_bank = (b1[item_cols]\n",
    "             .sort_values(\"ts\", kind=\"mergesort\")\n",
    "             .drop_duplicates(subset=[\"video_id\"], keep=\"first\")\n",
    "             .reset_index(drop=True))\n",
    "\n",
    "# estimate each video's upload time ≈ first_seen_ts - first_seen_age_days\n",
    "if {\"ts\",\"i_age_since_upload_days\"}.issubset(item_bank.columns):\n",
    "    upload_ts_est = item_bank[\"ts\"] - pd.to_timedelta(item_bank[\"i_age_since_upload_days\"].astype(float), unit=\"D\")\n",
    "    item_bank = item_bank.assign(_upload_ts_est=upload_ts_est)\n",
    "else:\n",
    "    # fallback if age column missing: assume \"today\" at session time (age=0)\n",
    "    item_bank = item_bank.assign(_upload_ts_est=pd.Timestamp(t_session))\n",
    "\n",
    "# we won't keep the helper columns in final matrix\n",
    "# (but keep them around here for recomputing per-session item age)\n",
    "# ---------- 3) session-level features to broadcast (exactly as in b1) ----------\n",
    "sess_row = g.iloc[0]  # one row from this session to copy/broadcast session/user/history fields\n",
    "\n",
    "broadcast_cols = [\n",
    " 'u_user_active_degree','u_is_lowactive_period','u_is_live_streamer','u_is_video_author',\n",
    " 'u_follow_user_num','u_follow_user_num_range','u_fans_user_num','u_fans_user_num_range',\n",
    " 'u_friend_user_num','u_friend_user_num_range','u_register_days','u_register_days_range',\n",
    " 'u_onehot_feat0','u_onehot_feat1','u_onehot_feat2','u_onehot_feat3','u_onehot_feat4',\n",
    " 'u_onehot_feat5','u_onehot_feat6','u_onehot_feat7','u_onehot_feat8','u_onehot_feat9',\n",
    " 'u_onehot_feat10','u_onehot_feat11','u_onehot_feat12','u_onehot_feat13','u_onehot_feat14',\n",
    " 'u_onehot_feat15','u_onehot_feat16','u_onehot_feat17','u_follow_user_num_log1p',\n",
    " 'u_fans_user_num_log1p','u_friend_user_num_log1p','u_register_days_log1p',\n",
    " 'ctx_hour_sin','ctx_hour_cos','ctx_dow','sess_index','prev_session_length_min',\n",
    " 'inter_session_gap_hours','hist_last3_complete_rate','hist_last10_complete_rate',\n",
    " 'hist_last3_wr_mean','hist_last3_wr_var','hist_ema_complete','hist_ema_wr_mean',\n",
    " 'hist_prev_within_sess_wr_slope','hist_user_cat_complete_ema','hist_cat_entropy','burst_id'\n",
    "]\n",
    "\n",
    "# compute/overwrite ctx_* from actual session time to be safe\n",
    "hour = t_session.hour + t_session.minute/60.0\n",
    "angle = 2*np.pi*hour/24.0\n",
    "ctx_hour_sin = np.sin(angle)\n",
    "ctx_hour_cos = np.cos(angle)\n",
    "ctx_dow = t_session.dayofweek\n",
    "\n",
    "# build a dict of broadcast values with correct dtypes\n",
    "bvals = {}\n",
    "for c in broadcast_cols:\n",
    "    if c in (\"ctx_hour_sin\",\"ctx_hour_cos\",\"ctx_dow\"):\n",
    "        continue\n",
    "    if c in g.columns:\n",
    "        bvals[c] = g[c].iloc[0]\n",
    "    else:\n",
    "        bvals[c] = np.nan\n",
    "\n",
    "bvals.update({\"ctx_hour_sin\": ctx_hour_sin, \"ctx_hour_cos\": ctx_hour_cos, \"ctx_dow\": ctx_dow})\n",
    "\n",
    "# ---------- 4) recompute candidate-dependent features ----------\n",
    "# author recency (days since this user last saw that author before t_session)\n",
    "past = b1[(b1.user_id==u0) & (b1.ts < t_session)][[\"i_author_id\",\"ts\"]].copy()\n",
    "last_seen = (past.sort_values(\"ts\")\n",
    "                  .drop_duplicates(subset=[\"i_author_id\"], keep=\"last\")\n",
    "                  .set_index(\"i_author_id\")[\"ts\"])\n",
    "\n",
    "def _author_recency_days(author_id: int) -> float:\n",
    "    try:\n",
    "        ts_last = last_seen.loc[author_id]\n",
    "        return max((t_session - ts_last).total_seconds() / 86400.0, 0.0)\n",
    "    except KeyError:\n",
    "        return 365.0  # deterministic default for unseen authors\n",
    "\n",
    "# per-candidate age at session time\n",
    "def _item_age_days(up_ts: pd.Timestamp) -> float:\n",
    "    return max((t_session - up_ts).total_seconds()/86400.0, 0.0)\n",
    "\n",
    "cand = item_bank[[\"video_id\",\"i_author_id\",\"i_video_type\",\"i_upload_type\",\"i_visible_status\",\n",
    "                  \"video_width\",\"video_height\",\"i_music_id\",\"i_video_tag_id\",\"i_video_duration_s\",\n",
    "                  \"i_aspect_ratio\",\"i_top_category_id\",\"_upload_ts_est\"]].copy()\n",
    "\n",
    "cand[\"i_age_since_upload_days\"] = cand[\"_upload_ts_est\"].apply(_item_age_days).astype(\"float32\")\n",
    "cand.drop(columns=[\"_upload_ts_est\"], inplace=True)\n",
    "\n",
    "cand[\"hist_author_recency_days\"]  = cand[\"i_author_id\"].apply(_author_recency_days).astype(\"float32\")\n",
    "cand[\"hist_author_recency_log1p\"] = np.log1p(cand[\"hist_author_recency_days\"]).astype(\"float32\")\n",
    "\n",
    "# ---------- 5) assemble 2,919 × 64 matrix in the exact feat_cols order ----------\n",
    "# start with per-candidate item fields\n",
    "need_item = ['i_author_id','i_video_type','i_upload_type','i_visible_status','video_width',\n",
    "             'video_height','i_music_id','i_video_tag_id','i_video_duration_s','i_aspect_ratio',\n",
    "             'i_age_since_upload_days','i_top_category_id',\n",
    "             'hist_author_recency_days','hist_author_recency_log1p']\n",
    "# add placeholders for all broadcast fields\n",
    "for c in broadcast_cols:\n",
    "    cand[c] = bvals[c]\n",
    "\n",
    "# ensure dtypes for categoricals match b1 (so LightGBM doesn't complain)\n",
    "from pandas.api.types import CategoricalDtype\n",
    "for c in ['i_author_id','i_video_type','i_upload_type','i_visible_status','i_music_id',\n",
    "          'i_video_tag_id','i_top_category_id']:\n",
    "    if c in b1.columns and pd.api.types.is_categorical_dtype(b1[c].dtype):\n",
    "        cat_type: CategoricalDtype = b1[c].dtype\n",
    "        cand[c] = cand[c].astype(cat_type)\n",
    "\n",
    "# add session ids if you want to keep them around (not part of the 64)\n",
    "cand[\"user_id\"] = u0\n",
    "cand[\"session\"] = s0\n",
    "\n",
    "# finally select the 64 features in *exact* training order\n",
    "X_cand = cand[feat_cols].copy()\n",
    "\n",
    "print(\"Candidate feature matrix for (user, session) =\", (u0, s0))\n",
    "print(\"Shape:\", X_cand.shape)   # should be (2919, 64)\n",
    "display(X_cand.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b99490b4-b375-449a-9cd9-128a82d4819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Day-4: 2020-07-07\n",
      "Sessions used: 10\n",
      "Feature matrix shape (expect 10*2919 x 64): (29190, 64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_user_active_degree</th>\n",
       "      <th>u_is_lowactive_period</th>\n",
       "      <th>u_is_live_streamer</th>\n",
       "      <th>u_is_video_author</th>\n",
       "      <th>u_follow_user_num</th>\n",
       "      <th>u_follow_user_num_range</th>\n",
       "      <th>u_fans_user_num</th>\n",
       "      <th>u_fans_user_num_range</th>\n",
       "      <th>u_friend_user_num</th>\n",
       "      <th>u_friend_user_num_range</th>\n",
       "      <th>...</th>\n",
       "      <th>hist_last3_wr_mean</th>\n",
       "      <th>hist_last3_wr_var</th>\n",
       "      <th>hist_ema_complete</th>\n",
       "      <th>hist_ema_wr_mean</th>\n",
       "      <th>hist_prev_within_sess_wr_slope</th>\n",
       "      <th>hist_author_recency_days</th>\n",
       "      <th>hist_author_recency_log1p</th>\n",
       "      <th>hist_user_cat_complete_ema</th>\n",
       "      <th>hist_cat_entropy</th>\n",
       "      <th>burst_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001.0000</td>\n",
       "      <td>500+</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>[1,10)</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>[1,5)</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0996</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.7937</td>\n",
       "      <td>1.5927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365.0000</td>\n",
       "      <td>5.9026</td>\n",
       "      <td>0.8747</td>\n",
       "      <td>2.7236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001.0000</td>\n",
       "      <td>500+</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>[1,10)</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>[1,5)</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0996</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.7937</td>\n",
       "      <td>1.5927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365.0000</td>\n",
       "      <td>5.9026</td>\n",
       "      <td>0.8747</td>\n",
       "      <td>2.7236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001.0000</td>\n",
       "      <td>500+</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>[1,10)</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>[1,5)</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0996</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.7937</td>\n",
       "      <td>1.5927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365.0000</td>\n",
       "      <td>5.9026</td>\n",
       "      <td>0.8747</td>\n",
       "      <td>2.7236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001.0000</td>\n",
       "      <td>500+</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>[1,10)</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>[1,5)</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0996</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.7937</td>\n",
       "      <td>1.5927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9928</td>\n",
       "      <td>0.6895</td>\n",
       "      <td>0.8747</td>\n",
       "      <td>2.7236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001.0000</td>\n",
       "      <td>500+</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>[1,10)</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>[1,5)</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0996</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.7937</td>\n",
       "      <td>1.5927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365.0000</td>\n",
       "      <td>5.9026</td>\n",
       "      <td>0.8747</td>\n",
       "      <td>2.7236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  u_user_active_degree  u_is_lowactive_period  u_is_live_streamer  \\\n",
       "0          full_active                      0                   0   \n",
       "1          full_active                      0                   0   \n",
       "2          full_active                      0                   0   \n",
       "3          full_active                      0                   0   \n",
       "4          full_active                      0                   0   \n",
       "\n",
       "   u_is_video_author  u_follow_user_num u_follow_user_num_range  \\\n",
       "0                  0          2001.0000                    500+   \n",
       "1                  0          2001.0000                    500+   \n",
       "2                  0          2001.0000                    500+   \n",
       "3                  0          2001.0000                    500+   \n",
       "4                  0          2001.0000                    500+   \n",
       "\n",
       "   u_fans_user_num u_fans_user_num_range  u_friend_user_num  \\\n",
       "0           3.0000                [1,10)             2.0000   \n",
       "1           3.0000                [1,10)             2.0000   \n",
       "2           3.0000                [1,10)             2.0000   \n",
       "3           3.0000                [1,10)             2.0000   \n",
       "4           3.0000                [1,10)             2.0000   \n",
       "\n",
       "  u_friend_user_num_range  ...  hist_last3_wr_mean hist_last3_wr_var  \\\n",
       "0                   [1,5)  ...              1.0996            0.2604   \n",
       "1                   [1,5)  ...              1.0996            0.2604   \n",
       "2                   [1,5)  ...              1.0996            0.2604   \n",
       "3                   [1,5)  ...              1.0996            0.2604   \n",
       "4                   [1,5)  ...              1.0996            0.2604   \n",
       "\n",
       "  hist_ema_complete hist_ema_wr_mean hist_prev_within_sess_wr_slope  \\\n",
       "0            0.7937           1.5927                            NaN   \n",
       "1            0.7937           1.5927                            NaN   \n",
       "2            0.7937           1.5927                            NaN   \n",
       "3            0.7937           1.5927                            NaN   \n",
       "4            0.7937           1.5927                            NaN   \n",
       "\n",
       "  hist_author_recency_days hist_author_recency_log1p  \\\n",
       "0                 365.0000                    5.9026   \n",
       "1                 365.0000                    5.9026   \n",
       "2                 365.0000                    5.9026   \n",
       "3                   0.9928                    0.6895   \n",
       "4                 365.0000                    5.9026   \n",
       "\n",
       "  hist_user_cat_complete_ema hist_cat_entropy burst_id  \n",
       "0                     0.8747           2.7236        1  \n",
       "1                     0.8747           2.7236        1  \n",
       "2                     0.8747           2.7236        1  \n",
       "3                     0.8747           2.7236        1  \n",
       "4                     0.8747           2.7236        1  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session</th>\n",
       "      <th>video_id</th>\n",
       "      <th>u_user_active_degree</th>\n",
       "      <th>u_is_lowactive_period</th>\n",
       "      <th>u_is_live_streamer</th>\n",
       "      <th>u_is_video_author</th>\n",
       "      <th>u_follow_user_num</th>\n",
       "      <th>u_follow_user_num_range</th>\n",
       "      <th>u_fans_user_num</th>\n",
       "      <th>...</th>\n",
       "      <th>hist_last3_wr_mean</th>\n",
       "      <th>hist_last3_wr_var</th>\n",
       "      <th>hist_ema_complete</th>\n",
       "      <th>hist_ema_wr_mean</th>\n",
       "      <th>hist_prev_within_sess_wr_slope</th>\n",
       "      <th>hist_author_recency_days</th>\n",
       "      <th>hist_author_recency_log1p</th>\n",
       "      <th>hist_user_cat_complete_ema</th>\n",
       "      <th>hist_cat_entropy</th>\n",
       "      <th>burst_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6605</td>\n",
       "      <td>8</td>\n",
       "      <td>1907</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001.0000</td>\n",
       "      <td>500+</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0996</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.7937</td>\n",
       "      <td>1.5927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365.0000</td>\n",
       "      <td>5.9026</td>\n",
       "      <td>0.8747</td>\n",
       "      <td>2.7236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6605</td>\n",
       "      <td>8</td>\n",
       "      <td>3634</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001.0000</td>\n",
       "      <td>500+</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0996</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.7937</td>\n",
       "      <td>1.5927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365.0000</td>\n",
       "      <td>5.9026</td>\n",
       "      <td>0.8747</td>\n",
       "      <td>2.7236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6605</td>\n",
       "      <td>8</td>\n",
       "      <td>8203</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001.0000</td>\n",
       "      <td>500+</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0996</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.7937</td>\n",
       "      <td>1.5927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365.0000</td>\n",
       "      <td>5.9026</td>\n",
       "      <td>0.8747</td>\n",
       "      <td>2.7236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6605</td>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001.0000</td>\n",
       "      <td>500+</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0996</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.7937</td>\n",
       "      <td>1.5927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9928</td>\n",
       "      <td>0.6895</td>\n",
       "      <td>0.8747</td>\n",
       "      <td>2.7236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6605</td>\n",
       "      <td>8</td>\n",
       "      <td>139</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001.0000</td>\n",
       "      <td>500+</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0996</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.7937</td>\n",
       "      <td>1.5927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365.0000</td>\n",
       "      <td>5.9026</td>\n",
       "      <td>0.8747</td>\n",
       "      <td>2.7236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  session  video_id u_user_active_degree  u_is_lowactive_period  \\\n",
       "0     6605        8      1907          full_active                      0   \n",
       "1     6605        8      3634          full_active                      0   \n",
       "2     6605        8      8203          full_active                      0   \n",
       "3     6605        8       183          full_active                      0   \n",
       "4     6605        8       139          full_active                      0   \n",
       "\n",
       "   u_is_live_streamer  u_is_video_author  u_follow_user_num  \\\n",
       "0                   0                  0          2001.0000   \n",
       "1                   0                  0          2001.0000   \n",
       "2                   0                  0          2001.0000   \n",
       "3                   0                  0          2001.0000   \n",
       "4                   0                  0          2001.0000   \n",
       "\n",
       "  u_follow_user_num_range  u_fans_user_num  ... hist_last3_wr_mean  \\\n",
       "0                    500+           3.0000  ...             1.0996   \n",
       "1                    500+           3.0000  ...             1.0996   \n",
       "2                    500+           3.0000  ...             1.0996   \n",
       "3                    500+           3.0000  ...             1.0996   \n",
       "4                    500+           3.0000  ...             1.0996   \n",
       "\n",
       "   hist_last3_wr_var hist_ema_complete  hist_ema_wr_mean  \\\n",
       "0             0.2604            0.7937            1.5927   \n",
       "1             0.2604            0.7937            1.5927   \n",
       "2             0.2604            0.7937            1.5927   \n",
       "3             0.2604            0.7937            1.5927   \n",
       "4             0.2604            0.7937            1.5927   \n",
       "\n",
       "  hist_prev_within_sess_wr_slope hist_author_recency_days  \\\n",
       "0                            NaN                 365.0000   \n",
       "1                            NaN                 365.0000   \n",
       "2                            NaN                 365.0000   \n",
       "3                            NaN                   0.9928   \n",
       "4                            NaN                 365.0000   \n",
       "\n",
       "  hist_author_recency_log1p hist_user_cat_complete_ema hist_cat_entropy  \\\n",
       "0                    5.9026                     0.8747           2.7236   \n",
       "1                    5.9026                     0.8747           2.7236   \n",
       "2                    5.9026                     0.8747           2.7236   \n",
       "3                    0.6895                     0.8747           2.7236   \n",
       "4                    5.9026                     0.8747           2.7236   \n",
       "\n",
       "  burst_id  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# =========================================================\n",
    "# 0) One-time item bank (keeps training dtypes)\n",
    "# =========================================================\n",
    "_item_cols = [\"video_id\",\"i_author_id\",\"i_video_type\",\"i_upload_type\",\"i_visible_status\",\n",
    "              \"video_width\",\"video_height\",\"i_music_id\",\"i_video_tag_id\",\"i_video_duration_s\",\n",
    "              \"i_aspect_ratio\",\"i_top_category_id\",\"i_age_since_upload_days\",\"ts\"]\n",
    "_item_cols = [c for c in _item_cols if c in b1.columns]\n",
    "\n",
    "ITEM_BANK = (b1[_item_cols]\n",
    "             .sort_values(\"ts\", kind=\"mergesort\")\n",
    "             .drop_duplicates(subset=[\"video_id\"], keep=\"first\")\n",
    "             .reset_index(drop=True))\n",
    "\n",
    "# estimate upload timestamp from first-seen age\n",
    "if {\"ts\",\"i_age_since_upload_days\"}.issubset(ITEM_BANK.columns):\n",
    "    ITEM_BANK[\"_upload_ts_est\"] = (\n",
    "        ITEM_BANK[\"ts\"] - pd.to_timedelta(ITEM_BANK[\"i_age_since_upload_days\"].astype(float), unit=\"D\")\n",
    "    )\n",
    "else:\n",
    "    ITEM_BANK[\"_upload_ts_est\"] = ITEM_BANK[\"ts\"]  # harmless fallback\n",
    "\n",
    "# which columns will be broadcast per-session\n",
    "BROADCAST_COLS = [\n",
    " 'u_user_active_degree','u_is_lowactive_period','u_is_live_streamer','u_is_video_author',\n",
    " 'u_follow_user_num','u_follow_user_num_range','u_fans_user_num','u_fans_user_num_range',\n",
    " 'u_friend_user_num','u_friend_user_num_range','u_register_days','u_register_days_range',\n",
    " 'u_onehot_feat0','u_onehot_feat1','u_onehot_feat2','u_onehot_feat3','u_onehot_feat4',\n",
    " 'u_onehot_feat5','u_onehot_feat6','u_onehot_feat7','u_onehot_feat8','u_onehot_feat9',\n",
    " 'u_onehot_feat10','u_onehot_feat11','u_onehot_feat12','u_onehot_feat13','u_onehot_feat14',\n",
    " 'u_onehot_feat15','u_onehot_feat16','u_onehot_feat17','u_follow_user_num_log1p',\n",
    " 'u_fans_user_num_log1p','u_friend_user_num_log1p','u_register_days_log1p',\n",
    " 'ctx_hour_sin','ctx_hour_cos','ctx_dow','sess_index','prev_session_length_min',\n",
    " 'inter_session_gap_hours','hist_last3_complete_rate','hist_last10_complete_rate',\n",
    " 'hist_last3_wr_mean','hist_last3_wr_var','hist_ema_complete','hist_ema_wr_mean',\n",
    " 'hist_prev_within_sess_wr_slope','hist_user_cat_complete_ema','hist_cat_entropy','burst_id'\n",
    "]\n",
    "\n",
    "# item fields needed per candidate (including author recency placeholders we compute below)\n",
    "ITEM_NEED = ['i_author_id','i_video_type','i_upload_type','i_visible_status','video_width',\n",
    "             'video_height','i_music_id','i_video_tag_id','i_video_duration_s','i_aspect_ratio',\n",
    "             'i_top_category_id']\n",
    "\n",
    "# categories to enforce dtype equality with training\n",
    "CAT_ENFORCE = ['i_author_id','i_video_type','i_upload_type','i_visible_status',\n",
    "               'i_music_id','i_video_tag_id','i_top_category_id']\n",
    "\n",
    "# =========================================================\n",
    "# 1) per-session builder\n",
    "# =========================================================\n",
    "def build_candidate_matrix_for_session(u: int, s: int, *, return_ids: bool=False):\n",
    "    \"\"\"\n",
    "    Returns X (2919 x 64) for (u,s). If return_ids=True, also returns ids DF with user_id/session/video_id.\n",
    "    \"\"\"\n",
    "    g = (b1[(b1.user_id==u) & (b1.session==s)]\n",
    "           .sort_values(\"ts\", kind=\"mergesort\")\n",
    "           .reset_index(drop=True))\n",
    "    if g.empty:\n",
    "        return (pd.DataFrame(), pd.DataFrame()) if return_ids else pd.DataFrame()\n",
    "\n",
    "    # anchor to session start\n",
    "    t_session = pd.Timestamp(g[\"ts\"].iloc[0])\n",
    "\n",
    "    # ---- broadcast session/user/history ----\n",
    "    hour = t_session.hour + t_session.minute/60.0\n",
    "    angle = 2*np.pi*hour/24.0\n",
    "    bvals = {c: (g[c].iloc[0] if c in g.columns else np.nan) for c in BROADCAST_COLS}\n",
    "    bvals.update({\"ctx_hour_sin\": np.sin(angle), \"ctx_hour_cos\": np.cos(angle), \"ctx_dow\": t_session.dayofweek})\n",
    "\n",
    "    # ---- author recency relative to this session ----\n",
    "    past = b1[(b1.user_id==u) & (b1.ts < t_session)][[\"i_author_id\",\"ts\"]]\n",
    "    last_seen = (past.sort_values(\"ts\", kind=\"mergesort\")\n",
    "                      .drop_duplicates(subset=[\"i_author_id\"], keep=\"last\")\n",
    "                      .set_index(\"i_author_id\")[\"ts\"])\n",
    "\n",
    "    def _author_recency_days(aid):\n",
    "        try:\n",
    "            ts_last = last_seen.loc[aid]\n",
    "            return max((t_session - ts_last).total_seconds()/86400.0, 0.0)\n",
    "        except KeyError:\n",
    "            return 365.0  # deterministic default\n",
    "\n",
    "    # ---- assemble candidates from bank ----\n",
    "    cand = ITEM_BANK[[\"video_id\"] + ITEM_NEED + [\"_upload_ts_est\"]].copy()\n",
    "    # per-candidate age at this session\n",
    "    cand[\"i_age_since_upload_days\"] = (\n",
    "        (t_session - cand[\"_upload_ts_est\"]).dt.total_seconds()/86400.0\n",
    "    ).clip(lower=0).astype(\"float32\")\n",
    "    cand.drop(columns=[\"_upload_ts_est\"], inplace=True)\n",
    "\n",
    "    # author recency\n",
    "    cand[\"hist_author_recency_days\"]  = cand[\"i_author_id\"].apply(_author_recency_days).astype(\"float32\")\n",
    "    cand[\"hist_author_recency_log1p\"] = np.log1p(cand[\"hist_author_recency_days\"]).astype(\"float32\")\n",
    "\n",
    "    # broadcast session fields\n",
    "    for c, v in bvals.items():\n",
    "        cand[c] = v\n",
    "\n",
    "    # enforce categorical dtypes to match training data\n",
    "    for c in CAT_ENFORCE:\n",
    "        if c in b1.columns and isinstance(b1[c].dtype, CategoricalDtype):\n",
    "            cand[c] = cand[c].astype(b1[c].dtype)\n",
    "\n",
    "    # final 64 in exact order\n",
    "    X = cand[feat_cols].copy()\n",
    "\n",
    "    if return_ids:\n",
    "        ids = cand[[\"video_id\"]].copy()\n",
    "        ids.insert(0, \"session\", s)\n",
    "        ids.insert(0, \"user_id\", u)\n",
    "        return X, ids\n",
    "    return X\n",
    "\n",
    "# =========================================================\n",
    "# 2) grab Day-4, take the first 10 sessions, build giant matrix\n",
    "# =========================================================\n",
    "# find Day-4 (4th unique calendar day)\n",
    "uniq_days = np.sort(b1[\"ts\"].dt.normalize().unique())\n",
    "if len(uniq_days) < 4:\n",
    "    raise RuntimeError(\"Less than 4 days available in b1.\")\n",
    "d4 = uniq_days[3]\n",
    "\n",
    "d4_sessions = (b1.loc[b1[\"ts\"].dt.normalize()==d4, [\"user_id\",\"session\",\"ts\"]]\n",
    "                 .sort_values([\"ts\"], kind=\"mergesort\")\n",
    "                 .drop_duplicates(subset=[\"user_id\",\"session\"])\n",
    "                 .head(10)[[\"user_id\",\"session\"]]\n",
    "                 .reset_index(drop=True))\n",
    "\n",
    "blocks = []\n",
    "id_blocks = []\n",
    "for u, s in d4_sessions.itertuples(index=False):\n",
    "    Xs, ids = build_candidate_matrix_for_session(int(u), int(s), return_ids=True)\n",
    "    if Xs.empty:\n",
    "        continue\n",
    "    blocks.append(Xs)\n",
    "    id_blocks.append(ids)\n",
    "\n",
    "X_10 = pd.concat(blocks, axis=0, ignore_index=True) if blocks else pd.DataFrame(columns=feat_cols)\n",
    "ids_10 = pd.concat(id_blocks, axis=0, ignore_index=True) if id_blocks else pd.DataFrame(columns=[\"user_id\",\"session\",\"video_id\"])\n",
    "\n",
    "print(\"Selected Day-4:\", pd.to_datetime(d4).date())\n",
    "print(\"Sessions used:\", len(d4_sessions))\n",
    "print(\"Feature matrix shape (expect 10*2919 x 64):\", X_10.shape)\n",
    "display(X_10.head())\n",
    "\n",
    "# (optional) join ids for inspection only (keep features separate for model input)\n",
    "preview = ids_10.join(X_10, how=\"left\")\n",
    "display(preview.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad947589-9a26-42a4-abda-56b8c81d0a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction table shape: (2919, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kx/mcwvbnt12wl9fy5xcs5jlnx00000gn/T/ipykernel_12303/120144092.py:33: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if is_categorical_dtype(ref[c].dtype):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>y_complete</th>\n",
       "      <th>y_long</th>\n",
       "      <th>y_rewatch</th>\n",
       "      <th>y_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1907</td>\n",
       "      <td>0.8024</td>\n",
       "      <td>0.6203</td>\n",
       "      <td>0.1820</td>\n",
       "      <td>0.0552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3634</td>\n",
       "      <td>0.6712</td>\n",
       "      <td>0.5054</td>\n",
       "      <td>0.1327</td>\n",
       "      <td>0.0586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8203</td>\n",
       "      <td>0.9257</td>\n",
       "      <td>0.8184</td>\n",
       "      <td>0.4729</td>\n",
       "      <td>0.0518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>183</td>\n",
       "      <td>0.8963</td>\n",
       "      <td>0.7513</td>\n",
       "      <td>0.3290</td>\n",
       "      <td>0.0538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139</td>\n",
       "      <td>0.7629</td>\n",
       "      <td>0.5759</td>\n",
       "      <td>0.1141</td>\n",
       "      <td>0.0530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_id  y_complete  y_long  y_rewatch  y_neg\n",
       "0      1907      0.8024  0.6203     0.1820 0.0552\n",
       "1      3634      0.6712  0.5054     0.1327 0.0586\n",
       "2      8203      0.9257  0.8184     0.4729 0.0518\n",
       "3       183      0.8963  0.7513     0.3290 0.0538\n",
       "4       139      0.7629  0.5759     0.1141 0.0530"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json, numpy as np, pandas as pd, lightgbm as lgb\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "# ---- 0) Reference df used in training (for category sets & dtypes) ----\n",
    "# If you still have `train` from the head training cell, use that.\n",
    "# Otherwise, approximate with the subset of b1 you trained on (e.g., D1–D3).\n",
    "try:\n",
    "    ref_df = train[feat_cols].copy()\n",
    "except NameError:\n",
    "    # Fallback: use first three unique days as the training proxy\n",
    "    uniq_days = np.sort(b1[\"ts\"].dt.normalize().unique())\n",
    "    train_days = set(uniq_days[:3])\n",
    "    ref_df = b1.loc[b1[\"ts\"].dt.normalize().isin(train_days), feat_cols].copy()\n",
    "\n",
    "# categorical features as used in training\n",
    "if \"cat_cols\" in globals():\n",
    "    cat_cols_ref = [c for c in cat_cols if c in feat_cols]\n",
    "else:\n",
    "    # Infer: any column that was category dtype during training\n",
    "    cat_cols_ref = [c for c in feat_cols if c in ref_df.columns and is_categorical_dtype(ref_df[c].dtype)]\n",
    "\n",
    "# ---- 1) Align function: make X_pred match training dtypes & categories ----\n",
    "def align_to_training_schema(X_pred: pd.DataFrame, ref: pd.DataFrame,\n",
    "                             feat_cols: list[str], cat_cols: list[str]) -> pd.DataFrame:\n",
    "    X = X_pred.copy()\n",
    "    # keep only and order by feat_cols\n",
    "    X = X[feat_cols].copy()\n",
    "    # cast categorical columns to the exact categories & dtype from training\n",
    "    for c in cat_cols:\n",
    "        if c not in X.columns or c not in ref.columns:\n",
    "            continue\n",
    "        # if ref has categories, use them; otherwise just cast dtype\n",
    "        if is_categorical_dtype(ref[c].dtype):\n",
    "            cats = ref[c].cat.categories\n",
    "            X[c] = pd.Categorical(X[c], categories=cats)  # unseen labels -> NaN\n",
    "        else:\n",
    "            X[c] = X[c].astype(ref[c].dtype, copy=False)\n",
    "    # for non-categorical columns, ensure numeric dtype compatibility\n",
    "    for c in feat_cols:\n",
    "        if c in cat_cols:\n",
    "            continue\n",
    "        if c in ref.columns:\n",
    "            try:\n",
    "                X[c] = X[c].astype(ref[c].dtype, copy=False)\n",
    "            except Exception:\n",
    "                pass  # leave as-is if safe cast fails\n",
    "    return X\n",
    "\n",
    "# ---- 2) Load models + calibrators once ----\n",
    "heads = [\"y_complete\",\"y_long\",\"y_rewatch\",\"y_neg\"]\n",
    "\n",
    "def _load_head(head_name: str):\n",
    "    head_dir = OUTDIR / head_name\n",
    "    booster = lgb.Booster(model_file=str(head_dir / f\"lgb_b1_{head_name}.txt\"))\n",
    "    with open(head_dir / \"calibrator.json\") as f:\n",
    "        c = json.load(f)\n",
    "    return booster, float(c.get(\"coef\", 1.0)), float(c.get(\"intercept\", 0.0))\n",
    "\n",
    "MODELS = {h: _load_head(h) for h in heads}\n",
    "\n",
    "def _predict_head(head_name: str, X_feat: pd.DataFrame) -> np.ndarray:\n",
    "    booster, a, b = MODELS[head_name]\n",
    "    # IMPORTANT: disable LightGBM’s internal feature-name/category checks?\n",
    "    # Not recommended. Instead, ensure exact schema using align_to_training_schema above.\n",
    "    margin = booster.predict(X_feat, num_iteration=booster.best_iteration, raw_score=True)\n",
    "    return 1.0 / (1.0 + np.exp(-(a * margin + b)))\n",
    "\n",
    "# ---- 3) Build candidates for the first Day-4 session and ALIGN schema ----\n",
    "uniq_days = np.sort(b1[\"ts\"].dt.normalize().unique())\n",
    "d4 = uniq_days[3]\n",
    "first_sess = (b1.loc[b1[\"ts\"].dt.normalize()==d4, [\"user_id\",\"session\",\"ts\"]]\n",
    "                .sort_values(\"ts\", kind=\"mergesort\")\n",
    "                .drop_duplicates(subset=[\"user_id\",\"session\"])\n",
    "                .iloc[0])\n",
    "u0, s0 = int(first_sess.user_id), int(first_sess.session)\n",
    "\n",
    "# Your helper from earlier that returns a (2919 x 64) feature matrix for (u0,s0)\n",
    "X_cand_raw, ids_cand = build_candidate_matrix_for_session(u0, s0, return_ids=True)\n",
    "\n",
    "# Align EXACTLY to the training schema (order, dtypes, categories)\n",
    "X_cand = align_to_training_schema(X_cand_raw, ref_df, feat_cols, cat_cols_ref)\n",
    "assert list(X_cand.columns) == list(feat_cols), \"Feature order mismatch after alignment\"\n",
    "\n",
    "# ---- 4) Predict all four heads (2919 x 4) ----\n",
    "preds = {h: _predict_head(h, X_cand) for h in heads}\n",
    "pred_tbl = pd.DataFrame(preds)\n",
    "pred_tbl.insert(0, \"video_id\", ids_cand[\"video_id\"])\n",
    "print(\"Prediction table shape:\", pred_tbl.shape)  # expect (2919, 5 with video_id)\n",
    "display(pred_tbl.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a61687e-19a1-4928-b211-3ff778ea54ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Day 4] finished 50/15455 sessions …\n",
      "[Day 4] finished 100/15455 sessions …\n",
      "[Day 4] finished 150/15455 sessions …\n",
      "[Day 4] finished 200/15455 sessions …\n",
      "[Day 4] finished 250/15455 sessions …\n",
      "[Day 4] finished 300/15455 sessions …\n",
      "[Day 4] finished 350/15455 sessions …\n",
      "[Day 4] finished 400/15455 sessions …\n",
      "[Day 4] finished 450/15455 sessions …\n",
      "[Day 4] finished 500/15455 sessions …\n",
      "[Day 4] finished 550/15455 sessions …\n",
      "[Day 4] finished 600/15455 sessions …\n",
      "[Day 4] finished 650/15455 sessions …\n",
      "[Day 4] finished 700/15455 sessions …\n",
      "[Day 4] finished 750/15455 sessions …\n",
      "[Day 4] finished 800/15455 sessions …\n",
      "[Day 4] finished 850/15455 sessions …\n",
      "[Day 4] finished 900/15455 sessions …\n",
      "[Day 4] finished 950/15455 sessions …\n",
      "[Day 4] finished 1000/15455 sessions …\n",
      "[Day 4] finished 1050/15455 sessions …\n",
      "[Day 4] finished 1100/15455 sessions …\n",
      "[Day 4] finished 1150/15455 sessions …\n",
      "[Day 4] finished 1200/15455 sessions …\n",
      "[Day 4] finished 1250/15455 sessions …\n",
      "[Day 4] finished 1300/15455 sessions …\n",
      "[Day 4] finished 1350/15455 sessions …\n",
      "[Day 4] finished 1400/15455 sessions …\n",
      "[Day 4] finished 1450/15455 sessions …\n",
      "[Day 4] finished 1500/15455 sessions …\n",
      "[Day 4] finished 1550/15455 sessions …\n",
      "[Day 4] finished 1600/15455 sessions …\n",
      "[Day 4] finished 1650/15455 sessions …\n",
      "[Day 4] finished 1700/15455 sessions …\n",
      "[Day 4] finished 1750/15455 sessions …\n",
      "[Day 4] finished 1800/15455 sessions …\n",
      "[Day 4] finished 1850/15455 sessions …\n",
      "[Day 4] finished 1900/15455 sessions …\n",
      "[Day 4] finished 1950/15455 sessions …\n",
      "[Day 4] finished 2000/15455 sessions …\n",
      "[Day 4] finished 2050/15455 sessions …\n",
      "[Day 4] finished 2100/15455 sessions …\n",
      "[Day 4] finished 2150/15455 sessions …\n",
      "[Day 4] finished 2200/15455 sessions …\n",
      "[Day 4] finished 2250/15455 sessions …\n",
      "[Day 4] finished 2300/15455 sessions …\n",
      "[Day 4] finished 2350/15455 sessions …\n",
      "[Day 4] finished 2400/15455 sessions …\n",
      "[Day 4] finished 2450/15455 sessions …\n",
      "[Day 4] finished 2500/15455 sessions …\n",
      "[Day 4] finished 2550/15455 sessions …\n",
      "[Day 4] finished 2600/15455 sessions …\n",
      "[Day 4] finished 2650/15455 sessions …\n",
      "[Day 4] finished 2700/15455 sessions …\n",
      "[Day 4] finished 2750/15455 sessions …\n",
      "[Day 4] finished 2800/15455 sessions …\n",
      "[Day 4] finished 2850/15455 sessions …\n",
      "[Day 4] finished 2900/15455 sessions …\n",
      "[Day 4] finished 2950/15455 sessions …\n",
      "[Day 4] finished 3000/15455 sessions …\n",
      "[Day 4] finished 3050/15455 sessions …\n",
      "[Day 4] finished 3100/15455 sessions …\n",
      "[Day 4] finished 3150/15455 sessions …\n",
      "[Day 4] finished 3200/15455 sessions …\n",
      "[Day 4] finished 3250/15455 sessions …\n",
      "[Day 4] finished 3300/15455 sessions …\n",
      "[Day 4] finished 3350/15455 sessions …\n",
      "[Day 4] finished 3400/15455 sessions …\n",
      "[Day 4] finished 3450/15455 sessions …\n",
      "[Day 4] finished 3500/15455 sessions …\n",
      "[Day 4] finished 3550/15455 sessions …\n",
      "[Day 4] finished 3600/15455 sessions …\n",
      "[Day 4] finished 3650/15455 sessions …\n",
      "[Day 4] finished 3700/15455 sessions …\n",
      "[Day 4] finished 3750/15455 sessions …\n",
      "[Day 4] finished 3800/15455 sessions …\n",
      "[Day 4] finished 3850/15455 sessions …\n",
      "[Day 4] finished 3900/15455 sessions …\n",
      "[Day 4] finished 3950/15455 sessions …\n",
      "[Day 4] finished 4000/15455 sessions …\n",
      "[Day 4] finished 4050/15455 sessions …\n",
      "[Day 4] finished 4100/15455 sessions …\n",
      "[Day 4] finished 4150/15455 sessions …\n",
      "[Day 4] finished 4200/15455 sessions …\n",
      "[Day 4] finished 4250/15455 sessions …\n",
      "[Day 4] finished 4300/15455 sessions …\n",
      "[Day 4] finished 4350/15455 sessions …\n",
      "[Day 4] finished 4400/15455 sessions …\n",
      "[Day 4] finished 4450/15455 sessions …\n",
      "[Day 4] finished 4500/15455 sessions …\n",
      "[Day 4] finished 4550/15455 sessions …\n",
      "[Day 4] finished 4600/15455 sessions …\n",
      "[Day 4] finished 4650/15455 sessions …\n",
      "[Day 4] finished 4700/15455 sessions …\n",
      "[Day 4] finished 4750/15455 sessions …\n",
      "[Day 4] finished 4800/15455 sessions …\n",
      "[Day 4] finished 4850/15455 sessions …\n",
      "[Day 4] finished 4900/15455 sessions …\n",
      "[Day 4] finished 4950/15455 sessions …\n",
      "[Day 4] finished 5000/15455 sessions …\n",
      "[Day 4] finished 5050/15455 sessions …\n",
      "[Day 4] finished 5100/15455 sessions …\n",
      "[Day 4] finished 5150/15455 sessions …\n",
      "[Day 4] finished 5200/15455 sessions …\n",
      "[Day 4] finished 5250/15455 sessions …\n",
      "[Day 4] finished 5300/15455 sessions …\n",
      "[Day 4] finished 5350/15455 sessions …\n",
      "[Day 4] finished 5400/15455 sessions …\n",
      "[Day 4] finished 5450/15455 sessions …\n",
      "[Day 4] finished 5500/15455 sessions …\n",
      "[Day 4] finished 5550/15455 sessions …\n",
      "[Day 4] finished 5600/15455 sessions …\n",
      "[Day 4] finished 5650/15455 sessions …\n",
      "[Day 4] finished 5700/15455 sessions …\n",
      "[Day 4] finished 5750/15455 sessions …\n",
      "[Day 4] finished 5800/15455 sessions …\n",
      "[Day 4] finished 5850/15455 sessions …\n",
      "[Day 4] finished 5900/15455 sessions …\n",
      "[Day 4] finished 5950/15455 sessions …\n",
      "[Day 4] finished 6000/15455 sessions …\n",
      "[Day 4] finished 6050/15455 sessions …\n",
      "[Day 4] finished 6100/15455 sessions …\n",
      "[Day 4] finished 6150/15455 sessions …\n",
      "[Day 4] finished 6200/15455 sessions …\n",
      "[Day 4] finished 6250/15455 sessions …\n",
      "[Day 4] finished 6300/15455 sessions …\n",
      "[Day 4] finished 6350/15455 sessions …\n",
      "[Day 4] finished 6400/15455 sessions …\n",
      "[Day 4] finished 6450/15455 sessions …\n",
      "[Day 4] finished 6500/15455 sessions …\n",
      "[Day 4] finished 6550/15455 sessions …\n",
      "[Day 4] finished 6600/15455 sessions …\n",
      "[Day 4] finished 6650/15455 sessions …\n",
      "[Day 4] finished 6700/15455 sessions …\n",
      "[Day 4] finished 6750/15455 sessions …\n",
      "[Day 4] finished 6800/15455 sessions …\n",
      "[Day 4] finished 6850/15455 sessions …\n",
      "[Day 4] finished 6900/15455 sessions …\n",
      "[Day 4] finished 6950/15455 sessions …\n",
      "[Day 4] finished 7000/15455 sessions …\n",
      "[Day 4] finished 7050/15455 sessions …\n",
      "[Day 4] finished 7100/15455 sessions …\n",
      "[Day 4] finished 7150/15455 sessions …\n",
      "[Day 4] finished 7200/15455 sessions …\n",
      "[Day 4] finished 7250/15455 sessions …\n",
      "[Day 4] finished 7300/15455 sessions …\n",
      "[Day 4] finished 7350/15455 sessions …\n",
      "[Day 4] finished 7400/15455 sessions …\n",
      "[Day 4] finished 7450/15455 sessions …\n",
      "[Day 4] finished 7500/15455 sessions …\n",
      "[Day 4] finished 7550/15455 sessions …\n",
      "[Day 4] finished 7600/15455 sessions …\n",
      "[Day 4] finished 7650/15455 sessions …\n",
      "[Day 4] finished 7700/15455 sessions …\n",
      "[Day 4] finished 7750/15455 sessions …\n",
      "[Day 4] finished 7800/15455 sessions …\n",
      "[Day 4] finished 7850/15455 sessions …\n",
      "[Day 4] finished 7900/15455 sessions …\n",
      "[Day 4] finished 7950/15455 sessions …\n",
      "[Day 4] finished 8000/15455 sessions …\n",
      "[Day 4] finished 8050/15455 sessions …\n",
      "[Day 4] finished 8100/15455 sessions …\n",
      "[Day 4] finished 8150/15455 sessions …\n",
      "[Day 4] finished 8200/15455 sessions …\n",
      "[Day 4] finished 8250/15455 sessions …\n",
      "[Day 4] finished 8300/15455 sessions …\n",
      "[Day 4] finished 8350/15455 sessions …\n",
      "[Day 4] finished 8400/15455 sessions …\n",
      "[Day 4] finished 8450/15455 sessions …\n",
      "[Day 4] finished 8500/15455 sessions …\n",
      "[Day 4] finished 8550/15455 sessions …\n",
      "[Day 4] finished 8600/15455 sessions …\n",
      "[Day 4] finished 8650/15455 sessions …\n",
      "[Day 4] finished 8700/15455 sessions …\n",
      "[Day 4] finished 8750/15455 sessions …\n",
      "[Day 4] finished 8800/15455 sessions …\n",
      "[Day 4] finished 8850/15455 sessions …\n",
      "[Day 4] finished 8900/15455 sessions …\n",
      "[Day 4] finished 8950/15455 sessions …\n",
      "[Day 4] finished 9000/15455 sessions …\n",
      "[Day 4] finished 9050/15455 sessions …\n",
      "[Day 4] finished 9100/15455 sessions …\n",
      "[Day 4] finished 9150/15455 sessions …\n",
      "[Day 4] finished 9200/15455 sessions …\n",
      "[Day 4] finished 9250/15455 sessions …\n",
      "[Day 4] finished 9300/15455 sessions …\n",
      "[Day 4] finished 9350/15455 sessions …\n",
      "[Day 4] finished 9400/15455 sessions …\n",
      "[Day 4] finished 9450/15455 sessions …\n",
      "[Day 4] finished 9500/15455 sessions …\n",
      "[Day 4] finished 9550/15455 sessions …\n",
      "[Day 4] finished 9600/15455 sessions …\n",
      "[Day 4] finished 9650/15455 sessions …\n",
      "[Day 4] finished 9700/15455 sessions …\n",
      "[Day 4] finished 9750/15455 sessions …\n",
      "[Day 4] finished 9800/15455 sessions …\n",
      "[Day 4] finished 9850/15455 sessions …\n",
      "[Day 4] finished 9900/15455 sessions …\n",
      "[Day 4] finished 9950/15455 sessions …\n",
      "[Day 4] finished 10000/15455 sessions …\n",
      "[Day 4] finished 10050/15455 sessions …\n",
      "[Day 4] finished 10100/15455 sessions …\n",
      "[Day 4] finished 10150/15455 sessions …\n",
      "[Day 4] finished 10200/15455 sessions …\n",
      "[Day 4] finished 10250/15455 sessions …\n",
      "[Day 4] finished 10300/15455 sessions …\n",
      "[Day 4] finished 10350/15455 sessions …\n",
      "[Day 4] finished 10400/15455 sessions …\n",
      "[Day 4] finished 10450/15455 sessions …\n",
      "[Day 4] finished 10500/15455 sessions …\n",
      "[Day 4] finished 10550/15455 sessions …\n",
      "[Day 4] finished 10600/15455 sessions …\n",
      "[Day 4] finished 10650/15455 sessions …\n",
      "[Day 4] finished 10700/15455 sessions …\n",
      "[Day 4] finished 10750/15455 sessions …\n",
      "[Day 4] finished 10800/15455 sessions …\n",
      "[Day 4] finished 10850/15455 sessions …\n",
      "[Day 4] finished 10900/15455 sessions …\n",
      "[Day 4] finished 10950/15455 sessions …\n",
      "[Day 4] finished 11000/15455 sessions …\n",
      "[Day 4] finished 11050/15455 sessions …\n",
      "[Day 4] finished 11100/15455 sessions …\n",
      "[Day 4] finished 11150/15455 sessions …\n",
      "[Day 4] finished 11200/15455 sessions …\n",
      "[Day 4] finished 11250/15455 sessions …\n",
      "[Day 4] finished 11300/15455 sessions …\n",
      "[Day 4] finished 11350/15455 sessions …\n",
      "[Day 4] finished 11400/15455 sessions …\n",
      "[Day 4] finished 11450/15455 sessions …\n",
      "[Day 4] finished 11500/15455 sessions …\n",
      "[Day 4] finished 11550/15455 sessions …\n",
      "[Day 4] finished 11600/15455 sessions …\n",
      "[Day 4] finished 11650/15455 sessions …\n",
      "[Day 4] finished 11700/15455 sessions …\n",
      "[Day 4] finished 11750/15455 sessions …\n",
      "[Day 4] finished 11800/15455 sessions …\n",
      "[Day 4] finished 11850/15455 sessions …\n",
      "[Day 4] finished 11900/15455 sessions …\n",
      "[Day 4] finished 11950/15455 sessions …\n",
      "[Day 4] finished 12000/15455 sessions …\n",
      "[Day 4] finished 12050/15455 sessions …\n",
      "[Day 4] finished 12100/15455 sessions …\n",
      "[Day 4] finished 12150/15455 sessions …\n",
      "[Day 4] finished 12200/15455 sessions …\n",
      "[Day 4] finished 12250/15455 sessions …\n",
      "[Day 4] finished 12300/15455 sessions …\n",
      "[Day 4] finished 12350/15455 sessions …\n",
      "[Day 4] finished 12400/15455 sessions …\n",
      "[Day 4] finished 12450/15455 sessions …\n",
      "[Day 4] finished 12500/15455 sessions …\n",
      "[Day 4] finished 12550/15455 sessions …\n",
      "[Day 4] finished 12600/15455 sessions …\n",
      "[Day 4] finished 12650/15455 sessions …\n",
      "[Day 4] finished 12700/15455 sessions …\n",
      "[Day 4] finished 12750/15455 sessions …\n",
      "[Day 4] finished 12800/15455 sessions …\n",
      "[Day 4] finished 12850/15455 sessions …\n",
      "[Day 4] finished 12900/15455 sessions …\n",
      "[Day 4] finished 12950/15455 sessions …\n",
      "[Day 4] finished 13000/15455 sessions …\n",
      "[Day 4] finished 13050/15455 sessions …\n",
      "[Day 4] finished 13100/15455 sessions …\n",
      "[Day 4] finished 13150/15455 sessions …\n",
      "[Day 4] finished 13200/15455 sessions …\n",
      "[Day 4] finished 13250/15455 sessions …\n",
      "[Day 4] finished 13300/15455 sessions …\n",
      "[Day 4] finished 13350/15455 sessions …\n",
      "[Day 4] finished 13400/15455 sessions …\n",
      "[Day 4] finished 13450/15455 sessions …\n",
      "[Day 4] finished 13500/15455 sessions …\n",
      "[Day 4] finished 13550/15455 sessions …\n",
      "[Day 4] finished 13600/15455 sessions …\n",
      "[Day 4] finished 13650/15455 sessions …\n",
      "[Day 4] finished 13700/15455 sessions …\n",
      "[Day 4] finished 13750/15455 sessions …\n",
      "[Day 4] finished 13800/15455 sessions …\n",
      "[Day 4] finished 13850/15455 sessions …\n",
      "[Day 4] finished 13900/15455 sessions …\n",
      "[Day 4] finished 13950/15455 sessions …\n",
      "[Day 4] finished 14000/15455 sessions …\n",
      "[Day 4] finished 14050/15455 sessions …\n",
      "[Day 4] finished 14100/15455 sessions …\n",
      "[Day 4] finished 14150/15455 sessions …\n",
      "[Day 4] finished 14200/15455 sessions …\n",
      "[Day 4] finished 14250/15455 sessions …\n",
      "[Day 4] finished 14300/15455 sessions …\n",
      "[Day 4] finished 14350/15455 sessions …\n",
      "[Day 4] finished 14400/15455 sessions …\n",
      "[Day 4] finished 14450/15455 sessions …\n",
      "[Day 4] finished 14500/15455 sessions …\n",
      "[Day 4] finished 14550/15455 sessions …\n",
      "[Day 4] finished 14600/15455 sessions …\n",
      "[Day 4] finished 14650/15455 sessions …\n",
      "[Day 4] finished 14700/15455 sessions …\n",
      "[Day 4] finished 14750/15455 sessions …\n",
      "[Day 4] finished 14800/15455 sessions …\n",
      "[Day 4] finished 14850/15455 sessions …\n",
      "[Day 4] finished 14900/15455 sessions …\n",
      "[Day 4] finished 14950/15455 sessions …\n",
      "[Day 4] finished 15000/15455 sessions …\n",
      "[Day 4] finished 15050/15455 sessions …\n",
      "[Day 4] finished 15100/15455 sessions …\n",
      "[Day 4] finished 15150/15455 sessions …\n",
      "[Day 4] finished 15200/15455 sessions …\n",
      "[Day 4] finished 15250/15455 sessions …\n",
      "[Day 4] finished 15300/15455 sessions …\n",
      "[Day 4] finished 15350/15455 sessions …\n",
      "[Day 4] finished 15400/15455 sessions …\n",
      "[Day 4] finished 15450/15455 sessions …\n",
      "D4 predictions shape: (45113145, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session</th>\n",
       "      <th>video_id</th>\n",
       "      <th>y_complete</th>\n",
       "      <th>y_long</th>\n",
       "      <th>y_rewatch</th>\n",
       "      <th>y_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1092</td>\n",
       "      <td>10</td>\n",
       "      <td>3649</td>\n",
       "      <td>0.2406</td>\n",
       "      <td>0.1994</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>0.4881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1092</td>\n",
       "      <td>10</td>\n",
       "      <td>9598</td>\n",
       "      <td>0.2189</td>\n",
       "      <td>0.2007</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.5077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1092</td>\n",
       "      <td>10</td>\n",
       "      <td>5262</td>\n",
       "      <td>0.2903</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>0.5696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1092</td>\n",
       "      <td>10</td>\n",
       "      <td>1963</td>\n",
       "      <td>0.2686</td>\n",
       "      <td>0.2281</td>\n",
       "      <td>0.0994</td>\n",
       "      <td>0.5218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1092</td>\n",
       "      <td>10</td>\n",
       "      <td>8234</td>\n",
       "      <td>0.2020</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>0.0620</td>\n",
       "      <td>0.5027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  session  video_id  y_complete  y_long  y_rewatch  y_neg\n",
       "0     1092       10      3649      0.2406  0.1994     0.0744 0.4881\n",
       "1     1092       10      9598      0.2189  0.2007     0.0351 0.5077\n",
       "2     1092       10      5262      0.2903  0.1933     0.1255 0.5696\n",
       "3     1092       10      1963      0.2686  0.2281     0.0994 0.5218\n",
       "4     1092       10      8234      0.2020  0.1632     0.0620 0.5027"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Fast D4 head prediction over all eligible sessions (CategoricalDtype-safe) ===\n",
    "import json, numpy as np, pandas as pd, lightgbm as lgb\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Load head models + Platt calibrators once\n",
    "# -----------------------------------------------------------------------------\n",
    "heads = [\"y_complete\",\"y_long\",\"y_rewatch\",\"y_neg\"]\n",
    "\n",
    "def _load_head(head_name: str):\n",
    "    head_dir = OUTDIR / head_name\n",
    "    booster = lgb.Booster(model_file=str(head_dir / f\"lgb_b1_{head_name}.txt\"))\n",
    "    with open(head_dir / \"calibrator.json\") as f:\n",
    "        c = json.load(f)\n",
    "    return booster, float(c.get(\"coef\", 1.0)), float(c.get(\"intercept\", 0.0))\n",
    "\n",
    "MODELS = {h: _load_head(h) for h in heads}\n",
    "\n",
    "def _predict_head(head_name: str, X_feat: pd.DataFrame) -> np.ndarray:\n",
    "    booster, a, b = MODELS[head_name]\n",
    "    margin = booster.predict(X_feat, num_iteration=booster.best_iteration, raw_score=True)\n",
    "    return 1.0 / (1.0 + np.exp(-(a*margin + b)))  # Platt-calibrated prob\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Reference schema for alignment (from your training split)\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    ref_df = train[feat_cols].copy()\n",
    "except NameError:\n",
    "    uniq_days = np.sort(b1[\"ts\"].dt.normalize().unique())\n",
    "    ref_df = b1.loc[b1[\"ts\"].dt.normalize().isin(uniq_days[:3]), feat_cols].copy()\n",
    "\n",
    "cat_cols_ref = [c for c in feat_cols\n",
    "                if c in ref_df.columns and isinstance(ref_df[c].dtype, CategoricalDtype)]\n",
    "\n",
    "def align_to_training_schema(X_pred: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Match training column order, categorical vocab, and numeric dtypes.\"\"\"\n",
    "    X = X_pred.reindex(columns=feat_cols, fill_value=np.nan).copy()\n",
    "    # enforce categorical vocab\n",
    "    for c in cat_cols_ref:\n",
    "        X[c] = pd.Categorical(X[c], categories=ref_df[c].cat.categories)\n",
    "    # cast numerics like training\n",
    "    for c in feat_cols:\n",
    "        if c in ref_df.columns and not isinstance(ref_df[c].dtype, CategoricalDtype):\n",
    "            try:\n",
    "                X[c] = X[c].astype(ref_df[c].dtype, copy=False)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return X\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Cache item-static features for all 2919 videos\n",
    "# -----------------------------------------------------------------------------\n",
    "ITEM_STATIC_COLS = [\n",
    "    \"video_id\",\"i_author_id\",\"i_video_type\",\"i_upload_type\",\"i_visible_status\",\n",
    "    \"video_width\",\"video_height\",\"i_music_id\",\"i_video_tag_id\",\n",
    "    \"i_video_duration_s\",\"i_aspect_ratio\",\"i_age_since_upload_days\",\"i_top_category_id\"\n",
    "]\n",
    "ITEM_STATIC = b1[ITEM_STATIC_COLS].drop_duplicates(\"video_id\").reset_index(drop=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Fast builder for one session’s 2919×64 matrix (recompute only author-recency)\n",
    "# -----------------------------------------------------------------------------\n",
    "SESSION_BROADCAST_COLS = [\n",
    "    'u_user_active_degree','u_is_lowactive_period','u_is_live_streamer','u_is_video_author',\n",
    "    'u_follow_user_num','u_follow_user_num_range','u_fans_user_num','u_fans_user_num_range',\n",
    "    'u_friend_user_num','u_friend_user_num_range','u_register_days','u_register_days_range',\n",
    "    'u_onehot_feat0','u_onehot_feat1','u_onehot_feat2','u_onehot_feat3','u_onehot_feat4','u_onehot_feat5',\n",
    "    'u_onehot_feat6','u_onehot_feat7','u_onehot_feat8','u_onehot_feat9','u_onehot_feat10','u_onehot_feat11',\n",
    "    'u_onehot_feat12','u_onehot_feat13','u_onehot_feat14','u_onehot_feat15','u_onehot_feat16','u_onehot_feat17',\n",
    "    'u_follow_user_num_log1p','u_fans_user_num_log1p','u_friend_user_num_log1p','u_register_days_log1p',\n",
    "    'ctx_hour_sin','ctx_hour_cos','ctx_dow',\n",
    "    'sess_index','prev_session_length_min','inter_session_gap_hours',\n",
    "    'hist_last3_complete_rate','hist_last10_complete_rate','hist_last3_wr_mean','hist_last3_wr_var',\n",
    "    'hist_ema_complete','hist_ema_wr_mean','hist_prev_within_sess_wr_slope',\n",
    "    'hist_user_cat_complete_ema','hist_cat_entropy',\n",
    "    'burst_id'\n",
    "]\n",
    "RANK_DEFAULT_COLS = ['sess_rank','sess_len','sess_rank_frac']  # deterministic defaults\n",
    "\n",
    "def build_candidate_matrix_for_session_fast(u: int, s: int, return_ids: bool = False):\n",
    "    g = b1[(b1.user_id == u) & (b1.session == s)]\n",
    "    if g.empty:\n",
    "        return (pd.DataFrame(columns=feat_cols), pd.DataFrame(columns=[\"video_id\"])) if return_ids else pd.DataFrame(columns=feat_cols)\n",
    "\n",
    "    row0 = g.iloc[0]\n",
    "    sess_start = g[\"ts\"].min()\n",
    "\n",
    "    # base: all items\n",
    "    C = ITEM_STATIC.copy()\n",
    "\n",
    "    # broadcast session/user/history cols\n",
    "    for c in SESSION_BROADCAST_COLS:\n",
    "        C[c] = row0[c] if c in b1.columns else pd.NA\n",
    "\n",
    "    # deterministic defaults for rank-ish features\n",
    "    sess_len = int(g[\"sess_len\"].iloc[0]) if \"sess_len\" in g.columns else int(g.shape[0])\n",
    "    C[\"sess_len\"] = sess_len\n",
    "    C[\"sess_rank\"] = sess_len + 1\n",
    "    C[\"sess_rank_frac\"] = 1.0\n",
    "\n",
    "    # recompute author recency per candidate\n",
    "    prior = (b1[(b1.user_id == u) & (b1[\"ts\"] < sess_start)]\n",
    "             .groupby(\"i_author_id\", as_index=False)[\"ts\"].max()\n",
    "             .rename(columns={\"ts\": \"last_seen_ts\"}))\n",
    "    C = C.merge(prior, on=\"i_author_id\", how=\"left\")\n",
    "    rec_days = (sess_start - C[\"last_seen_ts\"]).dt.total_seconds() / (3600*24)\n",
    "    C[\"hist_author_recency_days\"]  = rec_days.fillna(1e6).astype(\"float32\")\n",
    "    C[\"hist_author_recency_log1p\"] = np.log1p(C[\"hist_author_recency_days\"].clip(lower=0)).astype(\"float32\")\n",
    "    C.drop(columns=[\"last_seen_ts\"], inplace=True)\n",
    "\n",
    "    X = align_to_training_schema(C)\n",
    "    if return_ids:\n",
    "        return X, C[[\"video_id\"]]\n",
    "    return X\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Per-session prediction\n",
    "# -----------------------------------------------------------------------------\n",
    "def predict_heads_for_session(u: int, s: int) -> pd.DataFrame:\n",
    "    X, ids = build_candidate_matrix_for_session_fast(u, s, return_ids=True)\n",
    "    if X.empty:\n",
    "        return pd.DataFrame(columns=[\"user_id\",\"session\",\"video_id\"] + heads)\n",
    "    preds = {h: _predict_head(h, X) for h in heads}\n",
    "    out = pd.DataFrame(preds)\n",
    "    out.insert(0, \"video_id\", ids[\"video_id\"].values)\n",
    "    out.insert(0, \"session\", s)\n",
    "    out.insert(0, \"user_id\", u)\n",
    "    return out\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Multi-threaded D4 runner\n",
    "# -----------------------------------------------------------------------------\n",
    "def predict_heads_for_dayindex_fast(day_idx: int,\n",
    "                                    *, min_exposures: int = 3,\n",
    "                                    max_sessions: int | None = None,\n",
    "                                    max_workers: int = 6,\n",
    "                                    save_dir: Path | None = None) -> pd.DataFrame:\n",
    "    uniq_days = np.sort(b1[\"ts\"].dt.normalize().unique())\n",
    "    if day_idx < 0 or day_idx >= len(uniq_days):\n",
    "        raise IndexError(f\"day_idx out of range; have {len(uniq_days)} unique days.\")\n",
    "    d = uniq_days[day_idx]\n",
    "    day_mask = b1[\"ts\"].dt.normalize() == d\n",
    "\n",
    "    sess_counts = (b1.loc[day_mask, [\"user_id\",\"session\"]]\n",
    "                     .value_counts().rename(\"n_exposures\").reset_index())\n",
    "    keep = sess_counts[sess_counts[\"n_exposures\"] >= min_exposures][[\"user_id\",\"session\"]]\n",
    "    if max_sessions is not None and len(keep) > max_sessions:\n",
    "        keep = keep.sample(n=max_sessions, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    out_path = None\n",
    "    if save_dir is not None:\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        out_path = save_dir / f\"head_probs_day{day_idx+1}.csv\"\n",
    "        if out_path.exists(): out_path.unlink()\n",
    "\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = [ex.submit(predict_heads_for_session, int(u), int(s))\n",
    "                for u, s in keep.itertuples(index=False)]\n",
    "        for i, fut in enumerate(as_completed(futs), 1):\n",
    "            df_i = fut.result()\n",
    "            if df_i is not None and not df_i.empty:\n",
    "                if out_path is not None:\n",
    "                    df_i.to_csv(out_path, mode=\"a\", index=False, header=not out_path.exists())\n",
    "                results.append(df_i)\n",
    "            if i % 500 == 0:\n",
    "                print(f\"[Day {int(day_idx)+1}] finished {i}/{len(keep)} sessions …\")\n",
    "\n",
    "    return (pd.concat(results, ignore_index=True)\n",
    "            if results else pd.DataFrame(columns=[\"user_id\",\"session\",\"video_id\"]+heads))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Run for D4 and preview\n",
    "# -----------------------------------------------------------------------------\n",
    "d4_idx = 3  # 4th unique day\n",
    "save_dir = OUTDIR / \"policy\" / \"heads_by_day_fast\"\n",
    "\n",
    "pred_d4 = predict_heads_for_dayindex_fast(d4_idx, min_exposures=3, max_sessions=None,\n",
    "                                          max_workers=6, save_dir=save_dir)\n",
    "print(\"D4 predictions shape:\", pred_d4.shape)\n",
    "display(pred_d4.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9fe7e0e4-db29-4387-b5c1-31ee69fb098a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_d4 shape: (45113145, 7)\n",
      "Unique sessions: 15,455\n",
      "Unique videos:   2,919\n",
      "Per-session item count  min/max/mean: 2919 2919 2919\n",
      "Expected rows = 45,113,145\n",
      "y_complete: min=0.002257, max=0.999492\n",
      "y_long: min=0.000544, max=0.998993\n",
      "y_rewatch: min=0.000137, max=0.847327\n",
      "y_neg: min=0.000068, max=0.984444\n",
      "Approx. memory (GB): 2.57\n",
      "Saved head predictions to: /Users/haozhangao/Desktop/RecSys Research/KuaiRec 2.0/data/models/burst1/policy/heads_by_day_fast/heads_D4.parquet\n",
      "Saved manifest to: /Users/haozhangao/Desktop/RecSys Research/KuaiRec 2.0/data/models/burst1/policy/heads_by_day_fast/heads_D4_manifest.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1) Basic shape & sanity checks ---\n",
    "print(\"pred_d4 shape:\", pred_d4.shape)\n",
    "\n",
    "# unique sessions & videos\n",
    "n_sessions = pred_d4[[\"user_id\",\"session\"]].drop_duplicates().shape[0]\n",
    "n_videos   = pred_d4[\"video_id\"].nunique()\n",
    "print(f\"Unique sessions: {n_sessions:,}\")\n",
    "print(f\"Unique videos:   {n_videos:,}\")\n",
    "\n",
    "# per-session row counts (should all be N_videos)\n",
    "per_sess_counts = pred_d4.groupby([\"user_id\",\"session\"]).size()\n",
    "print(\"Per-session item count  min/max/mean:\",\n",
    "      per_sess_counts.min(), per_sess_counts.max(), int(per_sess_counts.mean()))\n",
    "\n",
    "# expected total rows check\n",
    "expected_rows = n_sessions * n_videos\n",
    "print(f\"Expected rows = {expected_rows:,}\")\n",
    "assert len(pred_d4) == expected_rows, \"Row count mismatch! (some sessions may be missing videos)\"\n",
    "\n",
    "# probability range checks\n",
    "for c in [\"y_complete\",\"y_long\",\"y_rewatch\",\"y_neg\"]:\n",
    "    m, M = pred_d4[c].min(), pred_d4[c].max()\n",
    "    print(f\"{c}: min={m:.6f}, max={M:.6f}\")\n",
    "    assert np.isfinite(pred_d4[c]).all() and (pred_d4[c].between(0,1).all()), f\"{c} out of [0,1]!\"\n",
    "\n",
    "# memory footprint\n",
    "mem_gb = pred_d4.memory_usage(deep=True).sum() / 1e9\n",
    "print(f\"Approx. memory (GB): {mem_gb:0.2f}\")\n",
    "\n",
    "# --- 2) Save (Parquet, compressed) ---\n",
    "save_dir = OUTDIR / \"policy\" / \"heads_by_day_fast\"\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# downcast to float32 to cut size ~50%\n",
    "for c in [\"y_complete\",\"y_long\",\"y_rewatch\",\"y_neg\"]:\n",
    "    pred_d4[c] = pred_d4[c].astype(\"float32\")\n",
    "\n",
    "out_path = save_dir / \"heads_D4.parquet\"\n",
    "pred_d4.to_parquet(out_path, index=False)  # snappy compression via pyarrow\n",
    "print(\"Saved head predictions to:\", out_path)\n",
    "\n",
    "# manifest: rows per session (handy for fast reads later)\n",
    "manifest = (pred_d4.groupby([\"user_id\",\"session\"]).size()\n",
    "            .rename(\"n_rows\").reset_index())\n",
    "manifest_path = save_dir / \"heads_D4_manifest.csv\"\n",
    "manifest.to_csv(manifest_path, index=False)\n",
    "print(\"Saved manifest to:\", manifest_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f40f6ff0-8347-46da-8758-41237ede89ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on session (user_id=1092, session=10)  rows=2919\n",
      "Sum of item probs: 1.0\n",
      "Sum of category probs: 1.0\n",
      "\n",
      "Top-5 items by p_item:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>p_item</th>\n",
       "      <th>i_top_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>6705</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>8143</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>9503</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>1894</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>1904</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id  p_item i_top_category_id\n",
       "1045      6705  0.0004                 8\n",
       "1041      8143  0.0004                 8\n",
       "1236      9503  0.0004                 8\n",
       "1273      1894  0.0004                28\n",
       "970       1904  0.0004                11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-10 categories by p_cat:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_top_category_id</th>\n",
       "      <th>p_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>28</td>\n",
       "      <td>0.1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>34</td>\n",
       "      <td>0.0848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19</td>\n",
       "      <td>0.0536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>25</td>\n",
       "      <td>0.0387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>0.0293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>0.0240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>31</td>\n",
       "      <td>0.0223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32</td>\n",
       "      <td>0.0211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "      <td>0.0205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>33</td>\n",
       "      <td>0.0186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>0.0148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>26</td>\n",
       "      <td>0.0147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>0.0065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>29</td>\n",
       "      <td>0.0065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>35</td>\n",
       "      <td>0.0062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>39</td>\n",
       "      <td>0.0061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i_top_category_id  p_cat\n",
       "21                28 0.1042\n",
       "34                 5 0.0921\n",
       "27                34 0.0848\n",
       "37                 8 0.0654\n",
       "11                19 0.0536\n",
       "3                 11 0.0531\n",
       "4                 12 0.0444\n",
       "38                 9 0.0426\n",
       "18                25 0.0387\n",
       "1                  1 0.0367\n",
       "35                 6 0.0355\n",
       "36                 7 0.0295\n",
       "7                 15 0.0293\n",
       "13                20 0.0268\n",
       "5                 13 0.0240\n",
       "24                31 0.0223\n",
       "25                32 0.0211\n",
       "9                 17 0.0205\n",
       "26                33 0.0186\n",
       "12                 2 0.0184\n",
       "33                 4 0.0168\n",
       "8                 16 0.0148\n",
       "19                26 0.0147\n",
       "2                 10 0.0119\n",
       "10                18 0.0113\n",
       "23                 3 0.0103\n",
       "6                 14 0.0065\n",
       "22                29 0.0065\n",
       "28                35 0.0062\n",
       "32                39 0.0061"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Inputs assumed available ---\n",
    "# pred_d4: columns [\"user_id\",\"session\",\"video_id\",\"y_complete\",\"y_long\",\"y_rewatch\",\"y_neg\"]\n",
    "# b1:      has [\"video_id\",\"i_top_category_id\"]\n",
    "\n",
    "# 0) Build a stable video_id -> category map (one row per video)\n",
    "vid2cat = (\n",
    "    b1[[\"video_id\",\"i_top_category_id\"]]\n",
    "      .drop_duplicates(\"video_id\")\n",
    "      .set_index(\"video_id\")[\"i_top_category_id\"]\n",
    ")\n",
    "\n",
    "# 1) Helpers\n",
    "def softmax1d(u: np.ndarray, tau: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Numerically-stable softmax over a 1D array u / tau.\"\"\"\n",
    "    z = u / float(tau)\n",
    "    m = np.max(z)\n",
    "    p = np.exp(z - m)\n",
    "    p /= p.sum()\n",
    "    return p\n",
    "\n",
    "def session_item_and_category_probs(\n",
    "    df_heads_session: pd.DataFrame,\n",
    "    vid2cat: pd.Series,\n",
    "    w: np.ndarray,\n",
    "    tau: float = 1.0\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Given head predictions for ONE session (2919 rows) and weights w (len=4),\n",
    "    return:\n",
    "      - item_probs:  [video_id, p_item]\n",
    "      - cat_probs:   [i_top_category_id, p_cat]  (sum over items in each category)\n",
    "    \"\"\"\n",
    "    # compute utilities U_j = w^T * head_probs_j\n",
    "    H = df_heads_session[[\"y_complete\",\"y_long\",\"y_rewatch\",\"y_neg\"]].to_numpy(dtype=\"float64\")\n",
    "    U = H @ w\n",
    "\n",
    "    # item softmax\n",
    "    p_item = softmax1d(U, tau=tau)\n",
    "\n",
    "    item_probs = df_heads_session[[\"video_id\"]].copy()\n",
    "    item_probs[\"p_item\"] = p_item.astype(\"float64\")\n",
    "\n",
    "    # attach category per video and aggregate\n",
    "    item_probs[\"i_top_category_id\"] = item_probs[\"video_id\"].map(vid2cat).values\n",
    "    cat_probs = (item_probs.groupby(\"i_top_category_id\", as_index=False)[\"p_item\"]\n",
    "                           .sum()\n",
    "                           .rename(columns={\"p_item\":\"p_cat\"}))\n",
    "    # normalize defensively (should already sum to 1)\n",
    "    s = float(cat_probs[\"p_cat\"].sum())\n",
    "    if s > 0:\n",
    "        cat_probs[\"p_cat\"] = cat_probs[\"p_cat\"] / s\n",
    "\n",
    "    return item_probs, cat_probs\n",
    "\n",
    "# 2) Test on the FIRST D4 session\n",
    "# pick the first (user_id, session) pair in pred_d4\n",
    "first_pair = pred_d4[[\"user_id\",\"session\"]].iloc[0].to_list()\n",
    "u0, s0 = int(first_pair[0]), int(first_pair[1])\n",
    "\n",
    "df_one = pred_d4[(pred_d4.user_id==u0) & (pred_d4.session==s0)].copy()\n",
    "print(f\"Testing on session (user_id={u0}, session={s0})  rows={len(df_one)}\")\n",
    "\n",
    "# example weights (you can change later)\n",
    "w_test = np.array([0.05, 0.03, 0.02, -0.05], dtype=\"float64\")\n",
    "tau = 1.0\n",
    "\n",
    "item_probs, cat_probs = session_item_and_category_probs(df_one, vid2cat, w_test, tau=tau)\n",
    "\n",
    "# 3) Quick sanity checks / preview\n",
    "print(\"Sum of item probs:\", item_probs[\"p_item\"].sum())\n",
    "print(\"Sum of category probs:\", cat_probs[\"p_cat\"].sum())\n",
    "\n",
    "print(\"\\nTop-5 items by p_item:\")\n",
    "display(item_probs.sort_values(\"p_item\", ascending=False).head(5))\n",
    "\n",
    "print(\"\\nTop-10 categories by p_cat:\")\n",
    "display(cat_probs.sort_values(\"p_cat\", ascending=False).head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "238773d0-86b0-43cc-9166-7a3c7067a2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 16.619739\n",
      "         Iterations: 59\n",
      "         Function evaluations: 135\n",
      "Status: Optimization terminated successfully. | success: True\n",
      "Learned w: [ 0.032791  0.883349  0.007782 -0.018293]\n",
      "Final (avg) CE: 16.619739359599635\n",
      "w: [0. 0. 0. 0.]  -> CE: 17.384395221707404\n",
      "w: [ 0.05  0.03  0.02 -0.05]  -> CE: 17.40793936081155\n",
      "w: [ 0.2   0.1   0.05 -0.15]  -> CE: 17.358627996747558\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Assumes available:\n",
    "# - packs_d4: list of dicts:\n",
    "#     {\"user_id\": int,\n",
    "#      \"session\": int,\n",
    "#      \"H\":  (n_items, 4)  # head probs (in order [complete,long,rewatch,neg])\n",
    "#      \"inv\": (n_items,)   # item -> category index\n",
    "#      \"cats\": (K,)        # category ids for this session's candidate set\n",
    "#      \"q\":   (K,) }       # empirical category PMF, sums to 1 (smoothed)\n",
    "#\n",
    "# - emp: DataFrame with [\"user_id\",\"session\",\"n_total\"] (from your empirical build),\n",
    "#        where n_total is the number of exposures that session had\n",
    "#\n",
    "# If you don’t already have the exposure counts per (u,s):\n",
    "e_by_sess = (emp[[\"user_id\",\"session\",\"n_total\"]]\n",
    "             .drop_duplicates([\"user_id\",\"session\"])\n",
    "             .set_index([\"user_id\",\"session\"])[\"n_total\"]\n",
    "             .to_dict())\n",
    "\n",
    "heads = [\"y_complete\",\"y_long\",\"y_rewatch\",\"y_neg\"]  # ordering used in H\n",
    "\n",
    "def theta_to_w(theta: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Map unconstrained theta -> signed weights with desired semantics:\n",
    "      w_c, w_l, w_r >= 0; w_neg <= 0\n",
    "    \"\"\"\n",
    "    # theta shape (4,)\n",
    "    return np.array([\n",
    "        np.exp(theta[0]),   # w_complete >= 0\n",
    "        np.exp(theta[1]),   # w_long     >= 0\n",
    "        np.exp(theta[2]),   # w_rewatch  >= 0\n",
    "        -np.exp(theta[3])   # w_neg      <= 0\n",
    "    ], dtype=\"float64\")\n",
    "\n",
    "def topk_category_ce_for_session(P, w: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    One session:\n",
    "      - score items: U = H @ w\n",
    "      - take top-K where K = exposure count for this (user, session)\n",
    "      - aggregate category distribution over those top-K (uniform 1/K each)\n",
    "      - return CE(q || pi_topK) = -sum_k q_k log pi_k\n",
    "    \"\"\"\n",
    "    u, s = P[\"user_id\"], P[\"session\"]\n",
    "    K_s = int(e_by_sess.get((u, s), 0))\n",
    "    if K_s <= 0:\n",
    "        return 0.0  # session not in exposure table (shouldn't happen if packs built from emp)\n",
    "\n",
    "    H   = P[\"H\"]        # (n_items, 4)\n",
    "    inv = P[\"inv\"]      # (n_items,)\n",
    "    q   = P[\"q\"]        # (K_cat,)\n",
    "\n",
    "    U = H @ w                      # (n_items,)\n",
    "    # top-K indices (stable ties)\n",
    "    K_take = min(K_s, U.shape[0])\n",
    "    idx = np.argpartition(U, -K_take)[-K_take:]   # O(n) selection\n",
    "    # If you prefer exactly sorted top-K (costlier):\n",
    "    idx = idx[np.argsort(U[idx])[::-1]]\n",
    "\n",
    "    # Aggregate category mass among the top-K (uniform over the slate)\n",
    "    K_cat = q.shape[0]\n",
    "    pi = np.zeros(K_cat, dtype=\"float64\")\n",
    "    for j in idx:\n",
    "        pi[inv[j]] += 1.0\n",
    "    pi /= K_take\n",
    "\n",
    "    # Cross-entropy: CE(q || pi) = - sum q log pi\n",
    "    pi_safe = np.clip(pi, 1e-12, 1.0)\n",
    "    ce = -float(np.sum(q * np.log(pi_safe)))\n",
    "    return ce\n",
    "\n",
    "def topk_ce_loss_theta(theta: np.ndarray, packs) -> float:\n",
    "    \"\"\"\n",
    "    Objective over all sessions (average CE) using Top-K slate matching.\n",
    "    theta -> w (signed), then sum per-session CE.\n",
    "    \"\"\"\n",
    "    w = theta_to_w(theta)\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    for P in packs:\n",
    "        # skip if we don't have an exposure count\n",
    "        if (P[\"user_id\"], P[\"session\"]) not in e_by_sess:\n",
    "            continue\n",
    "        total += topk_category_ce_for_session(P, w)\n",
    "        count += 1\n",
    "    if count == 0:\n",
    "        return np.inf\n",
    "    return total / count\n",
    "\n",
    "# -----------------------\n",
    "# Quick test / fit on D4\n",
    "# -----------------------\n",
    "# init theta so that w starts near [0.05, 0.03, 0.02, -0.05]\n",
    "theta0 = np.log(np.array([0.05, 0.03, 0.02, 0.05], dtype=\"float64\"))  # neg uses +0.05 then flipped by theta_to_w\n",
    "\n",
    "obj = lambda th: topk_ce_loss_theta(th, packs_d4)\n",
    "\n",
    "# Use gradient-free optimizer (Nelder-Mead). You can also try Powell.\n",
    "res = minimize(obj, theta0, method=\"Nelder-Mead\",\n",
    "               options=dict(maxiter=300, xatol=1e-4, fatol=1e-6, disp=True))\n",
    "\n",
    "w_hat = theta_to_w(res.x)\n",
    "print(\"Status:\", res.message, \"| success:\", res.success)\n",
    "print(\"Learned w:\", np.round(w_hat, 6))\n",
    "print(\"Final (avg) CE:\", res.fun)\n",
    "\n",
    "# # Sanity check: evaluate at a few hand-picked weights\n",
    "# for w_try in [np.array([0.0,0.0,0.0,0.0]),\n",
    "#               np.array([0.05,0.03,0.02,-0.05]),\n",
    "#               np.array([0.2,0.1,0.05,-0.15])]:\n",
    "#     # map back to theta just for printing a comparable value\n",
    "#     th_try = np.log(np.array([max(w_try[0],1e-6),\n",
    "#                               max(w_try[1],1e-6),\n",
    "#                               max(w_try[2],1e-6),\n",
    "#                               max(-w_try[3],1e-6)]))\n",
    "#     print(\"w:\", np.round(w_try, 4), \" -> CE:\", obj(th_try))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e19977a5-8104-42fd-a059-16bcd51edff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using weights: [ 0.032791  0.883349  0.007782 -0.018293]\n",
      "[simulate] Day index 4 (2020-07-08) …\n",
      "[day5] predictions not found on disk; computing …\n",
      "[Day 5] finished 50/15770 sessions …\n",
      "[Day 5] finished 100/15770 sessions …\n",
      "[Day 5] finished 150/15770 sessions …\n",
      "[Day 5] finished 200/15770 sessions …\n",
      "[Day 5] finished 250/15770 sessions …\n",
      "[Day 5] finished 300/15770 sessions …\n",
      "[Day 5] finished 350/15770 sessions …\n",
      "[Day 5] finished 400/15770 sessions …\n",
      "[Day 5] finished 450/15770 sessions …\n",
      "[Day 5] finished 500/15770 sessions …\n",
      "[Day 5] finished 550/15770 sessions …\n",
      "[Day 5] finished 600/15770 sessions …\n",
      "[Day 5] finished 650/15770 sessions …\n",
      "[Day 5] finished 700/15770 sessions …\n",
      "[Day 5] finished 750/15770 sessions …\n",
      "[Day 5] finished 800/15770 sessions …\n",
      "[Day 5] finished 850/15770 sessions …\n",
      "[Day 5] finished 900/15770 sessions …\n",
      "[Day 5] finished 950/15770 sessions …\n",
      "[Day 5] finished 1000/15770 sessions …\n",
      "[Day 5] finished 1050/15770 sessions …\n",
      "[Day 5] finished 1100/15770 sessions …\n",
      "[Day 5] finished 1150/15770 sessions …\n",
      "[Day 5] finished 1200/15770 sessions …\n",
      "[Day 5] finished 1250/15770 sessions …\n",
      "[Day 5] finished 1300/15770 sessions …\n",
      "[Day 5] finished 1350/15770 sessions …\n",
      "[Day 5] finished 1400/15770 sessions …\n",
      "[Day 5] finished 1450/15770 sessions …\n",
      "[Day 5] finished 1500/15770 sessions …\n",
      "[Day 5] finished 1550/15770 sessions …\n",
      "[Day 5] finished 1600/15770 sessions …\n",
      "[Day 5] finished 1650/15770 sessions …\n",
      "[Day 5] finished 1700/15770 sessions …\n",
      "[Day 5] finished 1750/15770 sessions …\n",
      "[Day 5] finished 1800/15770 sessions …\n",
      "[Day 5] finished 1850/15770 sessions …\n",
      "[Day 5] finished 1900/15770 sessions …\n",
      "[Day 5] finished 1950/15770 sessions …\n",
      "[Day 5] finished 2000/15770 sessions …\n",
      "[Day 5] finished 2050/15770 sessions …\n",
      "[Day 5] finished 2100/15770 sessions …\n",
      "[Day 5] finished 2150/15770 sessions …\n",
      "[Day 5] finished 2200/15770 sessions …\n",
      "[Day 5] finished 2250/15770 sessions …\n",
      "[Day 5] finished 2300/15770 sessions …\n",
      "[Day 5] finished 2350/15770 sessions …\n",
      "[Day 5] finished 2400/15770 sessions …\n",
      "[Day 5] finished 2450/15770 sessions …\n",
      "[Day 5] finished 2500/15770 sessions …\n",
      "[Day 5] finished 2550/15770 sessions …\n",
      "[Day 5] finished 2600/15770 sessions …\n",
      "[Day 5] finished 2650/15770 sessions …\n",
      "[Day 5] finished 2700/15770 sessions …\n",
      "[Day 5] finished 2750/15770 sessions …\n",
      "[Day 5] finished 2800/15770 sessions …\n",
      "[Day 5] finished 2850/15770 sessions …\n",
      "[Day 5] finished 2900/15770 sessions …\n",
      "[Day 5] finished 2950/15770 sessions …\n",
      "[Day 5] finished 3000/15770 sessions …\n",
      "[Day 5] finished 3050/15770 sessions …\n",
      "[Day 5] finished 3100/15770 sessions …\n",
      "[Day 5] finished 3150/15770 sessions …\n",
      "[Day 5] finished 3200/15770 sessions …\n",
      "[Day 5] finished 3250/15770 sessions …\n",
      "[Day 5] finished 3300/15770 sessions …\n",
      "[Day 5] finished 3350/15770 sessions …\n",
      "[Day 5] finished 3400/15770 sessions …\n",
      "[Day 5] finished 3450/15770 sessions …\n",
      "[Day 5] finished 3500/15770 sessions …\n",
      "[Day 5] finished 3550/15770 sessions …\n",
      "[Day 5] finished 3600/15770 sessions …\n",
      "[Day 5] finished 3650/15770 sessions …\n",
      "[Day 5] finished 3700/15770 sessions …\n",
      "[Day 5] finished 3750/15770 sessions …\n",
      "[Day 5] finished 3800/15770 sessions …\n",
      "[Day 5] finished 3850/15770 sessions …\n",
      "[Day 5] finished 3900/15770 sessions …\n",
      "[Day 5] finished 3950/15770 sessions …\n",
      "[Day 5] finished 4000/15770 sessions …\n",
      "[Day 5] finished 4050/15770 sessions …\n",
      "[Day 5] finished 4100/15770 sessions …\n",
      "[Day 5] finished 4150/15770 sessions …\n",
      "[Day 5] finished 4200/15770 sessions …\n",
      "[Day 5] finished 4250/15770 sessions …\n",
      "[Day 5] finished 4300/15770 sessions …\n",
      "[Day 5] finished 4350/15770 sessions …\n",
      "[Day 5] finished 4400/15770 sessions …\n",
      "[Day 5] finished 4450/15770 sessions …\n",
      "[Day 5] finished 4500/15770 sessions …\n",
      "[Day 5] finished 4550/15770 sessions …\n",
      "[Day 5] finished 4600/15770 sessions …\n",
      "[Day 5] finished 4650/15770 sessions …\n",
      "[Day 5] finished 4700/15770 sessions …\n",
      "[Day 5] finished 4750/15770 sessions …\n",
      "[Day 5] finished 4800/15770 sessions …\n",
      "[Day 5] finished 4850/15770 sessions …\n",
      "[Day 5] finished 4900/15770 sessions …\n",
      "[Day 5] finished 4950/15770 sessions …\n",
      "[Day 5] finished 5000/15770 sessions …\n",
      "[Day 5] finished 5050/15770 sessions …\n",
      "[Day 5] finished 5100/15770 sessions …\n",
      "[Day 5] finished 5150/15770 sessions …\n",
      "[Day 5] finished 5200/15770 sessions …\n",
      "[Day 5] finished 5250/15770 sessions …\n",
      "[Day 5] finished 5300/15770 sessions …\n",
      "[Day 5] finished 5350/15770 sessions …\n",
      "[Day 5] finished 5400/15770 sessions …\n",
      "[Day 5] finished 5450/15770 sessions …\n",
      "[Day 5] finished 5500/15770 sessions …\n",
      "[Day 5] finished 5550/15770 sessions …\n",
      "[Day 5] finished 5600/15770 sessions …\n",
      "[Day 5] finished 5650/15770 sessions …\n",
      "[Day 5] finished 5700/15770 sessions …\n",
      "[Day 5] finished 5750/15770 sessions …\n",
      "[Day 5] finished 5800/15770 sessions …\n",
      "[Day 5] finished 5850/15770 sessions …\n",
      "[Day 5] finished 5900/15770 sessions …\n",
      "[Day 5] finished 5950/15770 sessions …\n",
      "[Day 5] finished 6000/15770 sessions …\n",
      "[Day 5] finished 6050/15770 sessions …\n",
      "[Day 5] finished 6100/15770 sessions …\n",
      "[Day 5] finished 6150/15770 sessions …\n",
      "[Day 5] finished 6200/15770 sessions …\n",
      "[Day 5] finished 6250/15770 sessions …\n",
      "[Day 5] finished 6300/15770 sessions …\n",
      "[Day 5] finished 6350/15770 sessions …\n",
      "[Day 5] finished 6400/15770 sessions …\n",
      "[Day 5] finished 6450/15770 sessions …\n",
      "[Day 5] finished 6500/15770 sessions …\n",
      "[Day 5] finished 6550/15770 sessions …\n",
      "[Day 5] finished 6600/15770 sessions …\n",
      "[Day 5] finished 6650/15770 sessions …\n",
      "[Day 5] finished 6700/15770 sessions …\n",
      "[Day 5] finished 6750/15770 sessions …\n",
      "[Day 5] finished 6800/15770 sessions …\n",
      "[Day 5] finished 6850/15770 sessions …\n",
      "[Day 5] finished 6900/15770 sessions …\n",
      "[Day 5] finished 6950/15770 sessions …\n",
      "[Day 5] finished 7000/15770 sessions …\n",
      "[Day 5] finished 7050/15770 sessions …\n",
      "[Day 5] finished 7100/15770 sessions …\n",
      "[Day 5] finished 7150/15770 sessions …\n",
      "[Day 5] finished 7200/15770 sessions …\n",
      "[Day 5] finished 7250/15770 sessions …\n",
      "[Day 5] finished 7300/15770 sessions …\n",
      "[Day 5] finished 7350/15770 sessions …\n",
      "[Day 5] finished 7400/15770 sessions …\n",
      "[Day 5] finished 7450/15770 sessions …\n",
      "[Day 5] finished 7500/15770 sessions …\n",
      "[Day 5] finished 7550/15770 sessions …\n",
      "[Day 5] finished 7600/15770 sessions …\n",
      "[Day 5] finished 7650/15770 sessions …\n",
      "[Day 5] finished 7700/15770 sessions …\n",
      "[Day 5] finished 7750/15770 sessions …\n",
      "[Day 5] finished 7800/15770 sessions …\n",
      "[Day 5] finished 7850/15770 sessions …\n",
      "[Day 5] finished 7900/15770 sessions …\n",
      "[Day 5] finished 7950/15770 sessions …\n",
      "[Day 5] finished 8000/15770 sessions …\n",
      "[Day 5] finished 8050/15770 sessions …\n",
      "[Day 5] finished 8100/15770 sessions …\n",
      "[Day 5] finished 8150/15770 sessions …\n",
      "[Day 5] finished 8200/15770 sessions …\n",
      "[Day 5] finished 8250/15770 sessions …\n",
      "[Day 5] finished 8300/15770 sessions …\n",
      "[Day 5] finished 8350/15770 sessions …\n",
      "[Day 5] finished 8400/15770 sessions …\n",
      "[Day 5] finished 8450/15770 sessions …\n",
      "[Day 5] finished 8500/15770 sessions …\n",
      "[Day 5] finished 8550/15770 sessions …\n",
      "[Day 5] finished 8600/15770 sessions …\n",
      "[Day 5] finished 8650/15770 sessions …\n",
      "[Day 5] finished 8700/15770 sessions …\n",
      "[Day 5] finished 8750/15770 sessions …\n",
      "[Day 5] finished 8800/15770 sessions …\n",
      "[Day 5] finished 8850/15770 sessions …\n",
      "[Day 5] finished 8900/15770 sessions …\n",
      "[Day 5] finished 8950/15770 sessions …\n",
      "[Day 5] finished 9000/15770 sessions …\n",
      "[Day 5] finished 9050/15770 sessions …\n",
      "[Day 5] finished 9100/15770 sessions …\n",
      "[Day 5] finished 9150/15770 sessions …\n",
      "[Day 5] finished 9200/15770 sessions …\n",
      "[Day 5] finished 9250/15770 sessions …\n",
      "[Day 5] finished 9300/15770 sessions …\n",
      "[Day 5] finished 9350/15770 sessions …\n",
      "[Day 5] finished 9400/15770 sessions …\n",
      "[Day 5] finished 9450/15770 sessions …\n",
      "[Day 5] finished 9500/15770 sessions …\n",
      "[Day 5] finished 9550/15770 sessions …\n",
      "[Day 5] finished 9600/15770 sessions …\n",
      "[Day 5] finished 9650/15770 sessions …\n",
      "[Day 5] finished 9700/15770 sessions …\n",
      "[Day 5] finished 9750/15770 sessions …\n",
      "[Day 5] finished 9800/15770 sessions …\n",
      "[Day 5] finished 9850/15770 sessions …\n",
      "[Day 5] finished 9900/15770 sessions …\n",
      "[Day 5] finished 9950/15770 sessions …\n",
      "[Day 5] finished 10000/15770 sessions …\n",
      "[Day 5] finished 10050/15770 sessions …\n",
      "[Day 5] finished 10100/15770 sessions …\n",
      "[Day 5] finished 10150/15770 sessions …\n",
      "[Day 5] finished 10200/15770 sessions …\n",
      "[Day 5] finished 10250/15770 sessions …\n",
      "[Day 5] finished 10300/15770 sessions …\n",
      "[Day 5] finished 10350/15770 sessions …\n",
      "[Day 5] finished 10400/15770 sessions …\n",
      "[Day 5] finished 10450/15770 sessions …\n",
      "[Day 5] finished 10500/15770 sessions …\n",
      "[Day 5] finished 10550/15770 sessions …\n",
      "[Day 5] finished 10600/15770 sessions …\n",
      "[Day 5] finished 10650/15770 sessions …\n",
      "[Day 5] finished 10700/15770 sessions …\n",
      "[Day 5] finished 10750/15770 sessions …\n",
      "[Day 5] finished 10800/15770 sessions …\n",
      "[Day 5] finished 10850/15770 sessions …\n",
      "[Day 5] finished 10900/15770 sessions …\n",
      "[Day 5] finished 10950/15770 sessions …\n",
      "[Day 5] finished 11000/15770 sessions …\n",
      "[Day 5] finished 11050/15770 sessions …\n",
      "[Day 5] finished 11100/15770 sessions …\n",
      "[Day 5] finished 11150/15770 sessions …\n",
      "[Day 5] finished 11200/15770 sessions …\n",
      "[Day 5] finished 11250/15770 sessions …\n",
      "[Day 5] finished 11300/15770 sessions …\n",
      "[Day 5] finished 11350/15770 sessions …\n",
      "[Day 5] finished 11400/15770 sessions …\n",
      "[Day 5] finished 11450/15770 sessions …\n",
      "[Day 5] finished 11500/15770 sessions …\n",
      "[Day 5] finished 11550/15770 sessions …\n",
      "[Day 5] finished 11600/15770 sessions …\n",
      "[Day 5] finished 11650/15770 sessions …\n",
      "[Day 5] finished 11700/15770 sessions …\n",
      "[Day 5] finished 11750/15770 sessions …\n",
      "[Day 5] finished 11800/15770 sessions …\n",
      "[Day 5] finished 11850/15770 sessions …\n",
      "[Day 5] finished 11900/15770 sessions …\n",
      "[Day 5] finished 11950/15770 sessions …\n",
      "[Day 5] finished 12000/15770 sessions …\n",
      "[Day 5] finished 12050/15770 sessions …\n",
      "[Day 5] finished 12100/15770 sessions …\n",
      "[Day 5] finished 12150/15770 sessions …\n",
      "[Day 5] finished 12200/15770 sessions …\n",
      "[Day 5] finished 12250/15770 sessions …\n",
      "[Day 5] finished 12300/15770 sessions …\n",
      "[Day 5] finished 12350/15770 sessions …\n",
      "[Day 5] finished 12400/15770 sessions …\n",
      "[Day 5] finished 12450/15770 sessions …\n",
      "[Day 5] finished 12500/15770 sessions …\n",
      "[Day 5] finished 12550/15770 sessions …\n",
      "[Day 5] finished 12600/15770 sessions …\n",
      "[Day 5] finished 12650/15770 sessions …\n",
      "[Day 5] finished 12700/15770 sessions …\n",
      "[Day 5] finished 12750/15770 sessions …\n",
      "[Day 5] finished 12800/15770 sessions …\n",
      "[Day 5] finished 12850/15770 sessions …\n",
      "[Day 5] finished 12900/15770 sessions …\n",
      "[Day 5] finished 12950/15770 sessions …\n",
      "[Day 5] finished 13000/15770 sessions …\n",
      "[Day 5] finished 13050/15770 sessions …\n",
      "[Day 5] finished 13100/15770 sessions …\n",
      "[Day 5] finished 13150/15770 sessions …\n",
      "[Day 5] finished 13200/15770 sessions …\n",
      "[Day 5] finished 13250/15770 sessions …\n",
      "[Day 5] finished 13300/15770 sessions …\n",
      "[Day 5] finished 13350/15770 sessions …\n",
      "[Day 5] finished 13400/15770 sessions …\n",
      "[Day 5] finished 13450/15770 sessions …\n",
      "[Day 5] finished 13500/15770 sessions …\n",
      "[Day 5] finished 13550/15770 sessions …\n",
      "[Day 5] finished 13600/15770 sessions …\n",
      "[Day 5] finished 13650/15770 sessions …\n",
      "[Day 5] finished 13700/15770 sessions …\n",
      "[Day 5] finished 13750/15770 sessions …\n",
      "[Day 5] finished 13800/15770 sessions …\n",
      "[Day 5] finished 13850/15770 sessions …\n",
      "[Day 5] finished 13900/15770 sessions …\n",
      "[Day 5] finished 13950/15770 sessions …\n",
      "[Day 5] finished 14000/15770 sessions …\n",
      "[Day 5] finished 14050/15770 sessions …\n",
      "[Day 5] finished 14100/15770 sessions …\n",
      "[Day 5] finished 14150/15770 sessions …\n",
      "[Day 5] finished 14200/15770 sessions …\n",
      "[Day 5] finished 14250/15770 sessions …\n",
      "[Day 5] finished 14300/15770 sessions …\n",
      "[Day 5] finished 14350/15770 sessions …\n",
      "[Day 5] finished 14400/15770 sessions …\n",
      "[Day 5] finished 14450/15770 sessions …\n",
      "[Day 5] finished 14500/15770 sessions …\n",
      "[Day 5] finished 14550/15770 sessions …\n",
      "[Day 5] finished 14600/15770 sessions …\n",
      "[Day 5] finished 14650/15770 sessions …\n",
      "[Day 5] finished 14700/15770 sessions …\n",
      "[Day 5] finished 14750/15770 sessions …\n",
      "[Day 5] finished 14800/15770 sessions …\n",
      "[Day 5] finished 14850/15770 sessions …\n",
      "[Day 5] finished 14900/15770 sessions …\n",
      "[Day 5] finished 14950/15770 sessions …\n",
      "[Day 5] finished 15000/15770 sessions …\n",
      "[Day 5] finished 15050/15770 sessions …\n",
      "[Day 5] finished 15100/15770 sessions …\n",
      "[Day 5] finished 15150/15770 sessions …\n",
      "[Day 5] finished 15200/15770 sessions …\n",
      "[Day 5] finished 15250/15770 sessions …\n",
      "[Day 5] finished 15300/15770 sessions …\n",
      "[Day 5] finished 15350/15770 sessions …\n",
      "[Day 5] finished 15400/15770 sessions …\n",
      "[Day 5] finished 15450/15770 sessions …\n",
      "[Day 5] finished 15500/15770 sessions …\n",
      "[Day 5] finished 15550/15770 sessions …\n",
      "[Day 5] finished 15600/15770 sessions …\n",
      "[Day 5] finished 15650/15770 sessions …\n",
      "[Day 5] finished 15700/15770 sessions …\n",
      "[Day 5] finished 15750/15770 sessions …\n",
      "  wrote 630,800 rows (cumulative 630,800).\n",
      "[simulate] Day index 5 (2020-07-09) …\n",
      "[day6] predictions not found on disk; computing …\n",
      "[Day 6] finished 50/16062 sessions …\n",
      "[Day 6] finished 100/16062 sessions …\n",
      "[Day 6] finished 150/16062 sessions …\n",
      "[Day 6] finished 200/16062 sessions …\n",
      "[Day 6] finished 250/16062 sessions …\n",
      "[Day 6] finished 300/16062 sessions …\n",
      "[Day 6] finished 350/16062 sessions …\n",
      "[Day 6] finished 400/16062 sessions …\n",
      "[Day 6] finished 450/16062 sessions …\n",
      "[Day 6] finished 500/16062 sessions …\n",
      "[Day 6] finished 550/16062 sessions …\n",
      "[Day 6] finished 600/16062 sessions …\n",
      "[Day 6] finished 650/16062 sessions …\n",
      "[Day 6] finished 700/16062 sessions …\n",
      "[Day 6] finished 750/16062 sessions …\n",
      "[Day 6] finished 800/16062 sessions …\n",
      "[Day 6] finished 850/16062 sessions …\n",
      "[Day 6] finished 900/16062 sessions …\n",
      "[Day 6] finished 950/16062 sessions …\n",
      "[Day 6] finished 1000/16062 sessions …\n",
      "[Day 6] finished 1050/16062 sessions …\n",
      "[Day 6] finished 1100/16062 sessions …\n",
      "[Day 6] finished 1150/16062 sessions …\n",
      "[Day 6] finished 1200/16062 sessions …\n",
      "[Day 6] finished 1250/16062 sessions …\n",
      "[Day 6] finished 1300/16062 sessions …\n",
      "[Day 6] finished 1350/16062 sessions …\n",
      "[Day 6] finished 1400/16062 sessions …\n",
      "[Day 6] finished 1450/16062 sessions …\n",
      "[Day 6] finished 1500/16062 sessions …\n",
      "[Day 6] finished 1550/16062 sessions …\n",
      "[Day 6] finished 1600/16062 sessions …\n",
      "[Day 6] finished 1650/16062 sessions …\n",
      "[Day 6] finished 1700/16062 sessions …\n",
      "[Day 6] finished 1750/16062 sessions …\n",
      "[Day 6] finished 1800/16062 sessions …\n",
      "[Day 6] finished 1850/16062 sessions …\n",
      "[Day 6] finished 1900/16062 sessions …\n",
      "[Day 6] finished 1950/16062 sessions …\n",
      "[Day 6] finished 2000/16062 sessions …\n",
      "[Day 6] finished 2050/16062 sessions …\n",
      "[Day 6] finished 2100/16062 sessions …\n",
      "[Day 6] finished 2150/16062 sessions …\n",
      "[Day 6] finished 2200/16062 sessions …\n",
      "[Day 6] finished 2250/16062 sessions …\n",
      "[Day 6] finished 2300/16062 sessions …\n",
      "[Day 6] finished 2350/16062 sessions …\n",
      "[Day 6] finished 2400/16062 sessions …\n",
      "[Day 6] finished 2450/16062 sessions …\n",
      "[Day 6] finished 2500/16062 sessions …\n",
      "[Day 6] finished 2550/16062 sessions …\n",
      "[Day 6] finished 2600/16062 sessions …\n",
      "[Day 6] finished 2650/16062 sessions …\n",
      "[Day 6] finished 2700/16062 sessions …\n",
      "[Day 6] finished 2750/16062 sessions …\n",
      "[Day 6] finished 2800/16062 sessions …\n",
      "[Day 6] finished 2850/16062 sessions …\n",
      "[Day 6] finished 2900/16062 sessions …\n",
      "[Day 6] finished 2950/16062 sessions …\n",
      "[Day 6] finished 3000/16062 sessions …\n",
      "[Day 6] finished 3050/16062 sessions …\n",
      "[Day 6] finished 3100/16062 sessions …\n",
      "[Day 6] finished 3150/16062 sessions …\n",
      "[Day 6] finished 3200/16062 sessions …\n",
      "[Day 6] finished 3250/16062 sessions …\n",
      "[Day 6] finished 3300/16062 sessions …\n",
      "[Day 6] finished 3350/16062 sessions …\n",
      "[Day 6] finished 3400/16062 sessions …\n",
      "[Day 6] finished 3450/16062 sessions …\n",
      "[Day 6] finished 3500/16062 sessions …\n",
      "[Day 6] finished 3550/16062 sessions …\n",
      "[Day 6] finished 3600/16062 sessions …\n",
      "[Day 6] finished 3650/16062 sessions …\n",
      "[Day 6] finished 3700/16062 sessions …\n",
      "[Day 6] finished 3750/16062 sessions …\n",
      "[Day 6] finished 3800/16062 sessions …\n",
      "[Day 6] finished 3850/16062 sessions …\n",
      "[Day 6] finished 3900/16062 sessions …\n",
      "[Day 6] finished 3950/16062 sessions …\n",
      "[Day 6] finished 4000/16062 sessions …\n",
      "[Day 6] finished 4050/16062 sessions …\n",
      "[Day 6] finished 4100/16062 sessions …\n",
      "[Day 6] finished 4150/16062 sessions …\n",
      "[Day 6] finished 4200/16062 sessions …\n",
      "[Day 6] finished 4250/16062 sessions …\n",
      "[Day 6] finished 4300/16062 sessions …\n",
      "[Day 6] finished 4350/16062 sessions …\n",
      "[Day 6] finished 4400/16062 sessions …\n",
      "[Day 6] finished 4450/16062 sessions …\n",
      "[Day 6] finished 4500/16062 sessions …\n",
      "[Day 6] finished 4550/16062 sessions …\n",
      "[Day 6] finished 4600/16062 sessions …\n",
      "[Day 6] finished 4650/16062 sessions …\n",
      "[Day 6] finished 4700/16062 sessions …\n",
      "[Day 6] finished 4750/16062 sessions …\n",
      "[Day 6] finished 4800/16062 sessions …\n",
      "[Day 6] finished 4850/16062 sessions …\n",
      "[Day 6] finished 4900/16062 sessions …\n",
      "[Day 6] finished 4950/16062 sessions …\n",
      "[Day 6] finished 5000/16062 sessions …\n",
      "[Day 6] finished 5050/16062 sessions …\n",
      "[Day 6] finished 5100/16062 sessions …\n",
      "[Day 6] finished 5150/16062 sessions …\n",
      "[Day 6] finished 5200/16062 sessions …\n",
      "[Day 6] finished 5250/16062 sessions …\n",
      "[Day 6] finished 5300/16062 sessions …\n",
      "[Day 6] finished 5350/16062 sessions …\n",
      "[Day 6] finished 5400/16062 sessions …\n",
      "[Day 6] finished 5450/16062 sessions …\n",
      "[Day 6] finished 5500/16062 sessions …\n",
      "[Day 6] finished 5550/16062 sessions …\n",
      "[Day 6] finished 5600/16062 sessions …\n",
      "[Day 6] finished 5650/16062 sessions …\n",
      "[Day 6] finished 5700/16062 sessions …\n",
      "[Day 6] finished 5750/16062 sessions …\n",
      "[Day 6] finished 5800/16062 sessions …\n",
      "[Day 6] finished 5850/16062 sessions …\n",
      "[Day 6] finished 5900/16062 sessions …\n",
      "[Day 6] finished 5950/16062 sessions …\n",
      "[Day 6] finished 6000/16062 sessions …\n",
      "[Day 6] finished 6050/16062 sessions …\n",
      "[Day 6] finished 6100/16062 sessions …\n",
      "[Day 6] finished 6150/16062 sessions …\n",
      "[Day 6] finished 6200/16062 sessions …\n",
      "[Day 6] finished 6250/16062 sessions …\n",
      "[Day 6] finished 6300/16062 sessions …\n",
      "[Day 6] finished 6350/16062 sessions …\n",
      "[Day 6] finished 6400/16062 sessions …\n",
      "[Day 6] finished 6450/16062 sessions …\n",
      "[Day 6] finished 6500/16062 sessions …\n",
      "[Day 6] finished 6550/16062 sessions …\n",
      "[Day 6] finished 6600/16062 sessions …\n",
      "[Day 6] finished 6650/16062 sessions …\n",
      "[Day 6] finished 6700/16062 sessions …\n",
      "[Day 6] finished 6750/16062 sessions …\n",
      "[Day 6] finished 6800/16062 sessions …\n",
      "[Day 6] finished 6850/16062 sessions …\n",
      "[Day 6] finished 6900/16062 sessions …\n",
      "[Day 6] finished 6950/16062 sessions …\n",
      "[Day 6] finished 7000/16062 sessions …\n",
      "[Day 6] finished 7050/16062 sessions …\n",
      "[Day 6] finished 7100/16062 sessions …\n",
      "[Day 6] finished 7150/16062 sessions …\n",
      "[Day 6] finished 7200/16062 sessions …\n",
      "[Day 6] finished 7250/16062 sessions …\n",
      "[Day 6] finished 7300/16062 sessions …\n",
      "[Day 6] finished 7350/16062 sessions …\n",
      "[Day 6] finished 7400/16062 sessions …\n",
      "[Day 6] finished 7450/16062 sessions …\n",
      "[Day 6] finished 7500/16062 sessions …\n",
      "[Day 6] finished 7550/16062 sessions …\n",
      "[Day 6] finished 7600/16062 sessions …\n",
      "[Day 6] finished 7650/16062 sessions …\n",
      "[Day 6] finished 7700/16062 sessions …\n",
      "[Day 6] finished 7750/16062 sessions …\n",
      "[Day 6] finished 7800/16062 sessions …\n",
      "[Day 6] finished 7850/16062 sessions …\n",
      "[Day 6] finished 7900/16062 sessions …\n",
      "[Day 6] finished 7950/16062 sessions …\n",
      "[Day 6] finished 8000/16062 sessions …\n",
      "[Day 6] finished 8050/16062 sessions …\n",
      "[Day 6] finished 8100/16062 sessions …\n",
      "[Day 6] finished 8150/16062 sessions …\n",
      "[Day 6] finished 8200/16062 sessions …\n",
      "[Day 6] finished 8250/16062 sessions …\n",
      "[Day 6] finished 8300/16062 sessions …\n",
      "[Day 6] finished 8350/16062 sessions …\n",
      "[Day 6] finished 8400/16062 sessions …\n",
      "[Day 6] finished 8450/16062 sessions …\n",
      "[Day 6] finished 8500/16062 sessions …\n",
      "[Day 6] finished 8550/16062 sessions …\n",
      "[Day 6] finished 8600/16062 sessions …\n",
      "[Day 6] finished 8650/16062 sessions …\n",
      "[Day 6] finished 8700/16062 sessions …\n",
      "[Day 6] finished 8750/16062 sessions …\n",
      "[Day 6] finished 8800/16062 sessions …\n",
      "[Day 6] finished 8850/16062 sessions …\n",
      "[Day 6] finished 8900/16062 sessions …\n",
      "[Day 6] finished 8950/16062 sessions …\n",
      "[Day 6] finished 9000/16062 sessions …\n",
      "[Day 6] finished 9050/16062 sessions …\n",
      "[Day 6] finished 9100/16062 sessions …\n",
      "[Day 6] finished 9150/16062 sessions …\n",
      "[Day 6] finished 9200/16062 sessions …\n",
      "[Day 6] finished 9250/16062 sessions …\n",
      "[Day 6] finished 9300/16062 sessions …\n",
      "[Day 6] finished 9350/16062 sessions …\n",
      "[Day 6] finished 9400/16062 sessions …\n",
      "[Day 6] finished 9450/16062 sessions …\n",
      "[Day 6] finished 9500/16062 sessions …\n",
      "[Day 6] finished 9550/16062 sessions …\n",
      "[Day 6] finished 9600/16062 sessions …\n",
      "[Day 6] finished 9650/16062 sessions …\n",
      "[Day 6] finished 9700/16062 sessions …\n",
      "[Day 6] finished 9750/16062 sessions …\n",
      "[Day 6] finished 9800/16062 sessions …\n",
      "[Day 6] finished 9850/16062 sessions …\n",
      "[Day 6] finished 9900/16062 sessions …\n",
      "[Day 6] finished 9950/16062 sessions …\n",
      "[Day 6] finished 10000/16062 sessions …\n",
      "[Day 6] finished 10050/16062 sessions …\n",
      "[Day 6] finished 10100/16062 sessions …\n",
      "[Day 6] finished 10150/16062 sessions …\n",
      "[Day 6] finished 10200/16062 sessions …\n",
      "[Day 6] finished 10250/16062 sessions …\n",
      "[Day 6] finished 10300/16062 sessions …\n",
      "[Day 6] finished 10350/16062 sessions …\n",
      "[Day 6] finished 10400/16062 sessions …\n",
      "[Day 6] finished 10450/16062 sessions …\n",
      "[Day 6] finished 10500/16062 sessions …\n",
      "[Day 6] finished 10550/16062 sessions …\n",
      "[Day 6] finished 10600/16062 sessions …\n",
      "[Day 6] finished 10650/16062 sessions …\n",
      "[Day 6] finished 10700/16062 sessions …\n",
      "[Day 6] finished 10750/16062 sessions …\n",
      "[Day 6] finished 10800/16062 sessions …\n",
      "[Day 6] finished 10850/16062 sessions …\n",
      "[Day 6] finished 10900/16062 sessions …\n",
      "[Day 6] finished 10950/16062 sessions …\n",
      "[Day 6] finished 11000/16062 sessions …\n",
      "[Day 6] finished 11050/16062 sessions …\n",
      "[Day 6] finished 11100/16062 sessions …\n",
      "[Day 6] finished 11150/16062 sessions …\n",
      "[Day 6] finished 11200/16062 sessions …\n",
      "[Day 6] finished 11250/16062 sessions …\n",
      "[Day 6] finished 11300/16062 sessions …\n",
      "[Day 6] finished 11350/16062 sessions …\n",
      "[Day 6] finished 11400/16062 sessions …\n",
      "[Day 6] finished 11450/16062 sessions …\n",
      "[Day 6] finished 11500/16062 sessions …\n",
      "[Day 6] finished 11550/16062 sessions …\n",
      "[Day 6] finished 11600/16062 sessions …\n",
      "[Day 6] finished 11650/16062 sessions …\n",
      "[Day 6] finished 11700/16062 sessions …\n",
      "[Day 6] finished 11750/16062 sessions …\n",
      "[Day 6] finished 11800/16062 sessions …\n",
      "[Day 6] finished 11850/16062 sessions …\n",
      "[Day 6] finished 11900/16062 sessions …\n",
      "[Day 6] finished 11950/16062 sessions …\n",
      "[Day 6] finished 12000/16062 sessions …\n",
      "[Day 6] finished 12050/16062 sessions …\n",
      "[Day 6] finished 12100/16062 sessions …\n",
      "[Day 6] finished 12150/16062 sessions …\n",
      "[Day 6] finished 12200/16062 sessions …\n",
      "[Day 6] finished 12250/16062 sessions …\n",
      "[Day 6] finished 12300/16062 sessions …\n",
      "[Day 6] finished 12350/16062 sessions …\n",
      "[Day 6] finished 12400/16062 sessions …\n",
      "[Day 6] finished 12450/16062 sessions …\n",
      "[Day 6] finished 12500/16062 sessions …\n",
      "[Day 6] finished 12550/16062 sessions …\n",
      "[Day 6] finished 12600/16062 sessions …\n",
      "[Day 6] finished 12650/16062 sessions …\n",
      "[Day 6] finished 12700/16062 sessions …\n",
      "[Day 6] finished 12750/16062 sessions …\n",
      "[Day 6] finished 12800/16062 sessions …\n",
      "[Day 6] finished 12850/16062 sessions …\n",
      "[Day 6] finished 12900/16062 sessions …\n",
      "[Day 6] finished 12950/16062 sessions …\n",
      "[Day 6] finished 13000/16062 sessions …\n",
      "[Day 6] finished 13050/16062 sessions …\n",
      "[Day 6] finished 13100/16062 sessions …\n",
      "[Day 6] finished 13150/16062 sessions …\n",
      "[Day 6] finished 13200/16062 sessions …\n",
      "[Day 6] finished 13250/16062 sessions …\n",
      "[Day 6] finished 13300/16062 sessions …\n",
      "[Day 6] finished 13350/16062 sessions …\n",
      "[Day 6] finished 13400/16062 sessions …\n",
      "[Day 6] finished 13450/16062 sessions …\n",
      "[Day 6] finished 13500/16062 sessions …\n",
      "[Day 6] finished 13550/16062 sessions …\n",
      "[Day 6] finished 13600/16062 sessions …\n",
      "[Day 6] finished 13650/16062 sessions …\n",
      "[Day 6] finished 13700/16062 sessions …\n",
      "[Day 6] finished 13750/16062 sessions …\n",
      "[Day 6] finished 13800/16062 sessions …\n",
      "[Day 6] finished 13850/16062 sessions …\n",
      "[Day 6] finished 13900/16062 sessions …\n",
      "[Day 6] finished 13950/16062 sessions …\n",
      "[Day 6] finished 14000/16062 sessions …\n",
      "[Day 6] finished 14050/16062 sessions …\n",
      "[Day 6] finished 14100/16062 sessions …\n",
      "[Day 6] finished 14150/16062 sessions …\n",
      "[Day 6] finished 14200/16062 sessions …\n",
      "[Day 6] finished 14250/16062 sessions …\n",
      "[Day 6] finished 14300/16062 sessions …\n",
      "[Day 6] finished 14350/16062 sessions …\n",
      "[Day 6] finished 14400/16062 sessions …\n",
      "[Day 6] finished 14450/16062 sessions …\n",
      "[Day 6] finished 14500/16062 sessions …\n",
      "[Day 6] finished 14550/16062 sessions …\n",
      "[Day 6] finished 14600/16062 sessions …\n",
      "[Day 6] finished 14650/16062 sessions …\n",
      "[Day 6] finished 14700/16062 sessions …\n",
      "[Day 6] finished 14750/16062 sessions …\n",
      "[Day 6] finished 14800/16062 sessions …\n",
      "[Day 6] finished 14850/16062 sessions …\n",
      "[Day 6] finished 14900/16062 sessions …\n",
      "[Day 6] finished 14950/16062 sessions …\n",
      "[Day 6] finished 15000/16062 sessions …\n",
      "[Day 6] finished 15050/16062 sessions …\n",
      "[Day 6] finished 15100/16062 sessions …\n",
      "[Day 6] finished 15150/16062 sessions …\n",
      "[Day 6] finished 15200/16062 sessions …\n",
      "[Day 6] finished 15250/16062 sessions …\n",
      "[Day 6] finished 15300/16062 sessions …\n",
      "[Day 6] finished 15350/16062 sessions …\n",
      "[Day 6] finished 15400/16062 sessions …\n",
      "[Day 6] finished 15450/16062 sessions …\n",
      "[Day 6] finished 15500/16062 sessions …\n",
      "[Day 6] finished 15550/16062 sessions …\n",
      "[Day 6] finished 15600/16062 sessions …\n",
      "[Day 6] finished 15650/16062 sessions …\n",
      "[Day 6] finished 15700/16062 sessions …\n",
      "[Day 6] finished 15750/16062 sessions …\n",
      "[Day 6] finished 15800/16062 sessions …\n",
      "[Day 6] finished 15850/16062 sessions …\n",
      "[Day 6] finished 15900/16062 sessions …\n",
      "[Day 6] finished 15950/16062 sessions …\n",
      "[Day 6] finished 16000/16062 sessions …\n",
      "[Day 6] finished 16050/16062 sessions …\n",
      "  wrote 642,480 rows (cumulative 1,273,280).\n",
      "[simulate] Day index 6 (2020-07-10) …\n",
      "[day7] predictions not found on disk; computing …\n",
      "[Day 7] finished 50/15731 sessions …\n",
      "[Day 7] finished 100/15731 sessions …\n",
      "[Day 7] finished 150/15731 sessions …\n",
      "[Day 7] finished 200/15731 sessions …\n",
      "[Day 7] finished 250/15731 sessions …\n",
      "[Day 7] finished 300/15731 sessions …\n",
      "[Day 7] finished 350/15731 sessions …\n",
      "[Day 7] finished 400/15731 sessions …\n",
      "[Day 7] finished 450/15731 sessions …\n",
      "[Day 7] finished 500/15731 sessions …\n",
      "[Day 7] finished 550/15731 sessions …\n",
      "[Day 7] finished 600/15731 sessions …\n",
      "[Day 7] finished 650/15731 sessions …\n",
      "[Day 7] finished 700/15731 sessions …\n",
      "[Day 7] finished 750/15731 sessions …\n",
      "[Day 7] finished 800/15731 sessions …\n",
      "[Day 7] finished 850/15731 sessions …\n",
      "[Day 7] finished 900/15731 sessions …\n",
      "[Day 7] finished 950/15731 sessions …\n",
      "[Day 7] finished 1000/15731 sessions …\n",
      "[Day 7] finished 1050/15731 sessions …\n",
      "[Day 7] finished 1100/15731 sessions …\n",
      "[Day 7] finished 1150/15731 sessions …\n",
      "[Day 7] finished 1200/15731 sessions …\n",
      "[Day 7] finished 1250/15731 sessions …\n",
      "[Day 7] finished 1300/15731 sessions …\n",
      "[Day 7] finished 1350/15731 sessions …\n",
      "[Day 7] finished 1400/15731 sessions …\n",
      "[Day 7] finished 1450/15731 sessions …\n",
      "[Day 7] finished 1500/15731 sessions …\n",
      "[Day 7] finished 1550/15731 sessions …\n",
      "[Day 7] finished 1600/15731 sessions …\n",
      "[Day 7] finished 1650/15731 sessions …\n",
      "[Day 7] finished 1700/15731 sessions …\n",
      "[Day 7] finished 1750/15731 sessions …\n",
      "[Day 7] finished 1800/15731 sessions …\n",
      "[Day 7] finished 1850/15731 sessions …\n",
      "[Day 7] finished 1900/15731 sessions …\n",
      "[Day 7] finished 1950/15731 sessions …\n",
      "[Day 7] finished 2000/15731 sessions …\n",
      "[Day 7] finished 2050/15731 sessions …\n",
      "[Day 7] finished 2100/15731 sessions …\n",
      "[Day 7] finished 2150/15731 sessions …\n",
      "[Day 7] finished 2200/15731 sessions …\n",
      "[Day 7] finished 2250/15731 sessions …\n",
      "[Day 7] finished 2300/15731 sessions …\n",
      "[Day 7] finished 2350/15731 sessions …\n",
      "[Day 7] finished 2400/15731 sessions …\n",
      "[Day 7] finished 2450/15731 sessions …\n",
      "[Day 7] finished 2500/15731 sessions …\n",
      "[Day 7] finished 2550/15731 sessions …\n",
      "[Day 7] finished 2600/15731 sessions …\n",
      "[Day 7] finished 2650/15731 sessions …\n",
      "[Day 7] finished 2700/15731 sessions …\n",
      "[Day 7] finished 2750/15731 sessions …\n",
      "[Day 7] finished 2800/15731 sessions …\n",
      "[Day 7] finished 2850/15731 sessions …\n",
      "[Day 7] finished 2900/15731 sessions …\n",
      "[Day 7] finished 2950/15731 sessions …\n",
      "[Day 7] finished 3000/15731 sessions …\n",
      "[Day 7] finished 3050/15731 sessions …\n",
      "[Day 7] finished 3100/15731 sessions …\n",
      "[Day 7] finished 3150/15731 sessions …\n",
      "[Day 7] finished 3200/15731 sessions …\n",
      "[Day 7] finished 3250/15731 sessions …\n",
      "[Day 7] finished 3300/15731 sessions …\n",
      "[Day 7] finished 3350/15731 sessions …\n",
      "[Day 7] finished 3400/15731 sessions …\n",
      "[Day 7] finished 3450/15731 sessions …\n",
      "[Day 7] finished 3500/15731 sessions …\n",
      "[Day 7] finished 3550/15731 sessions …\n",
      "[Day 7] finished 3600/15731 sessions …\n",
      "[Day 7] finished 3650/15731 sessions …\n",
      "[Day 7] finished 3700/15731 sessions …\n",
      "[Day 7] finished 3750/15731 sessions …\n",
      "[Day 7] finished 3800/15731 sessions …\n",
      "[Day 7] finished 3850/15731 sessions …\n",
      "[Day 7] finished 3900/15731 sessions …\n",
      "[Day 7] finished 3950/15731 sessions …\n",
      "[Day 7] finished 4000/15731 sessions …\n",
      "[Day 7] finished 4050/15731 sessions …\n",
      "[Day 7] finished 4100/15731 sessions …\n",
      "[Day 7] finished 4150/15731 sessions …\n",
      "[Day 7] finished 4200/15731 sessions …\n",
      "[Day 7] finished 4250/15731 sessions …\n",
      "[Day 7] finished 4300/15731 sessions …\n",
      "[Day 7] finished 4350/15731 sessions …\n",
      "[Day 7] finished 4400/15731 sessions …\n",
      "[Day 7] finished 4450/15731 sessions …\n",
      "[Day 7] finished 4500/15731 sessions …\n",
      "[Day 7] finished 4550/15731 sessions …\n",
      "[Day 7] finished 4600/15731 sessions …\n",
      "[Day 7] finished 4650/15731 sessions …\n",
      "[Day 7] finished 4700/15731 sessions …\n",
      "[Day 7] finished 4750/15731 sessions …\n",
      "[Day 7] finished 4800/15731 sessions …\n",
      "[Day 7] finished 4850/15731 sessions …\n",
      "[Day 7] finished 4900/15731 sessions …\n",
      "[Day 7] finished 4950/15731 sessions …\n",
      "[Day 7] finished 5000/15731 sessions …\n",
      "[Day 7] finished 5050/15731 sessions …\n",
      "[Day 7] finished 5100/15731 sessions …\n",
      "[Day 7] finished 5150/15731 sessions …\n",
      "[Day 7] finished 5200/15731 sessions …\n",
      "[Day 7] finished 5250/15731 sessions …\n",
      "[Day 7] finished 5300/15731 sessions …\n",
      "[Day 7] finished 5350/15731 sessions …\n",
      "[Day 7] finished 5400/15731 sessions …\n",
      "[Day 7] finished 5450/15731 sessions …\n",
      "[Day 7] finished 5500/15731 sessions …\n",
      "[Day 7] finished 5550/15731 sessions …\n",
      "[Day 7] finished 5600/15731 sessions …\n",
      "[Day 7] finished 5650/15731 sessions …\n",
      "[Day 7] finished 5700/15731 sessions …\n",
      "[Day 7] finished 5750/15731 sessions …\n",
      "[Day 7] finished 5800/15731 sessions …\n",
      "[Day 7] finished 5850/15731 sessions …\n",
      "[Day 7] finished 5900/15731 sessions …\n",
      "[Day 7] finished 5950/15731 sessions …\n",
      "[Day 7] finished 6000/15731 sessions …\n",
      "[Day 7] finished 6050/15731 sessions …\n",
      "[Day 7] finished 6100/15731 sessions …\n",
      "[Day 7] finished 6150/15731 sessions …\n",
      "[Day 7] finished 6200/15731 sessions …\n",
      "[Day 7] finished 6250/15731 sessions …\n",
      "[Day 7] finished 6300/15731 sessions …\n",
      "[Day 7] finished 6350/15731 sessions …\n",
      "[Day 7] finished 6400/15731 sessions …\n",
      "[Day 7] finished 6450/15731 sessions …\n",
      "[Day 7] finished 6500/15731 sessions …\n",
      "[Day 7] finished 6550/15731 sessions …\n",
      "[Day 7] finished 6600/15731 sessions …\n",
      "[Day 7] finished 6650/15731 sessions …\n",
      "[Day 7] finished 6700/15731 sessions …\n",
      "[Day 7] finished 6750/15731 sessions …\n",
      "[Day 7] finished 6800/15731 sessions …\n",
      "[Day 7] finished 6850/15731 sessions …\n",
      "[Day 7] finished 6900/15731 sessions …\n",
      "[Day 7] finished 6950/15731 sessions …\n",
      "[Day 7] finished 7000/15731 sessions …\n",
      "[Day 7] finished 7050/15731 sessions …\n",
      "[Day 7] finished 7100/15731 sessions …\n",
      "[Day 7] finished 7150/15731 sessions …\n",
      "[Day 7] finished 7200/15731 sessions …\n",
      "[Day 7] finished 7250/15731 sessions …\n",
      "[Day 7] finished 7300/15731 sessions …\n",
      "[Day 7] finished 7350/15731 sessions …\n",
      "[Day 7] finished 7400/15731 sessions …\n",
      "[Day 7] finished 7450/15731 sessions …\n",
      "[Day 7] finished 7500/15731 sessions …\n",
      "[Day 7] finished 7550/15731 sessions …\n",
      "[Day 7] finished 7600/15731 sessions …\n",
      "[Day 7] finished 7650/15731 sessions …\n",
      "[Day 7] finished 7700/15731 sessions …\n",
      "[Day 7] finished 7750/15731 sessions …\n",
      "[Day 7] finished 7800/15731 sessions …\n",
      "[Day 7] finished 7850/15731 sessions …\n",
      "[Day 7] finished 7900/15731 sessions …\n",
      "[Day 7] finished 7950/15731 sessions …\n",
      "[Day 7] finished 8000/15731 sessions …\n",
      "[Day 7] finished 8050/15731 sessions …\n",
      "[Day 7] finished 8100/15731 sessions …\n",
      "[Day 7] finished 8150/15731 sessions …\n",
      "[Day 7] finished 8200/15731 sessions …\n",
      "[Day 7] finished 8250/15731 sessions …\n",
      "[Day 7] finished 8300/15731 sessions …\n",
      "[Day 7] finished 8350/15731 sessions …\n",
      "[Day 7] finished 8400/15731 sessions …\n",
      "[Day 7] finished 8450/15731 sessions …\n",
      "[Day 7] finished 8500/15731 sessions …\n",
      "[Day 7] finished 8550/15731 sessions …\n",
      "[Day 7] finished 8600/15731 sessions …\n",
      "[Day 7] finished 8650/15731 sessions …\n",
      "[Day 7] finished 8700/15731 sessions …\n",
      "[Day 7] finished 8750/15731 sessions …\n",
      "[Day 7] finished 8800/15731 sessions …\n",
      "[Day 7] finished 8850/15731 sessions …\n",
      "[Day 7] finished 8900/15731 sessions …\n",
      "[Day 7] finished 8950/15731 sessions …\n",
      "[Day 7] finished 9000/15731 sessions …\n",
      "[Day 7] finished 9050/15731 sessions …\n",
      "[Day 7] finished 9100/15731 sessions …\n",
      "[Day 7] finished 9150/15731 sessions …\n",
      "[Day 7] finished 9200/15731 sessions …\n",
      "[Day 7] finished 9250/15731 sessions …\n",
      "[Day 7] finished 9300/15731 sessions …\n",
      "[Day 7] finished 9350/15731 sessions …\n",
      "[Day 7] finished 9400/15731 sessions …\n",
      "[Day 7] finished 9450/15731 sessions …\n",
      "[Day 7] finished 9500/15731 sessions …\n",
      "[Day 7] finished 9550/15731 sessions …\n",
      "[Day 7] finished 9600/15731 sessions …\n",
      "[Day 7] finished 9650/15731 sessions …\n",
      "[Day 7] finished 9700/15731 sessions …\n",
      "[Day 7] finished 9750/15731 sessions …\n",
      "[Day 7] finished 9800/15731 sessions …\n",
      "[Day 7] finished 9850/15731 sessions …\n",
      "[Day 7] finished 9900/15731 sessions …\n",
      "[Day 7] finished 9950/15731 sessions …\n",
      "[Day 7] finished 10000/15731 sessions …\n",
      "[Day 7] finished 10050/15731 sessions …\n",
      "[Day 7] finished 10100/15731 sessions …\n",
      "[Day 7] finished 10150/15731 sessions …\n",
      "[Day 7] finished 10200/15731 sessions …\n",
      "[Day 7] finished 10250/15731 sessions …\n",
      "[Day 7] finished 10300/15731 sessions …\n",
      "[Day 7] finished 10350/15731 sessions …\n",
      "[Day 7] finished 10400/15731 sessions …\n",
      "[Day 7] finished 10450/15731 sessions …\n",
      "[Day 7] finished 10500/15731 sessions …\n",
      "[Day 7] finished 10550/15731 sessions …\n",
      "[Day 7] finished 10600/15731 sessions …\n",
      "[Day 7] finished 10650/15731 sessions …\n",
      "[Day 7] finished 10700/15731 sessions …\n",
      "[Day 7] finished 10750/15731 sessions …\n",
      "[Day 7] finished 10800/15731 sessions …\n",
      "[Day 7] finished 10850/15731 sessions …\n",
      "[Day 7] finished 10900/15731 sessions …\n",
      "[Day 7] finished 10950/15731 sessions …\n",
      "[Day 7] finished 11000/15731 sessions …\n",
      "[Day 7] finished 11050/15731 sessions …\n",
      "[Day 7] finished 11100/15731 sessions …\n",
      "[Day 7] finished 11150/15731 sessions …\n",
      "[Day 7] finished 11200/15731 sessions …\n",
      "[Day 7] finished 11250/15731 sessions …\n",
      "[Day 7] finished 11300/15731 sessions …\n",
      "[Day 7] finished 11350/15731 sessions …\n",
      "[Day 7] finished 11400/15731 sessions …\n",
      "[Day 7] finished 11450/15731 sessions …\n",
      "[Day 7] finished 11500/15731 sessions …\n",
      "[Day 7] finished 11550/15731 sessions …\n",
      "[Day 7] finished 11600/15731 sessions …\n",
      "[Day 7] finished 11650/15731 sessions …\n",
      "[Day 7] finished 11700/15731 sessions …\n",
      "[Day 7] finished 11750/15731 sessions …\n",
      "[Day 7] finished 11800/15731 sessions …\n",
      "[Day 7] finished 11850/15731 sessions …\n",
      "[Day 7] finished 11900/15731 sessions …\n",
      "[Day 7] finished 11950/15731 sessions …\n",
      "[Day 7] finished 12000/15731 sessions …\n",
      "[Day 7] finished 12050/15731 sessions …\n",
      "[Day 7] finished 12100/15731 sessions …\n",
      "[Day 7] finished 12150/15731 sessions …\n",
      "[Day 7] finished 12200/15731 sessions …\n",
      "[Day 7] finished 12250/15731 sessions …\n",
      "[Day 7] finished 12300/15731 sessions …\n",
      "[Day 7] finished 12350/15731 sessions …\n",
      "[Day 7] finished 12400/15731 sessions …\n",
      "[Day 7] finished 12450/15731 sessions …\n",
      "[Day 7] finished 12500/15731 sessions …\n",
      "[Day 7] finished 12550/15731 sessions …\n",
      "[Day 7] finished 12600/15731 sessions …\n",
      "[Day 7] finished 12650/15731 sessions …\n",
      "[Day 7] finished 12700/15731 sessions …\n",
      "[Day 7] finished 12750/15731 sessions …\n",
      "[Day 7] finished 12800/15731 sessions …\n",
      "[Day 7] finished 12850/15731 sessions …\n",
      "[Day 7] finished 12900/15731 sessions …\n",
      "[Day 7] finished 12950/15731 sessions …\n",
      "[Day 7] finished 13000/15731 sessions …\n",
      "[Day 7] finished 13050/15731 sessions …\n",
      "[Day 7] finished 13100/15731 sessions …\n",
      "[Day 7] finished 13150/15731 sessions …\n",
      "[Day 7] finished 13200/15731 sessions …\n",
      "[Day 7] finished 13250/15731 sessions …\n",
      "[Day 7] finished 13300/15731 sessions …\n",
      "[Day 7] finished 13350/15731 sessions …\n",
      "[Day 7] finished 13400/15731 sessions …\n",
      "[Day 7] finished 13450/15731 sessions …\n",
      "[Day 7] finished 13500/15731 sessions …\n",
      "[Day 7] finished 13550/15731 sessions …\n",
      "[Day 7] finished 13600/15731 sessions …\n",
      "[Day 7] finished 13650/15731 sessions …\n",
      "[Day 7] finished 13700/15731 sessions …\n",
      "[Day 7] finished 13750/15731 sessions …\n",
      "[Day 7] finished 13800/15731 sessions …\n",
      "[Day 7] finished 13850/15731 sessions …\n",
      "[Day 7] finished 13900/15731 sessions …\n",
      "[Day 7] finished 13950/15731 sessions …\n",
      "[Day 7] finished 14000/15731 sessions …\n",
      "[Day 7] finished 14050/15731 sessions …\n",
      "[Day 7] finished 14100/15731 sessions …\n",
      "[Day 7] finished 14150/15731 sessions …\n",
      "[Day 7] finished 14200/15731 sessions …\n",
      "[Day 7] finished 14250/15731 sessions …\n",
      "[Day 7] finished 14300/15731 sessions …\n",
      "[Day 7] finished 14350/15731 sessions …\n",
      "[Day 7] finished 14400/15731 sessions …\n",
      "[Day 7] finished 14450/15731 sessions …\n",
      "[Day 7] finished 14500/15731 sessions …\n",
      "[Day 7] finished 14550/15731 sessions …\n",
      "[Day 7] finished 14600/15731 sessions …\n",
      "[Day 7] finished 14650/15731 sessions …\n",
      "[Day 7] finished 14700/15731 sessions …\n",
      "[Day 7] finished 14750/15731 sessions …\n",
      "[Day 7] finished 14800/15731 sessions …\n",
      "[Day 7] finished 14850/15731 sessions …\n",
      "[Day 7] finished 14900/15731 sessions …\n",
      "[Day 7] finished 14950/15731 sessions …\n",
      "[Day 7] finished 15000/15731 sessions …\n",
      "[Day 7] finished 15050/15731 sessions …\n",
      "[Day 7] finished 15100/15731 sessions …\n",
      "[Day 7] finished 15150/15731 sessions …\n",
      "[Day 7] finished 15200/15731 sessions …\n",
      "[Day 7] finished 15250/15731 sessions …\n",
      "[Day 7] finished 15300/15731 sessions …\n",
      "[Day 7] finished 15350/15731 sessions …\n",
      "[Day 7] finished 15400/15731 sessions …\n",
      "[Day 7] finished 15450/15731 sessions …\n",
      "[Day 7] finished 15500/15731 sessions …\n",
      "[Day 7] finished 15550/15731 sessions …\n",
      "[Day 7] finished 15600/15731 sessions …\n",
      "[Day 7] finished 15650/15731 sessions …\n",
      "[Day 7] finished 15700/15731 sessions …\n",
      "  wrote 629,240 rows (cumulative 1,902,520).\n",
      "[simulate] Day index 7 (2020-07-11) …\n",
      "[day8] predictions not found on disk; computing …\n",
      "[Day 8] finished 50/15672 sessions …\n",
      "[Day 8] finished 100/15672 sessions …\n",
      "[Day 8] finished 150/15672 sessions …\n",
      "[Day 8] finished 200/15672 sessions …\n",
      "[Day 8] finished 250/15672 sessions …\n",
      "[Day 8] finished 300/15672 sessions …\n",
      "[Day 8] finished 350/15672 sessions …\n",
      "[Day 8] finished 400/15672 sessions …\n",
      "[Day 8] finished 450/15672 sessions …\n",
      "[Day 8] finished 500/15672 sessions …\n",
      "[Day 8] finished 550/15672 sessions …\n",
      "[Day 8] finished 600/15672 sessions …\n",
      "[Day 8] finished 650/15672 sessions …\n",
      "[Day 8] finished 700/15672 sessions …\n",
      "[Day 8] finished 750/15672 sessions …\n",
      "[Day 8] finished 800/15672 sessions …\n",
      "[Day 8] finished 850/15672 sessions …\n",
      "[Day 8] finished 900/15672 sessions …\n",
      "[Day 8] finished 950/15672 sessions …\n",
      "[Day 8] finished 1000/15672 sessions …\n",
      "[Day 8] finished 1050/15672 sessions …\n",
      "[Day 8] finished 1100/15672 sessions …\n",
      "[Day 8] finished 1150/15672 sessions …\n",
      "[Day 8] finished 1200/15672 sessions …\n",
      "[Day 8] finished 1250/15672 sessions …\n",
      "[Day 8] finished 1300/15672 sessions …\n",
      "[Day 8] finished 1350/15672 sessions …\n",
      "[Day 8] finished 1400/15672 sessions …\n",
      "[Day 8] finished 1450/15672 sessions …\n",
      "[Day 8] finished 1500/15672 sessions …\n",
      "[Day 8] finished 1550/15672 sessions …\n",
      "[Day 8] finished 1600/15672 sessions …\n",
      "[Day 8] finished 1650/15672 sessions …\n",
      "[Day 8] finished 1700/15672 sessions …\n",
      "[Day 8] finished 1750/15672 sessions …\n",
      "[Day 8] finished 1800/15672 sessions …\n",
      "[Day 8] finished 1850/15672 sessions …\n",
      "[Day 8] finished 1900/15672 sessions …\n",
      "[Day 8] finished 1950/15672 sessions …\n",
      "[Day 8] finished 2000/15672 sessions …\n",
      "[Day 8] finished 2050/15672 sessions …\n",
      "[Day 8] finished 2100/15672 sessions …\n",
      "[Day 8] finished 2150/15672 sessions …\n",
      "[Day 8] finished 2200/15672 sessions …\n",
      "[Day 8] finished 2250/15672 sessions …\n",
      "[Day 8] finished 2300/15672 sessions …\n",
      "[Day 8] finished 2350/15672 sessions …\n",
      "[Day 8] finished 2400/15672 sessions …\n",
      "[Day 8] finished 2450/15672 sessions …\n",
      "[Day 8] finished 2500/15672 sessions …\n",
      "[Day 8] finished 2550/15672 sessions …\n",
      "[Day 8] finished 2600/15672 sessions …\n",
      "[Day 8] finished 2650/15672 sessions …\n",
      "[Day 8] finished 2700/15672 sessions …\n",
      "[Day 8] finished 2750/15672 sessions …\n",
      "[Day 8] finished 2800/15672 sessions …\n",
      "[Day 8] finished 2850/15672 sessions …\n",
      "[Day 8] finished 2900/15672 sessions …\n",
      "[Day 8] finished 2950/15672 sessions …\n",
      "[Day 8] finished 3000/15672 sessions …\n",
      "[Day 8] finished 3050/15672 sessions …\n",
      "[Day 8] finished 3100/15672 sessions …\n",
      "[Day 8] finished 3150/15672 sessions …\n",
      "[Day 8] finished 3200/15672 sessions …\n",
      "[Day 8] finished 3250/15672 sessions …\n",
      "[Day 8] finished 3300/15672 sessions …\n",
      "[Day 8] finished 3350/15672 sessions …\n",
      "[Day 8] finished 3400/15672 sessions …\n",
      "[Day 8] finished 3450/15672 sessions …\n",
      "[Day 8] finished 3500/15672 sessions …\n",
      "[Day 8] finished 3550/15672 sessions …\n",
      "[Day 8] finished 3600/15672 sessions …\n",
      "[Day 8] finished 3650/15672 sessions …\n",
      "[Day 8] finished 3700/15672 sessions …\n",
      "[Day 8] finished 3750/15672 sessions …\n",
      "[Day 8] finished 3800/15672 sessions …\n",
      "[Day 8] finished 3850/15672 sessions …\n",
      "[Day 8] finished 3900/15672 sessions …\n",
      "[Day 8] finished 3950/15672 sessions …\n",
      "[Day 8] finished 4000/15672 sessions …\n",
      "[Day 8] finished 4050/15672 sessions …\n",
      "[Day 8] finished 4100/15672 sessions …\n",
      "[Day 8] finished 4150/15672 sessions …\n",
      "[Day 8] finished 4200/15672 sessions …\n",
      "[Day 8] finished 4250/15672 sessions …\n",
      "[Day 8] finished 4300/15672 sessions …\n",
      "[Day 8] finished 4350/15672 sessions …\n",
      "[Day 8] finished 4400/15672 sessions …\n",
      "[Day 8] finished 4450/15672 sessions …\n",
      "[Day 8] finished 4500/15672 sessions …\n",
      "[Day 8] finished 4550/15672 sessions …\n",
      "[Day 8] finished 4600/15672 sessions …\n",
      "[Day 8] finished 4650/15672 sessions …\n",
      "[Day 8] finished 4700/15672 sessions …\n",
      "[Day 8] finished 4750/15672 sessions …\n",
      "[Day 8] finished 4800/15672 sessions …\n",
      "[Day 8] finished 4850/15672 sessions …\n",
      "[Day 8] finished 4900/15672 sessions …\n",
      "[Day 8] finished 4950/15672 sessions …\n",
      "[Day 8] finished 5000/15672 sessions …\n",
      "[Day 8] finished 5050/15672 sessions …\n",
      "[Day 8] finished 5100/15672 sessions …\n",
      "[Day 8] finished 5150/15672 sessions …\n",
      "[Day 8] finished 5200/15672 sessions …\n",
      "[Day 8] finished 5250/15672 sessions …\n",
      "[Day 8] finished 5300/15672 sessions …\n",
      "[Day 8] finished 5350/15672 sessions …\n",
      "[Day 8] finished 5400/15672 sessions …\n",
      "[Day 8] finished 5450/15672 sessions …\n",
      "[Day 8] finished 5500/15672 sessions …\n",
      "[Day 8] finished 5550/15672 sessions …\n",
      "[Day 8] finished 5600/15672 sessions …\n",
      "[Day 8] finished 5650/15672 sessions …\n",
      "[Day 8] finished 5700/15672 sessions …\n",
      "[Day 8] finished 5750/15672 sessions …\n",
      "[Day 8] finished 5800/15672 sessions …\n",
      "[Day 8] finished 5850/15672 sessions …\n",
      "[Day 8] finished 5900/15672 sessions …\n",
      "[Day 8] finished 5950/15672 sessions …\n",
      "[Day 8] finished 6000/15672 sessions …\n",
      "[Day 8] finished 6050/15672 sessions …\n",
      "[Day 8] finished 6100/15672 sessions …\n",
      "[Day 8] finished 6150/15672 sessions …\n",
      "[Day 8] finished 6200/15672 sessions …\n",
      "[Day 8] finished 6250/15672 sessions …\n",
      "[Day 8] finished 6300/15672 sessions …\n",
      "[Day 8] finished 6350/15672 sessions …\n",
      "[Day 8] finished 6400/15672 sessions …\n",
      "[Day 8] finished 6450/15672 sessions …\n",
      "[Day 8] finished 6500/15672 sessions …\n",
      "[Day 8] finished 6550/15672 sessions …\n",
      "[Day 8] finished 6600/15672 sessions …\n",
      "[Day 8] finished 6650/15672 sessions …\n",
      "[Day 8] finished 6700/15672 sessions …\n",
      "[Day 8] finished 6750/15672 sessions …\n",
      "[Day 8] finished 6800/15672 sessions …\n",
      "[Day 8] finished 6850/15672 sessions …\n",
      "[Day 8] finished 6900/15672 sessions …\n",
      "[Day 8] finished 6950/15672 sessions …\n",
      "[Day 8] finished 7000/15672 sessions …\n",
      "[Day 8] finished 7050/15672 sessions …\n",
      "[Day 8] finished 7100/15672 sessions …\n",
      "[Day 8] finished 7150/15672 sessions …\n",
      "[Day 8] finished 7200/15672 sessions …\n",
      "[Day 8] finished 7250/15672 sessions …\n",
      "[Day 8] finished 7300/15672 sessions …\n",
      "[Day 8] finished 7350/15672 sessions …\n",
      "[Day 8] finished 7400/15672 sessions …\n",
      "[Day 8] finished 7450/15672 sessions …\n",
      "[Day 8] finished 7500/15672 sessions …\n",
      "[Day 8] finished 7550/15672 sessions …\n",
      "[Day 8] finished 7600/15672 sessions …\n",
      "[Day 8] finished 7650/15672 sessions …\n",
      "[Day 8] finished 7700/15672 sessions …\n",
      "[Day 8] finished 7750/15672 sessions …\n",
      "[Day 8] finished 7800/15672 sessions …\n",
      "[Day 8] finished 7850/15672 sessions …\n",
      "[Day 8] finished 7900/15672 sessions …\n",
      "[Day 8] finished 7950/15672 sessions …\n",
      "[Day 8] finished 8000/15672 sessions …\n",
      "[Day 8] finished 8050/15672 sessions …\n",
      "[Day 8] finished 8100/15672 sessions …\n",
      "[Day 8] finished 8150/15672 sessions …\n",
      "[Day 8] finished 8200/15672 sessions …\n",
      "[Day 8] finished 8250/15672 sessions …\n",
      "[Day 8] finished 8300/15672 sessions …\n",
      "[Day 8] finished 8350/15672 sessions …\n",
      "[Day 8] finished 8400/15672 sessions …\n",
      "[Day 8] finished 8450/15672 sessions …\n",
      "[Day 8] finished 8500/15672 sessions …\n",
      "[Day 8] finished 8550/15672 sessions …\n",
      "[Day 8] finished 8600/15672 sessions …\n",
      "[Day 8] finished 8650/15672 sessions …\n",
      "[Day 8] finished 8700/15672 sessions …\n",
      "[Day 8] finished 8750/15672 sessions …\n",
      "[Day 8] finished 8800/15672 sessions …\n",
      "[Day 8] finished 8850/15672 sessions …\n",
      "[Day 8] finished 8900/15672 sessions …\n",
      "[Day 8] finished 8950/15672 sessions …\n",
      "[Day 8] finished 9000/15672 sessions …\n",
      "[Day 8] finished 9050/15672 sessions …\n",
      "[Day 8] finished 9100/15672 sessions …\n",
      "[Day 8] finished 9150/15672 sessions …\n",
      "[Day 8] finished 9200/15672 sessions …\n",
      "[Day 8] finished 9250/15672 sessions …\n",
      "[Day 8] finished 9300/15672 sessions …\n",
      "[Day 8] finished 9350/15672 sessions …\n",
      "[Day 8] finished 9400/15672 sessions …\n",
      "[Day 8] finished 9450/15672 sessions …\n",
      "[Day 8] finished 9500/15672 sessions …\n",
      "[Day 8] finished 9550/15672 sessions …\n",
      "[Day 8] finished 9600/15672 sessions …\n",
      "[Day 8] finished 9650/15672 sessions …\n",
      "[Day 8] finished 9700/15672 sessions …\n",
      "[Day 8] finished 9750/15672 sessions …\n",
      "[Day 8] finished 9800/15672 sessions …\n",
      "[Day 8] finished 9850/15672 sessions …\n",
      "[Day 8] finished 9900/15672 sessions …\n",
      "[Day 8] finished 9950/15672 sessions …\n",
      "[Day 8] finished 10000/15672 sessions …\n",
      "[Day 8] finished 10050/15672 sessions …\n",
      "[Day 8] finished 10100/15672 sessions …\n",
      "[Day 8] finished 10150/15672 sessions …\n",
      "[Day 8] finished 10200/15672 sessions …\n",
      "[Day 8] finished 10250/15672 sessions …\n",
      "[Day 8] finished 10300/15672 sessions …\n",
      "[Day 8] finished 10350/15672 sessions …\n",
      "[Day 8] finished 10400/15672 sessions …\n",
      "[Day 8] finished 10450/15672 sessions …\n",
      "[Day 8] finished 10500/15672 sessions …\n",
      "[Day 8] finished 10550/15672 sessions …\n",
      "[Day 8] finished 10600/15672 sessions …\n",
      "[Day 8] finished 10650/15672 sessions …\n",
      "[Day 8] finished 10700/15672 sessions …\n",
      "[Day 8] finished 10750/15672 sessions …\n",
      "[Day 8] finished 10800/15672 sessions …\n",
      "[Day 8] finished 10850/15672 sessions …\n",
      "[Day 8] finished 10900/15672 sessions …\n",
      "[Day 8] finished 10950/15672 sessions …\n",
      "[Day 8] finished 11000/15672 sessions …\n",
      "[Day 8] finished 11050/15672 sessions …\n",
      "[Day 8] finished 11100/15672 sessions …\n",
      "[Day 8] finished 11150/15672 sessions …\n",
      "[Day 8] finished 11200/15672 sessions …\n",
      "[Day 8] finished 11250/15672 sessions …\n",
      "[Day 8] finished 11300/15672 sessions …\n",
      "[Day 8] finished 11350/15672 sessions …\n",
      "[Day 8] finished 11400/15672 sessions …\n",
      "[Day 8] finished 11450/15672 sessions …\n",
      "[Day 8] finished 11500/15672 sessions …\n",
      "[Day 8] finished 11550/15672 sessions …\n",
      "[Day 8] finished 11600/15672 sessions …\n",
      "[Day 8] finished 11650/15672 sessions …\n",
      "[Day 8] finished 11700/15672 sessions …\n",
      "[Day 8] finished 11750/15672 sessions …\n",
      "[Day 8] finished 11800/15672 sessions …\n",
      "[Day 8] finished 11850/15672 sessions …\n",
      "[Day 8] finished 11900/15672 sessions …\n",
      "[Day 8] finished 11950/15672 sessions …\n",
      "[Day 8] finished 12000/15672 sessions …\n",
      "[Day 8] finished 12050/15672 sessions …\n",
      "[Day 8] finished 12100/15672 sessions …\n",
      "[Day 8] finished 12150/15672 sessions …\n",
      "[Day 8] finished 12200/15672 sessions …\n",
      "[Day 8] finished 12250/15672 sessions …\n",
      "[Day 8] finished 12300/15672 sessions …\n",
      "[Day 8] finished 12350/15672 sessions …\n",
      "[Day 8] finished 12400/15672 sessions …\n",
      "[Day 8] finished 12450/15672 sessions …\n",
      "[Day 8] finished 12500/15672 sessions …\n",
      "[Day 8] finished 12550/15672 sessions …\n",
      "[Day 8] finished 12600/15672 sessions …\n",
      "[Day 8] finished 12650/15672 sessions …\n",
      "[Day 8] finished 12700/15672 sessions …\n",
      "[Day 8] finished 12750/15672 sessions …\n",
      "[Day 8] finished 12800/15672 sessions …\n",
      "[Day 8] finished 12850/15672 sessions …\n",
      "[Day 8] finished 12900/15672 sessions …\n",
      "[Day 8] finished 12950/15672 sessions …\n",
      "[Day 8] finished 13000/15672 sessions …\n",
      "[Day 8] finished 13050/15672 sessions …\n",
      "[Day 8] finished 13100/15672 sessions …\n",
      "[Day 8] finished 13150/15672 sessions …\n",
      "[Day 8] finished 13200/15672 sessions …\n",
      "[Day 8] finished 13250/15672 sessions …\n",
      "[Day 8] finished 13300/15672 sessions …\n",
      "[Day 8] finished 13350/15672 sessions …\n",
      "[Day 8] finished 13400/15672 sessions …\n",
      "[Day 8] finished 13450/15672 sessions …\n",
      "[Day 8] finished 13500/15672 sessions …\n",
      "[Day 8] finished 13550/15672 sessions …\n",
      "[Day 8] finished 13600/15672 sessions …\n",
      "[Day 8] finished 13650/15672 sessions …\n",
      "[Day 8] finished 13700/15672 sessions …\n",
      "[Day 8] finished 13750/15672 sessions …\n",
      "[Day 8] finished 13800/15672 sessions …\n",
      "[Day 8] finished 13850/15672 sessions …\n",
      "[Day 8] finished 13900/15672 sessions …\n",
      "[Day 8] finished 13950/15672 sessions …\n",
      "[Day 8] finished 14000/15672 sessions …\n",
      "[Day 8] finished 14050/15672 sessions …\n",
      "[Day 8] finished 14100/15672 sessions …\n",
      "[Day 8] finished 14150/15672 sessions …\n",
      "[Day 8] finished 14200/15672 sessions …\n",
      "[Day 8] finished 14250/15672 sessions …\n",
      "[Day 8] finished 14300/15672 sessions …\n",
      "[Day 8] finished 14350/15672 sessions …\n",
      "[Day 8] finished 14400/15672 sessions …\n",
      "[Day 8] finished 14450/15672 sessions …\n",
      "[Day 8] finished 14500/15672 sessions …\n",
      "[Day 8] finished 14550/15672 sessions …\n",
      "[Day 8] finished 14600/15672 sessions …\n",
      "[Day 8] finished 14650/15672 sessions …\n",
      "[Day 8] finished 14700/15672 sessions …\n",
      "[Day 8] finished 14750/15672 sessions …\n",
      "[Day 8] finished 14800/15672 sessions …\n",
      "[Day 8] finished 14850/15672 sessions …\n",
      "[Day 8] finished 14900/15672 sessions …\n",
      "[Day 8] finished 14950/15672 sessions …\n",
      "[Day 8] finished 15000/15672 sessions …\n",
      "[Day 8] finished 15050/15672 sessions …\n",
      "[Day 8] finished 15100/15672 sessions …\n",
      "[Day 8] finished 15150/15672 sessions …\n",
      "[Day 8] finished 15200/15672 sessions …\n",
      "[Day 8] finished 15250/15672 sessions …\n",
      "[Day 8] finished 15300/15672 sessions …\n",
      "[Day 8] finished 15350/15672 sessions …\n",
      "[Day 8] finished 15400/15672 sessions …\n",
      "[Day 8] finished 15450/15672 sessions …\n",
      "[Day 8] finished 15500/15672 sessions …\n",
      "[Day 8] finished 15550/15672 sessions …\n",
      "[Day 8] finished 15600/15672 sessions …\n",
      "[Day 8] finished 15650/15672 sessions …\n",
      "  wrote 626,880 rows (cumulative 2,529,400).\n",
      "[simulate] Day index 8 (2020-07-12) …\n",
      "[day9] predictions not found on disk; computing …\n",
      "[Day 9] finished 50/11095 sessions …\n",
      "[Day 9] finished 100/11095 sessions …\n",
      "[Day 9] finished 150/11095 sessions …\n",
      "[Day 9] finished 200/11095 sessions …\n",
      "[Day 9] finished 250/11095 sessions …\n",
      "[Day 9] finished 300/11095 sessions …\n",
      "[Day 9] finished 350/11095 sessions …\n",
      "[Day 9] finished 400/11095 sessions …\n",
      "[Day 9] finished 450/11095 sessions …\n",
      "[Day 9] finished 500/11095 sessions …\n",
      "[Day 9] finished 550/11095 sessions …\n",
      "[Day 9] finished 600/11095 sessions …\n",
      "[Day 9] finished 650/11095 sessions …\n",
      "[Day 9] finished 700/11095 sessions …\n",
      "[Day 9] finished 750/11095 sessions …\n",
      "[Day 9] finished 800/11095 sessions …\n",
      "[Day 9] finished 850/11095 sessions …\n",
      "[Day 9] finished 900/11095 sessions …\n",
      "[Day 9] finished 950/11095 sessions …\n",
      "[Day 9] finished 1000/11095 sessions …\n",
      "[Day 9] finished 1050/11095 sessions …\n",
      "[Day 9] finished 1100/11095 sessions …\n",
      "[Day 9] finished 1150/11095 sessions …\n",
      "[Day 9] finished 1200/11095 sessions …\n",
      "[Day 9] finished 1250/11095 sessions …\n",
      "[Day 9] finished 1300/11095 sessions …\n",
      "[Day 9] finished 1350/11095 sessions …\n",
      "[Day 9] finished 1400/11095 sessions …\n",
      "[Day 9] finished 1450/11095 sessions …\n",
      "[Day 9] finished 1500/11095 sessions …\n",
      "[Day 9] finished 1550/11095 sessions …\n",
      "[Day 9] finished 1600/11095 sessions …\n",
      "[Day 9] finished 1650/11095 sessions …\n",
      "[Day 9] finished 1700/11095 sessions …\n",
      "[Day 9] finished 1750/11095 sessions …\n",
      "[Day 9] finished 1800/11095 sessions …\n",
      "[Day 9] finished 1850/11095 sessions …\n",
      "[Day 9] finished 1900/11095 sessions …\n",
      "[Day 9] finished 1950/11095 sessions …\n",
      "[Day 9] finished 2000/11095 sessions …\n",
      "[Day 9] finished 2050/11095 sessions …\n",
      "[Day 9] finished 2100/11095 sessions …\n",
      "[Day 9] finished 2150/11095 sessions …\n",
      "[Day 9] finished 2200/11095 sessions …\n",
      "[Day 9] finished 2250/11095 sessions …\n",
      "[Day 9] finished 2300/11095 sessions …\n",
      "[Day 9] finished 2350/11095 sessions …\n",
      "[Day 9] finished 2400/11095 sessions …\n",
      "[Day 9] finished 2450/11095 sessions …\n",
      "[Day 9] finished 2500/11095 sessions …\n",
      "[Day 9] finished 2550/11095 sessions …\n",
      "[Day 9] finished 2600/11095 sessions …\n",
      "[Day 9] finished 2650/11095 sessions …\n",
      "[Day 9] finished 2700/11095 sessions …\n",
      "[Day 9] finished 2750/11095 sessions …\n",
      "[Day 9] finished 2800/11095 sessions …\n",
      "[Day 9] finished 2850/11095 sessions …\n",
      "[Day 9] finished 2900/11095 sessions …\n",
      "[Day 9] finished 2950/11095 sessions …\n",
      "[Day 9] finished 3000/11095 sessions …\n",
      "[Day 9] finished 3050/11095 sessions …\n",
      "[Day 9] finished 3100/11095 sessions …\n",
      "[Day 9] finished 3150/11095 sessions …\n",
      "[Day 9] finished 3200/11095 sessions …\n",
      "[Day 9] finished 3250/11095 sessions …\n",
      "[Day 9] finished 3300/11095 sessions …\n",
      "[Day 9] finished 3350/11095 sessions …\n",
      "[Day 9] finished 3400/11095 sessions …\n",
      "[Day 9] finished 3450/11095 sessions …\n",
      "[Day 9] finished 3500/11095 sessions …\n",
      "[Day 9] finished 3550/11095 sessions …\n",
      "[Day 9] finished 3600/11095 sessions …\n",
      "[Day 9] finished 3650/11095 sessions …\n",
      "[Day 9] finished 3700/11095 sessions …\n",
      "[Day 9] finished 3750/11095 sessions …\n",
      "[Day 9] finished 3800/11095 sessions …\n",
      "[Day 9] finished 3850/11095 sessions …\n",
      "[Day 9] finished 3900/11095 sessions …\n",
      "[Day 9] finished 3950/11095 sessions …\n",
      "[Day 9] finished 4000/11095 sessions …\n",
      "[Day 9] finished 4050/11095 sessions …\n",
      "[Day 9] finished 4100/11095 sessions …\n",
      "[Day 9] finished 4150/11095 sessions …\n",
      "[Day 9] finished 4200/11095 sessions …\n",
      "[Day 9] finished 4250/11095 sessions …\n",
      "[Day 9] finished 4300/11095 sessions …\n",
      "[Day 9] finished 4350/11095 sessions …\n",
      "[Day 9] finished 4400/11095 sessions …\n",
      "[Day 9] finished 4450/11095 sessions …\n",
      "[Day 9] finished 4500/11095 sessions …\n",
      "[Day 9] finished 4550/11095 sessions …\n",
      "[Day 9] finished 4600/11095 sessions …\n",
      "[Day 9] finished 4650/11095 sessions …\n",
      "[Day 9] finished 4700/11095 sessions …\n",
      "[Day 9] finished 4750/11095 sessions …\n",
      "[Day 9] finished 4800/11095 sessions …\n",
      "[Day 9] finished 4850/11095 sessions …\n",
      "[Day 9] finished 4900/11095 sessions …\n",
      "[Day 9] finished 4950/11095 sessions …\n",
      "[Day 9] finished 5000/11095 sessions …\n",
      "[Day 9] finished 5050/11095 sessions …\n",
      "[Day 9] finished 5100/11095 sessions …\n",
      "[Day 9] finished 5150/11095 sessions …\n",
      "[Day 9] finished 5200/11095 sessions …\n",
      "[Day 9] finished 5250/11095 sessions …\n",
      "[Day 9] finished 5300/11095 sessions …\n",
      "[Day 9] finished 5350/11095 sessions …\n",
      "[Day 9] finished 5400/11095 sessions …\n",
      "[Day 9] finished 5450/11095 sessions …\n",
      "[Day 9] finished 5500/11095 sessions …\n",
      "[Day 9] finished 5550/11095 sessions …\n",
      "[Day 9] finished 5600/11095 sessions …\n",
      "[Day 9] finished 5650/11095 sessions …\n",
      "[Day 9] finished 5700/11095 sessions …\n",
      "[Day 9] finished 5750/11095 sessions …\n",
      "[Day 9] finished 5800/11095 sessions …\n",
      "[Day 9] finished 5850/11095 sessions …\n",
      "[Day 9] finished 5900/11095 sessions …\n",
      "[Day 9] finished 5950/11095 sessions …\n",
      "[Day 9] finished 6000/11095 sessions …\n",
      "[Day 9] finished 6050/11095 sessions …\n",
      "[Day 9] finished 6100/11095 sessions …\n",
      "[Day 9] finished 6150/11095 sessions …\n",
      "[Day 9] finished 6200/11095 sessions …\n",
      "[Day 9] finished 6250/11095 sessions …\n",
      "[Day 9] finished 6300/11095 sessions …\n",
      "[Day 9] finished 6350/11095 sessions …\n",
      "[Day 9] finished 6400/11095 sessions …\n",
      "[Day 9] finished 6450/11095 sessions …\n",
      "[Day 9] finished 6500/11095 sessions …\n",
      "[Day 9] finished 6550/11095 sessions …\n",
      "[Day 9] finished 6600/11095 sessions …\n",
      "[Day 9] finished 6650/11095 sessions …\n",
      "[Day 9] finished 6700/11095 sessions …\n",
      "[Day 9] finished 6750/11095 sessions …\n",
      "[Day 9] finished 6800/11095 sessions …\n",
      "[Day 9] finished 6850/11095 sessions …\n",
      "[Day 9] finished 6900/11095 sessions …\n",
      "[Day 9] finished 6950/11095 sessions …\n",
      "[Day 9] finished 7000/11095 sessions …\n",
      "[Day 9] finished 7050/11095 sessions …\n",
      "[Day 9] finished 7100/11095 sessions …\n",
      "[Day 9] finished 7150/11095 sessions …\n",
      "[Day 9] finished 7200/11095 sessions …\n",
      "[Day 9] finished 7250/11095 sessions …\n",
      "[Day 9] finished 7300/11095 sessions …\n",
      "[Day 9] finished 7350/11095 sessions …\n",
      "[Day 9] finished 7400/11095 sessions …\n",
      "[Day 9] finished 7450/11095 sessions …\n",
      "[Day 9] finished 7500/11095 sessions …\n",
      "[Day 9] finished 7550/11095 sessions …\n",
      "[Day 9] finished 7600/11095 sessions …\n",
      "[Day 9] finished 7650/11095 sessions …\n",
      "[Day 9] finished 7700/11095 sessions …\n",
      "[Day 9] finished 7750/11095 sessions …\n",
      "[Day 9] finished 7800/11095 sessions …\n",
      "[Day 9] finished 7850/11095 sessions …\n",
      "[Day 9] finished 7900/11095 sessions …\n",
      "[Day 9] finished 7950/11095 sessions …\n",
      "[Day 9] finished 8000/11095 sessions …\n",
      "[Day 9] finished 8050/11095 sessions …\n",
      "[Day 9] finished 8100/11095 sessions …\n",
      "[Day 9] finished 8150/11095 sessions …\n",
      "[Day 9] finished 8200/11095 sessions …\n",
      "[Day 9] finished 8250/11095 sessions …\n",
      "[Day 9] finished 8300/11095 sessions …\n",
      "[Day 9] finished 8350/11095 sessions …\n",
      "[Day 9] finished 8400/11095 sessions …\n",
      "[Day 9] finished 8450/11095 sessions …\n",
      "[Day 9] finished 8500/11095 sessions …\n",
      "[Day 9] finished 8550/11095 sessions …\n",
      "[Day 9] finished 8600/11095 sessions …\n",
      "[Day 9] finished 8650/11095 sessions …\n",
      "[Day 9] finished 8700/11095 sessions …\n",
      "[Day 9] finished 8750/11095 sessions …\n",
      "[Day 9] finished 8800/11095 sessions …\n",
      "[Day 9] finished 8850/11095 sessions …\n",
      "[Day 9] finished 8900/11095 sessions …\n",
      "[Day 9] finished 8950/11095 sessions …\n",
      "[Day 9] finished 9000/11095 sessions …\n",
      "[Day 9] finished 9050/11095 sessions …\n",
      "[Day 9] finished 9100/11095 sessions …\n",
      "[Day 9] finished 9150/11095 sessions …\n",
      "[Day 9] finished 9200/11095 sessions …\n",
      "[Day 9] finished 9250/11095 sessions …\n",
      "[Day 9] finished 9300/11095 sessions …\n",
      "[Day 9] finished 9350/11095 sessions …\n",
      "[Day 9] finished 9400/11095 sessions …\n",
      "[Day 9] finished 9450/11095 sessions …\n",
      "[Day 9] finished 9500/11095 sessions …\n",
      "[Day 9] finished 9550/11095 sessions …\n",
      "[Day 9] finished 9600/11095 sessions …\n",
      "[Day 9] finished 9650/11095 sessions …\n",
      "[Day 9] finished 9700/11095 sessions …\n",
      "[Day 9] finished 9750/11095 sessions …\n",
      "[Day 9] finished 9800/11095 sessions …\n",
      "[Day 9] finished 9850/11095 sessions …\n",
      "[Day 9] finished 9900/11095 sessions …\n",
      "[Day 9] finished 9950/11095 sessions …\n",
      "[Day 9] finished 10000/11095 sessions …\n",
      "[Day 9] finished 10050/11095 sessions …\n",
      "[Day 9] finished 10100/11095 sessions …\n",
      "[Day 9] finished 10150/11095 sessions …\n",
      "[Day 9] finished 10200/11095 sessions …\n",
      "[Day 9] finished 10250/11095 sessions …\n",
      "[Day 9] finished 10300/11095 sessions …\n",
      "[Day 9] finished 10350/11095 sessions …\n",
      "[Day 9] finished 10400/11095 sessions …\n",
      "[Day 9] finished 10450/11095 sessions …\n",
      "[Day 9] finished 10500/11095 sessions …\n",
      "[Day 9] finished 10550/11095 sessions …\n",
      "[Day 9] finished 10600/11095 sessions …\n",
      "[Day 9] finished 10650/11095 sessions …\n",
      "[Day 9] finished 10700/11095 sessions …\n",
      "[Day 9] finished 10750/11095 sessions …\n",
      "[Day 9] finished 10800/11095 sessions …\n",
      "[Day 9] finished 10850/11095 sessions …\n",
      "[Day 9] finished 10900/11095 sessions …\n",
      "[Day 9] finished 10950/11095 sessions …\n",
      "[Day 9] finished 11000/11095 sessions …\n",
      "[Day 9] finished 11050/11095 sessions …\n",
      "  wrote 443,800 rows (cumulative 2,973,200).\n",
      "\n",
      "Saved residuals → /Users/haozhangao/Desktop/RecSys Research/KuaiRec 2.0/data/models/burst1/policy/residuals/D5D9_session_category_residuals.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session</th>\n",
       "      <th>i_top_category_id</th>\n",
       "      <th>q_emp</th>\n",
       "      <th>p_policy</th>\n",
       "      <th>residual</th>\n",
       "      <th>day_idx</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2467</td>\n",
       "      <td>1</td>\n",
       "      <td>-124</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-07-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2467</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-07-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2467</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-07-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2467</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0535</td>\n",
       "      <td>0.0617</td>\n",
       "      <td>-0.0083</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-07-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2467</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>0.0206</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-07-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2467</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>-0.0329</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-07-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2467</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>-0.0041</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-07-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2467</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-07-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2467</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0288</td>\n",
       "      <td>-0.0247</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-07-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2467</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-07-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  session  i_top_category_id  q_emp  p_policy  residual  day_idx  \\\n",
       "0     2467        1               -124 0.0000    0.0000    0.0000        4   \n",
       "1     2467        1                  1 0.0329    0.0247    0.0082        4   \n",
       "2     2467        1                 10 0.0247    0.0000    0.0247        4   \n",
       "3     2467        1                 11 0.0535    0.0617   -0.0083        4   \n",
       "4     2467        1                 12 0.0247    0.0206    0.0041        4   \n",
       "5     2467        1                 13 0.0041    0.0370   -0.0329        4   \n",
       "6     2467        1                 14 0.0041    0.0082   -0.0041        4   \n",
       "7     2467        1                 15 0.0247    0.0082    0.0165        4   \n",
       "8     2467        1                 16 0.0041    0.0288   -0.0247        4   \n",
       "9     2467        1                 17 0.0370    0.0123    0.0247        4   \n",
       "\n",
       "          day  \n",
       "0  2020-07-08  \n",
       "1  2020-07-08  \n",
       "2  2020-07-08  \n",
       "3  2020-07-08  \n",
       "4  2020-07-08  \n",
       "5  2020-07-08  \n",
       "6  2020-07-08  \n",
       "7  2020-07-08  \n",
       "8  2020-07-08  \n",
       "9  2020-07-08  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Residual summary (q_emp - p_policy):\n",
      "count   2973200.0000\n",
      "mean          0.0000\n",
      "std           0.0903\n",
      "min          -1.0000\n",
      "10%          -0.0420\n",
      "25%           0.0002\n",
      "50%           0.0010\n",
      "75%           0.0023\n",
      "90%           0.0495\n",
      "95%           0.1074\n",
      "max           0.9992\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Simulate category PMFs on D5–D9 with learned weights\n",
    "# and save per-session residuals:  residual = q_empirical - p_policy\n",
    "# ================================================================\n",
    "import json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- config / paths ----\n",
    "cat_col   = \"i_top_category_id\"\n",
    "heads     = [\"y_complete\",\"y_long\",\"y_rewatch\",\"y_neg\"]\n",
    "tau       = 1.0\n",
    "min_exposures = 3\n",
    "\n",
    "out_root  = OUTDIR / \"policy\"\n",
    "pred_dir  = out_root / \"heads_by_day_fast\"      # where head_probs_day*.csv were saved\n",
    "res_dir   = out_root / \"residuals\"\n",
    "res_dir.mkdir(parents=True, exist_ok=True)\n",
    "res_path  = res_dir / \"D5D9_session_category_residuals.csv\"\n",
    "\n",
    "# ---- load weights (or hardcode) ----\n",
    "pol_path = out_root / \"topk_ce\" / \"policy_weights.json\"\n",
    "if pol_path.exists():\n",
    "    with open(pol_path) as f:\n",
    "        pol = json.load(f)\n",
    "    w_hat = np.array(pol[\"w\"], dtype=\"float64\")\n",
    "else:\n",
    "    # fallback: paste your learned weights here\n",
    "    w_hat = np.array([0.032791, 0.883349, 0.007782, -0.018293], dtype=\"float64\")\n",
    "\n",
    "print(\"Using weights:\", np.round(w_hat, 6))\n",
    "\n",
    "# ---- helper: load-or-build predictions for a day index (1-based label only used for file name) ----\n",
    "def ensure_predictions_for_dayindex(day_idx: int) -> pd.DataFrame:\n",
    "    \"\"\"Return long DF: user_id, session, video_id, y_complete, y_long, y_rewatch, y_neg.\"\"\"\n",
    "    pred_path = pred_dir / f\"head_probs_day{day_idx+1}.csv\"\n",
    "    if pred_path.exists():\n",
    "        return pd.read_csv(pred_path)\n",
    "    # If not found, fall back to computing once (uses your fast builder if it's in this notebook)\n",
    "    print(f\"[day{day_idx+1}] predictions not found on disk; computing …\")\n",
    "    return predict_heads_for_dayindex_fast(day_idx, min_exposures=min_exposures, save_dir=pred_dir)\n",
    "\n",
    "# ---- empirical q (keep only robust sessions) ----\n",
    "emp_q = (\n",
    "    emp.loc[emp[\"n_total\"] >= min_exposures, [\"user_id\",\"session\",cat_col,\"p_empirical_cat_sm\"]]\n",
    "       .rename(columns={\"p_empirical_cat_sm\":\"q\"})\n",
    "       .copy()\n",
    ")\n",
    "\n",
    "# ---- item meta (video -> category) ----\n",
    "item_meta = (\n",
    "    b1[[\"video_id\", cat_col]]\n",
    "      .drop_duplicates(\"video_id\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ---- exposures per session (K_s for Top-K) ----\n",
    "# We’ll grab per-day shortly to avoid pulling the whole table repeatedly.\n",
    "def exposures_for_day(day_ts_norm) -> pd.DataFrame:\n",
    "    return (b1.loc[b1[\"ts\"].dt.normalize()==day_ts_norm, [\"user_id\",\"session\",\"video_id\"]]\n",
    "              .drop_duplicates()\n",
    "              .groupby([\"user_id\",\"session\"], as_index=False)\n",
    "              .size()\n",
    "              .rename(columns={\"size\":\"n_shown\"}))\n",
    "\n",
    "# ---- policy simulation on one day (Top-K slate) ----\n",
    "def simulate_day_topk(day_idx: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns long DF:\n",
    "      user_id, session, i_top_category_id, q_emp, p_policy, residual\n",
    "    for all D_s sessions on this day with >= min_exposures.\n",
    "    \"\"\"\n",
    "    # day index -> actual calendar day\n",
    "    uniq_days = np.sort(b1[\"ts\"].dt.normalize().unique())\n",
    "    d = uniq_days[day_idx]\n",
    "\n",
    "    # predictions and exposures for the day\n",
    "    pred = ensure_predictions_for_dayindex(day_idx)\n",
    "    if pred.empty:\n",
    "        return pd.DataFrame(columns=[\"user_id\",\"session\",cat_col,\"q_emp\",\"p_policy\",\"residual\",\"day_idx\",\"day\"])\n",
    "    ks   = exposures_for_day(d)  # K_s per session\n",
    "    # join categories to predictions once\n",
    "    pred = pred.merge(item_meta, on=\"video_id\", how=\"left\")\n",
    "\n",
    "    # restrict to sessions present in emp_q\n",
    "    key_cols = [\"user_id\",\"session\"]\n",
    "    have_q = emp_q.merge(ks[key_cols], on=key_cols, how=\"inner\")[key_cols].drop_duplicates()\n",
    "    pred   = pred.merge(have_q, on=key_cols, how=\"inner\")\n",
    "    ks     = ks.merge(have_q, on=key_cols, how=\"inner\")\n",
    "\n",
    "    # Build a quick dict (u,s) -> K_s\n",
    "    K_map = {(int(r.user_id), int(r.session)): int(r.n_shown) for _, r in ks.iterrows()}\n",
    "\n",
    "    # Prepare output rows\n",
    "    out_rows = []\n",
    "\n",
    "    # Group by session\n",
    "    for (u,s), g in pred.groupby(key_cols, sort=False):\n",
    "        K_s = K_map.get((int(u), int(s)), None)\n",
    "        if K_s is None or K_s <= 0:\n",
    "            continue\n",
    "\n",
    "        # utilities for all candidates in this session\n",
    "        H = g[heads].to_numpy(dtype=\"float64\")  # (N,4)\n",
    "        U = H @ w_hat\n",
    "\n",
    "        # pick Top-K_s indices\n",
    "        if K_s < len(U):\n",
    "            top_idx = np.argpartition(-U, K_s-1)[:K_s]\n",
    "        else:\n",
    "            top_idx = np.arange(len(U), dtype=int)\n",
    "\n",
    "        top = g.iloc[top_idx][[cat_col]].copy()\n",
    "        # aggregate to category shares within the slate\n",
    "        p_cat = (top.value_counts(cat_col, normalize=True)\n",
    "                     .rename(\"p_policy\")\n",
    "                     .reset_index())\n",
    "\n",
    "        # empirical q for this session (aligned on categories)\n",
    "        q_s = emp_q[(emp_q.user_id==u) & (emp_q.session==s)][[cat_col,\"q\"]].copy()\n",
    "        q_s = q_s.rename(columns={\"q\":\"q_emp\"})\n",
    "\n",
    "        # full outer join over categories seen in either pmf\n",
    "        both = q_s.merge(p_cat, on=cat_col, how=\"outer\")\n",
    "        both[\"q_emp\"]   = both[\"q_emp\"].fillna(0.0)\n",
    "        both[\"p_policy\"]= both[\"p_policy\"].fillna(0.0)\n",
    "        both[\"residual\"]= both[\"q_emp\"] - both[\"p_policy\"]\n",
    "        both.insert(0, \"session\", int(s))\n",
    "        both.insert(0, \"user_id\", int(u))\n",
    "        both[\"day_idx\"] = int(day_idx)\n",
    "        both[\"day\"]     = pd.to_datetime(d).date()\n",
    "\n",
    "        out_rows.append(both)\n",
    "\n",
    "    return pd.concat(out_rows, ignore_index=True) if out_rows else pd.DataFrame(\n",
    "        columns=[\"user_id\",\"session\",cat_col,\"q_emp\",\"p_policy\",\"residual\",\"day_idx\",\"day\"]\n",
    "    )\n",
    "\n",
    "# ---- run for D5–D9 and stream to disk ----\n",
    "if res_path.exists():\n",
    "    res_path.unlink()\n",
    "\n",
    "uniq_days = np.sort(b1[\"ts\"].dt.normalize().unique())\n",
    "# D4 index was 3 -> D5..D9 are 4..8 (guard if fewer days exist)\n",
    "start_idx = 4\n",
    "end_idx   = min(len(uniq_days)-1, 8)\n",
    "\n",
    "total_rows = 0\n",
    "for di in range(start_idx, end_idx+1):\n",
    "    print(f\"[simulate] Day index {di} ({pd.to_datetime(uniq_days[di]).date()}) …\")\n",
    "    resid_di = simulate_day_topk(di)\n",
    "    if resid_di.empty:\n",
    "        print(\"  nothing to write.\")\n",
    "        continue\n",
    "    # append mode\n",
    "    resid_di.to_csv(res_path, mode=\"a\", index=False, header=not res_path.exists())\n",
    "    total_rows += len(resid_di)\n",
    "    print(f\"  wrote {len(resid_di):,} rows (cumulative {total_rows:,}).\")\n",
    "\n",
    "print(f\"\\nSaved residuals → {res_path}\")\n",
    "# Quick peek\n",
    "resid_sample = pd.read_csv(res_path, nrows=10)\n",
    "display(resid_sample)\n",
    "\n",
    "# Optional: quick summaries\n",
    "resid_full = pd.read_csv(res_path, usecols=[\"residual\"])\n",
    "print(\"\\nResidual summary (q_emp - p_policy):\")\n",
    "print(resid_full[\"residual\"].describe(percentiles=[0.1,0.25,0.5,0.75,0.9,0.95]).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4350bf4f-b338-4e77-9294-10763bafcd1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled (D5–D9) mean residual by category (exclude -124):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_top_category_id</th>\n",
       "      <th>mean_residual_pooled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.3090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.0455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>34</td>\n",
       "      <td>-0.0223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>-0.0212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18</td>\n",
       "      <td>-0.0159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>38</td>\n",
       "      <td>-0.0074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>-0.0035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.0019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19</td>\n",
       "      <td>-0.0014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i_top_category_id  mean_residual_pooled\n",
       "37                 8               -0.3090\n",
       "34                 5               -0.0455\n",
       "27                34               -0.0223\n",
       "4                 12               -0.0212\n",
       "10                18               -0.0159\n",
       "31                38               -0.0074\n",
       "33                 4               -0.0050\n",
       "13                20               -0.0035\n",
       "23                 3               -0.0019\n",
       "11                19               -0.0014"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_top_category_id</th>\n",
       "      <th>mean_residual_pooled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>29</td>\n",
       "      <td>0.0073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>0.0134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>31</td>\n",
       "      <td>0.0161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32</td>\n",
       "      <td>0.0263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>28</td>\n",
       "      <td>0.1672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i_top_category_id  mean_residual_pooled\n",
       "22                29                0.0073\n",
       "12                 2                0.0123\n",
       "7                 15                0.0134\n",
       "3                 11                0.0148\n",
       "24                31                0.0161\n",
       "2                 10                0.0221\n",
       "25                32                0.0263\n",
       "35                 6                0.0468\n",
       "38                 9                0.0469\n",
       "21                28                0.1672"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall pooled mean residual (exclude -124): 6.587151576343353e-12\n",
      "\n",
      "Per-day mean residual by category (exclude -124):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>i_top_category_id</th>\n",
       "      <th>mean_residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-08</td>\n",
       "      <td>-124</td>\n",
       "      <td>0.0018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-08</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-08</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-08</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-08</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.0122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-07-08</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-07-08</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-07-08</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-07-08</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-07-08</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          day i_top_category_id  mean_residual\n",
       "0  2020-07-08              -124         0.0018\n",
       "1  2020-07-08                 1         0.0078\n",
       "2  2020-07-08                10         0.0261\n",
       "3  2020-07-08                11         0.0071\n",
       "4  2020-07-08                12        -0.0122\n",
       "5  2020-07-08                13         0.0050\n",
       "6  2020-07-08                14         0.0031\n",
       "7  2020-07-08                15         0.0055\n",
       "8  2020-07-08                16         0.0094\n",
       "9  2020-07-08                17         0.0054"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-day mean residual pivot (rows=category, cols=calendar day):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>day</th>\n",
       "      <th>2020-07-08</th>\n",
       "      <th>2020-07-09</th>\n",
       "      <th>2020-07-10</th>\n",
       "      <th>2020-07-11</th>\n",
       "      <th>2020-07-12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_top_category_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-124</th>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>-0.0040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0261</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.0392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.0122</td>\n",
       "      <td>-0.0149</td>\n",
       "      <td>-0.0272</td>\n",
       "      <td>-0.0300</td>\n",
       "      <td>-0.0221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0173</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>0.0143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.0151</td>\n",
       "      <td>-0.0149</td>\n",
       "      <td>-0.0133</td>\n",
       "      <td>-0.0198</td>\n",
       "      <td>-0.0165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0071</td>\n",
       "      <td>-0.0041</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>-0.0057</td>\n",
       "      <td>-0.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.0024</td>\n",
       "      <td>-0.0048</td>\n",
       "      <td>-0.0042</td>\n",
       "      <td>-0.0025</td>\n",
       "      <td>-0.0036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0105</td>\n",
       "      <td>-0.0095</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>-0.0046</td>\n",
       "      <td>-0.0038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>-0.0024</td>\n",
       "      <td>-0.0017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.1431</td>\n",
       "      <td>0.1557</td>\n",
       "      <td>0.1805</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>0.1892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0014</td>\n",
       "      <td>-0.0025</td>\n",
       "      <td>-0.0030</td>\n",
       "      <td>-0.0023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.0145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0259</td>\n",
       "      <td>0.0311</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>0.0248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>-0.0020</td>\n",
       "      <td>-0.0074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.0140</td>\n",
       "      <td>-0.0118</td>\n",
       "      <td>-0.0334</td>\n",
       "      <td>-0.0315</td>\n",
       "      <td>-0.0209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.0071</td>\n",
       "      <td>-0.0082</td>\n",
       "      <td>-0.0075</td>\n",
       "      <td>-0.0068</td>\n",
       "      <td>-0.0071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.0055</td>\n",
       "      <td>-0.0039</td>\n",
       "      <td>-0.0022</td>\n",
       "      <td>-0.0047</td>\n",
       "      <td>-0.0101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.0501</td>\n",
       "      <td>-0.0463</td>\n",
       "      <td>-0.0392</td>\n",
       "      <td>-0.0436</td>\n",
       "      <td>-0.0497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.0501</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.0382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.0003</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0076</td>\n",
       "      <td>0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.3213</td>\n",
       "      <td>-0.3003</td>\n",
       "      <td>-0.3066</td>\n",
       "      <td>-0.3037</td>\n",
       "      <td>-0.3150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0459</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unknown</th>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "day                2020-07-08  2020-07-09  2020-07-10  2020-07-11  2020-07-12\n",
       "i_top_category_id                                                            \n",
       "-124                   0.0018      0.0015      0.0016      0.0018      0.0018\n",
       "1                      0.0078      0.0023      0.0006      0.0004     -0.0040\n",
       "10                     0.0261      0.0190      0.0265      0.0199      0.0178\n",
       "11                     0.0071      0.0047      0.0059      0.0248      0.0392\n",
       "12                    -0.0122     -0.0149     -0.0272     -0.0300     -0.0221\n",
       "13                     0.0050      0.0014      0.0023      0.0118      0.0078\n",
       "14                     0.0031      0.0026      0.0050      0.0064      0.0050\n",
       "15                     0.0055      0.0055      0.0173      0.0249      0.0143\n",
       "16                     0.0094      0.0073      0.0080      0.0055      0.0052\n",
       "17                     0.0054      0.0072      0.0057      0.0012      0.0036\n",
       "18                    -0.0151     -0.0149     -0.0133     -0.0198     -0.0165\n",
       "19                     0.0071     -0.0041      0.0003     -0.0057     -0.0060\n",
       "2                      0.0050      0.0112      0.0239      0.0125      0.0075\n",
       "20                    -0.0024     -0.0048     -0.0042     -0.0025     -0.0036\n",
       "21                     0.0020      0.0031      0.0031      0.0052      0.0052\n",
       "22                     0.0021      0.0022      0.0024      0.0020      0.0022\n",
       "23                     0.0075      0.0049      0.0073      0.0039      0.0031\n",
       "24                     0.0016      0.0011      0.0016      0.0010      0.0021\n",
       "25                     0.0105     -0.0095      0.0052     -0.0046     -0.0038\n",
       "26                     0.0136      0.0040      0.0008     -0.0024     -0.0017\n",
       "27                     0.0076      0.0037      0.0021      0.0028      0.0034\n",
       "28                     0.1431      0.1557      0.1805      0.1741      0.1892\n",
       "29                     0.0084      0.0087      0.0069      0.0065      0.0051\n",
       "3                     -0.0005     -0.0014     -0.0025     -0.0030     -0.0023\n",
       "31                     0.0171      0.0126      0.0178      0.0183      0.0145\n",
       "32                     0.0259      0.0311      0.0239      0.0251      0.0248\n",
       "33                     0.0080      0.0005     -0.0011     -0.0020     -0.0074\n",
       "34                    -0.0140     -0.0118     -0.0334     -0.0315     -0.0209\n",
       "35                     0.0034      0.0038      0.0045      0.0078      0.0060\n",
       "36                     0.0055      0.0050      0.0032      0.0040      0.0042\n",
       "37                     0.0030      0.0022      0.0021      0.0027      0.0037\n",
       "38                    -0.0071     -0.0082     -0.0075     -0.0068     -0.0071\n",
       "39                     0.0015      0.0005      0.0005      0.0037      0.0016\n",
       "4                     -0.0055     -0.0039     -0.0022     -0.0047     -0.0101\n",
       "5                     -0.0501     -0.0463     -0.0392     -0.0436     -0.0497\n",
       "6                      0.0370      0.0529      0.0501      0.0530      0.0382\n",
       "7                     -0.0003      0.0094     -0.0005     -0.0076      0.0003\n",
       "8                     -0.3213     -0.3003     -0.3066     -0.3037     -0.3150\n",
       "9                      0.0459      0.0547      0.0276      0.0478      0.0632\n",
       "unknown                0.0011      0.0010      0.0010      0.0010      0.0011"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-day overall mean residual (pooled across categories, exclude -124):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>mean_residual_overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-08</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-09</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-10</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-12</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          day  mean_residual_overall\n",
       "0  2020-07-08                 0.0000\n",
       "1  2020-07-09                 0.0000\n",
       "2  2020-07-10                 0.0000\n",
       "3  2020-07-11                 0.0000\n",
       "4  2020-07-12                 0.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to your saved residuals\n",
    "res_path = OUTDIR / \"policy\" / \"residuals\" / \"D5D9_session_category_residuals.csv\"\n",
    "\n",
    "# Load and filter out the unrecorded category\n",
    "resid = pd.read_csv(res_path)\n",
    "resid = resid[resid[\"i_top_category_id\"] != -124].copy()\n",
    "\n",
    "# --- 1) POOLED mean residual across ALL days, by category\n",
    "pooled_cat_mean = (\n",
    "    resid.groupby(\"i_top_category_id\", as_index=False)[\"residual\"]\n",
    "         .mean()\n",
    "         .rename(columns={\"residual\": \"mean_residual_pooled\"})\n",
    "         .sort_values(\"mean_residual_pooled\")\n",
    ")\n",
    "print(\"Pooled (D5–D9) mean residual by category (exclude -124):\")\n",
    "display(pooled_cat_mean.head(10))\n",
    "display(pooled_cat_mean.tail(10))\n",
    "\n",
    "# Optional: overall pooled mean across all categories and sessions (single scalar)\n",
    "overall_pooled_mean = resid[\"residual\"].mean()\n",
    "print(\"Overall pooled mean residual (exclude -124):\", overall_pooled_mean)\n",
    "\n",
    "# --- 2) PER-DAY mean residual by category\n",
    "# day column is already in the CSV; if you prefer day_idx, swap \"day\" with \"day_idx\" below\n",
    "perday_cat_mean = (\n",
    "    resid.groupby([\"day\", \"i_top_category_id\"], as_index=False)[\"residual\"]\n",
    "         .mean()\n",
    "         .rename(columns={\"residual\": \"mean_residual\"})\n",
    ")\n",
    "\n",
    "print(\"\\nPer-day mean residual by category (exclude -124):\")\n",
    "display(perday_cat_mean.head(10))\n",
    "\n",
    "# (Nice view) Pivot to category x day matrix\n",
    "perday_pivot = (\n",
    "    perday_cat_mean.pivot(index=\"i_top_category_id\", columns=\"day\", values=\"mean_residual\")\n",
    "                   .sort_index()\n",
    ")\n",
    "print(\"\\nPer-day mean residual pivot (rows=category, cols=calendar day):\")\n",
    "display(perday_pivot)\n",
    "\n",
    "# --- 3) PER-DAY overall mean residual across all categories (one row per day)\n",
    "perday_overall_mean = (\n",
    "    resid.groupby(\"day\", as_index=False)[\"residual\"].mean()\n",
    "         .rename(columns={\"residual\": \"mean_residual_overall\"})\n",
    ")\n",
    "print(\"\\nPer-day overall mean residual (pooled across categories, exclude -124):\")\n",
    "display(perday_overall_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d12966b7-d643-45f8-952c-98f5799d42b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOST mean≈0 (±0.005) — Pooled D5–D9 ===\n",
      "mu=-0.000000  se=0.000000  df=2973199\n",
      "TOST p-value=0   [one-sided p's: p(mu>-δ)=0, p(mu<+δ)=0]\n",
      "Interpretation: p<0.05 → mean residual is within ±δ (practically zero).\n",
      "\n",
      "=== TOST mean≈0 (±0.005) — Day idx 4 ===\n",
      "mu=-0.000000  se=0.000000  df=630799\n",
      "TOST p-value=0   [one-sided p's: p(mu>-δ)=0, p(mu<+δ)=0]\n",
      "Interpretation: p<0.05 → mean residual is within ±δ (practically zero).\n",
      "\n",
      "=== TOST mean≈0 (±0.005) — Day idx 5 ===\n",
      "mu=-0.000000  se=0.000000  df=642479\n",
      "TOST p-value=0   [one-sided p's: p(mu>-δ)=0, p(mu<+δ)=0]\n",
      "Interpretation: p<0.05 → mean residual is within ±δ (practically zero).\n",
      "\n",
      "=== TOST mean≈0 (±0.005) — Day idx 6 ===\n",
      "mu=-0.000000  se=0.000000  df=629239\n",
      "TOST p-value=0   [one-sided p's: p(mu>-δ)=0, p(mu<+δ)=0]\n",
      "Interpretation: p<0.05 → mean residual is within ±δ (practically zero).\n",
      "\n",
      "=== TOST mean≈0 (±0.005) — Day idx 7 ===\n",
      "mu=-0.000000  se=0.000000  df=626879\n",
      "TOST p-value=0   [one-sided p's: p(mu>-δ)=0, p(mu<+δ)=0]\n",
      "Interpretation: p<0.05 → mean residual is within ±δ (practically zero).\n",
      "\n",
      "=== TOST mean≈0 (±0.005) — Day idx 8 ===\n",
      "mu=-0.000000  se=0.000000  df=443799\n",
      "TOST p-value=0   [one-sided p's: p(mu>-δ)=0, p(mu<+δ)=0]\n",
      "Interpretation: p<0.05 → mean residual is within ±δ (practically zero).\n",
      "\n",
      "=== Serial structure — Pooled D5–D9 ===\n",
      "users=6904 | median AC1=-0.051 | p10=-0.524 | p90=0.385\n",
      "Mean(AC1)=-0.058  t=-13.263  p=0\n",
      "Interpretation: small median and |mean| with non-sig p → little serial dependence.\n",
      "\n",
      "=== Serial structure — Day idx 4 ===\n",
      "users=2677 | median AC1=-0.612 | p10=-1.000 | p90=1.000\n",
      "Mean(AC1)=-0.299  t=-19.433  p=0\n",
      "Interpretation: small median and |mean| with non-sig p → little serial dependence.\n",
      "\n",
      "=== Serial structure — Day idx 5 ===\n",
      "users=2705 | median AC1=-0.601 | p10=-1.000 | p90=1.000\n",
      "Mean(AC1)=-0.296  t=-19.390  p=0\n",
      "Interpretation: small median and |mean| with non-sig p → little serial dependence.\n",
      "\n",
      "=== Serial structure — Day idx 6 ===\n",
      "users=2600 | median AC1=-0.689 | p10=-1.000 | p90=1.000\n",
      "Mean(AC1)=-0.344  t=-22.501  p=0\n",
      "Interpretation: small median and |mean| with non-sig p → little serial dependence.\n",
      "\n",
      "=== Serial structure — Day idx 7 ===\n",
      "users=2627 | median AC1=-0.715 | p10=-1.000 | p90=1.000\n",
      "Mean(AC1)=-0.340  t=-22.142  p=0\n",
      "Interpretation: small median and |mean| with non-sig p → little serial dependence.\n",
      "\n",
      "=== Serial structure — Day idx 8 ===\n",
      "users=1349 | median AC1=-0.979 | p10=-1.000 | p90=1.000\n",
      "Mean(AC1)=-0.353  t=-15.615  p=0\n",
      "Interpretation: small median and |mean| with non-sig p → little serial dependence.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Plan A: Diagnostics code\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# -------------------------\n",
    "# Load & basic filtering\n",
    "# -------------------------\n",
    "resid = pd.read_csv(res_path)\n",
    "\n",
    "# Optional: drop \"unknown\" category if you want\n",
    "resid = resid[resid[\"i_top_category_id\"] != -124].copy()\n",
    "\n",
    "# Sanity cleanups\n",
    "resid[\"user_id\"] = resid[\"user_id\"].astype(int)\n",
    "resid[\"session\"] = resid[\"session\"].astype(int)\n",
    "resid[\"p_policy\"] = resid[\"p_policy\"].fillna(0.0)\n",
    "resid[\"q_emp\"]    = resid[\"q_emp\"].fillna(0.0)\n",
    "resid[\"residual\"] = resid[\"q_emp\"] - resid[\"p_policy\"]  # defensively recompute\n",
    "\n",
    "# ===========================================================\n",
    "# 1) TOST (Two One-Sided Tests) : mean(residual) ≈ 0 ± delta\n",
    "#    Clustered SEs by user\n",
    "# ===========================================================\n",
    "def tost_mean_zero(resid_df: pd.DataFrame, delta: float = 0.005, label: str = \"All\"):\n",
    "    \"\"\"\n",
    "    Tests H0: |mu| >= delta  vs  H1: |mu| < delta  (equivalence)\n",
    "    Using OLS with intercept only and user-clustered SEs.\n",
    "    Prints the result and returns a dict.\n",
    "    \"\"\"\n",
    "    y = resid_df[\"residual\"].to_numpy()\n",
    "    uid = resid_df[\"user_id\"].to_numpy()\n",
    "    X = np.ones((len(y), 1))  # intercept only\n",
    "    fit = sm.OLS(y, X).fit(cov_type=\"cluster\", cov_kwds={\"groups\": uid})\n",
    "\n",
    "    mu = float(fit.params[0])\n",
    "    se = float(fit.bse[0])\n",
    "    df = fit.df_resid\n",
    "\n",
    "    # TOST: need BOTH one-sided tests to be significant to claim equivalence\n",
    "    t1 = (mu - (-delta)) / se   # H0: mu <= -delta  (test mu > -delta)\n",
    "    t2 = (delta - mu) / se      # H0: mu >= +delta (test mu < +delta)\n",
    "    p1 = 1 - t.cdf(t1, df)      # upper tail\n",
    "    p2 = 1 - t.cdf(t2, df)\n",
    "\n",
    "    p_tost = max(p1, p2)        # conservative p-value\n",
    "\n",
    "    print(f\"\\n=== TOST mean≈0 (±{delta:.3f}) — {label} ===\")\n",
    "    print(f\"mu={mu:.6f}  se={se:.6f}  df={df:.0f}\")\n",
    "    print(f\"TOST p-value={p_tost:.3g}   \"\n",
    "          f\"[one-sided p's: p(mu>-δ)={p1:.3g}, p(mu<+δ)={p2:.3g}]\")\n",
    "    print(\"Interpretation: p<0.05 → mean residual is within ±δ (practically zero).\")\n",
    "\n",
    "    return {\"label\": label, \"mu\": mu, \"se\": se, \"p_tost\": p_tost, \"p1\": p1, \"p2\": p2}\n",
    "\n",
    "# Run once on pooled and also per day (D5–D9)\n",
    "_ = tost_mean_zero(resid, delta=0.005, label=\"Pooled D5–D9\")\n",
    "for d in sorted(resid[\"day_idx\"].unique()):\n",
    "    tost_mean_zero(resid[resid[\"day_idx\"] == d], delta=0.005, label=f\"Day idx {d}\")\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# 3) Serial structure test:\n",
    "#    Lag-1 autocorrelation of session-level residual sums per user\n",
    "# ===========================================================\n",
    "def serial_structure(resid_df: pd.DataFrame, label: str = \"All\"):\n",
    "    \"\"\"\n",
    "    Collapse to (user, session) total residual, sort by (user, day_idx, session),\n",
    "    compute lag-1 autocorrelation per user. Summarize the distribution.\n",
    "    \"\"\"\n",
    "    # collapse residual vector within a session to a scalar (sum preserves sign)\n",
    "    sess = (resid_df.groupby([\"user_id\",\"day_idx\",\"session\"], as_index=False)[\"residual\"]\n",
    "                  .sum()\n",
    "                  .sort_values([\"user_id\",\"day_idx\",\"session\"]))\n",
    "\n",
    "    def ac1(x):\n",
    "        if len(x) < 2: return np.nan\n",
    "        return pd.Series(x).autocorr(lag=1)\n",
    "\n",
    "    ac_by_user = (sess.groupby(\"user_id\")[\"residual\"].apply(ac1).dropna())\n",
    "    if ac_by_user.empty:\n",
    "        print(f\"\\n=== Serial structure — {label} ===\\nNot enough per-user time series.\")\n",
    "        return ac_by_user\n",
    "\n",
    "    print(f\"\\n=== Serial structure — {label} ===\")\n",
    "    print(f\"users={ac_by_user.size} | \"\n",
    "          f\"median AC1={ac_by_user.median():.3f} | \"\n",
    "          f\"p10={ac_by_user.quantile(0.10):.3f} | \"\n",
    "          f\"p90={ac_by_user.quantile(0.90):.3f}\")\n",
    "\n",
    "    # Test mean(ac1) == 0 with a plain t-test (quick heuristic)\n",
    "    m = ac_by_user.mean(); s = ac_by_user.std(ddof=1); n = ac_by_user.size\n",
    "    tstat = m / (s / np.sqrt(n)) if s > 0 else np.nan\n",
    "    pval  = 2 * (1 - t.cdf(abs(tstat), n-1)) if s > 0 else np.nan\n",
    "    print(f\"Mean(AC1)={m:.3f}  t={tstat:.3f}  p={pval:.3g}\")\n",
    "    print(\"Interpretation: small median and |mean| with non-sig p → little serial dependence.\")\n",
    "    return ac_by_user\n",
    "\n",
    "_ = serial_structure(resid, label=\"Pooled D5–D9\")\n",
    "for d in sorted(resid[\"day_idx\"].unique()):\n",
    "    serial_structure(resid[resid[\"day_idx\"] == d], label=f\"Day idx {d}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
